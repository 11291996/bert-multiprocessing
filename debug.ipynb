{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking tokenizer\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing CoLA dataset\n",
    "\n",
    "### grammatically correct or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "import os\n",
    "\n",
    "print('Downloading dataset...')\n",
    "\n",
    "# The URL for the dataset zip file.\n",
    "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
    "\n",
    "# Download the file (if we haven't already)\n",
    "if not os.path.exists('/scratch/paneah/cola_public_1.1.zip'):\n",
    "    wget.download(url, '/scratch/paneah/cola_public_1.1.zip')\n",
    "\n",
    "# Unzip the dataset (if we haven't already)\n",
    "if not os.path.exists('/scratch/paneah/cola_public'):\n",
    "    !unzip /scratch/paneah/cola_public_1.1.zip -d /scratch/paneah/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec 14 22:58:45 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090         On | 00000000:81:00.0 Off |                  N/A |\n",
      "| 37%   28C    P8               23W / 350W|      3MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 3090         On | 00000000:A1:00.0 Off |                  N/A |\n",
      "| 38%   24C    P8               21W / 350W|      3MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 8,551\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_source</th>\n",
       "      <th>label</th>\n",
       "      <th>label_notes</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5071</th>\n",
       "      <td>ks08</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>I believe that the world is round strongly.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2294</th>\n",
       "      <td>l-93</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>The witch turned him from a prince.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>bc01</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John bought the picture of himself that Bill saw.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8366</th>\n",
       "      <td>ad03</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aphrodite may quickly free the animals.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5857</th>\n",
       "      <td>c_13</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Brazilians pumped the oil across the river.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5367</th>\n",
       "      <td>b_73</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Susan doesn't eat her vegetables enough.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4560</th>\n",
       "      <td>ks08</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>It has rain every day for the last week.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6403</th>\n",
       "      <td>d_98</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Every woman standing under that tree is Mary's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4356</th>\n",
       "      <td>ks08</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There are believed to be sheep in the park.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6299</th>\n",
       "      <td>c_13</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Frank will eat an apple and Morgan will too.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_source  label label_notes  \\\n",
       "5071            ks08      0           *   \n",
       "2294            l-93      0           *   \n",
       "979             bc01      1         NaN   \n",
       "8366            ad03      1         NaN   \n",
       "5857            c_13      1         NaN   \n",
       "5367            b_73      1         NaN   \n",
       "4560            ks08      0           *   \n",
       "6403            d_98      1         NaN   \n",
       "4356            ks08      1         NaN   \n",
       "6299            c_13      1         NaN   \n",
       "\n",
       "                                               sentence  \n",
       "5071        I believe that the world is round strongly.  \n",
       "2294                The witch turned him from a prince.  \n",
       "979   John bought the picture of himself that Bill saw.  \n",
       "8366            Aphrodite may quickly free the animals.  \n",
       "5857    The Brazilians pumped the oil across the river.  \n",
       "5367           Susan doesn't eat her vegetables enough.  \n",
       "4560           It has rain every day for the last week.  \n",
       "6403  Every woman standing under that tree is Mary's...  \n",
       "4356        There are believed to be sheep in the park.  \n",
       "6299       Frank will eat an apple and Morgan will too.  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#loading dataset with pandas\n",
    "df = pd.read_csv(\"/scratch/paneah/cola_public/raw/in_domain_train.tsv\", \\\n",
    "                 delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "#printing number of sentences\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "#display random 10 rows \n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
      "Token IDs: [101, 2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "# Get the lists of sentences and their labels.\n",
    "sentences = df.sentence.values\n",
    "labels = df.label.values\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "\n",
    "                        # This function also supports truncation and conversion\n",
    "                        # to pytorch tensors, but we need to do padding, so we\n",
    "                        # can't use these features :( .\n",
    "                        #max_length = 128,          # Truncate all sentences.\n",
    "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids.append(encoded_sent)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/paneah/anaconda3/envs/LIS8040/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
      "Token IDs: tensor([  101,  2256,  2814,  2180,  1005,  1056,  4965,  2023,  4106,  1010,\n",
      "         2292,  2894,  1996,  2279,  2028,  2057, 16599,  1012,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7,695 training samples\n",
      "  856 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paneah/anaconda3/envs/LIS8040/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   0%|                                                                                      | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.8277, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6166,  0.1010],\n",
      "        [ 0.7037,  0.0917],\n",
      "        [ 0.5382,  0.2110],\n",
      "        [ 0.3998,  0.0816],\n",
      "        [ 0.7558,  0.1214],\n",
      "        [ 0.5495,  0.0447],\n",
      "        [ 0.5952,  0.0849],\n",
      "        [ 0.6200, -0.1042],\n",
      "        [ 0.8252,  0.0635],\n",
      "        [ 0.5924,  0.1349],\n",
      "        [ 0.7005,  0.0552],\n",
      "        [ 0.9809,  0.1697],\n",
      "        [ 0.7981,  0.1985],\n",
      "        [ 0.6155,  0.1968],\n",
      "        [ 0.9245,  0.2495],\n",
      "        [ 1.0990,  0.1884],\n",
      "        [ 0.5896,  0.1726],\n",
      "        [ 0.7901,  0.0817],\n",
      "        [ 0.5910,  0.1578],\n",
      "        [ 0.7196,  0.1819],\n",
      "        [ 0.7065, -0.0479],\n",
      "        [ 0.8457, -0.1434],\n",
      "        [ 0.9219,  0.1112],\n",
      "        [ 1.0231,  0.0543],\n",
      "        [ 0.1673,  0.0660],\n",
      "        [ 0.8025,  0.3730],\n",
      "        [ 0.5688, -0.0068],\n",
      "        [ 0.8737,  0.4722],\n",
      "        [ 0.6920,  0.3135],\n",
      "        [ 0.5603,  0.0166],\n",
      "        [ 0.7782, -0.0880],\n",
      "        [ 0.9404,  0.3009]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8868, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9564,  0.0721],\n",
      "        [ 1.0005,  0.0703],\n",
      "        [ 0.4431, -0.0813],\n",
      "        [ 0.9738,  0.1988],\n",
      "        [ 1.0405,  0.3775],\n",
      "        [ 0.5652,  0.0487],\n",
      "        [ 0.8344,  0.0230],\n",
      "        [ 0.8433, -0.0220],\n",
      "        [ 0.8583,  0.1325],\n",
      "        [ 0.8632,  0.2118],\n",
      "        [ 1.4083,  0.2805],\n",
      "        [ 0.6858,  0.0111],\n",
      "        [ 0.7443,  0.3016],\n",
      "        [ 0.8942,  0.2742],\n",
      "        [ 0.6119,  0.2743],\n",
      "        [ 0.5718,  0.0999],\n",
      "        [-0.1938, -0.0777],\n",
      "        [ 0.5550, -0.0113],\n",
      "        [ 0.7462,  0.0631],\n",
      "        [ 0.6213, -0.0145],\n",
      "        [ 0.7355,  0.1408],\n",
      "        [ 1.2847, -0.0716],\n",
      "        [ 1.0547,  0.0702],\n",
      "        [ 0.7420,  0.0404],\n",
      "        [ 0.5783,  0.1418],\n",
      "        [ 0.7789, -0.0771],\n",
      "        [ 0.7856,  0.2264],\n",
      "        [ 0.9274,  0.1162],\n",
      "        [ 0.9018,  0.1389],\n",
      "        [ 0.6052, -0.1140],\n",
      "        [ 0.8408, -0.0381],\n",
      "        [ 0.5726,  0.0908]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8913, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9710,  0.0284],\n",
      "        [ 0.5551,  0.1834],\n",
      "        [ 0.3707, -0.2034],\n",
      "        [ 0.7490, -0.0382],\n",
      "        [ 0.6267,  0.2857],\n",
      "        [ 0.7414,  0.1375],\n",
      "        [ 0.6701,  0.3639],\n",
      "        [ 0.8094,  0.0895],\n",
      "        [ 0.7233, -0.1623],\n",
      "        [ 0.8629, -0.0156],\n",
      "        [ 0.9858,  0.0231],\n",
      "        [ 0.5659, -0.0053],\n",
      "        [ 0.7651,  0.2247],\n",
      "        [ 0.4609, -0.1729],\n",
      "        [ 0.7414, -0.0198],\n",
      "        [ 0.7467, -0.0731],\n",
      "        [ 0.9007, -0.0850],\n",
      "        [ 0.6898,  0.3076],\n",
      "        [ 0.5598,  0.0373],\n",
      "        [ 0.6232,  0.1040],\n",
      "        [ 0.9217,  0.0950],\n",
      "        [ 0.6589,  0.2405],\n",
      "        [ 0.7145,  0.0480],\n",
      "        [ 0.8632,  0.0667],\n",
      "        [ 0.9209,  0.2746],\n",
      "        [ 0.6337,  0.1371],\n",
      "        [ 0.7492,  0.1445],\n",
      "        [ 0.9813,  0.2050],\n",
      "        [ 0.5926,  0.0780],\n",
      "        [ 0.5100,  0.0178],\n",
      "        [ 0.5785,  0.0119],\n",
      "        [ 0.8836, -0.1090]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8924, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0138e+00,  3.0258e-01],\n",
      "        [ 1.0821e+00, -1.0632e-03],\n",
      "        [ 4.7144e-01,  2.6619e-01],\n",
      "        [ 6.8669e-01, -1.7881e-02],\n",
      "        [ 9.6073e-01,  7.4005e-02],\n",
      "        [ 9.3222e-01,  9.3783e-02],\n",
      "        [ 7.9435e-01,  4.0835e-02],\n",
      "        [ 3.8530e-01, -2.5041e-01],\n",
      "        [ 5.6847e-01,  2.2025e-01],\n",
      "        [ 1.0327e+00, -4.0613e-02],\n",
      "        [ 8.1289e-01,  9.6716e-02],\n",
      "        [ 2.5078e-01, -3.7256e-02],\n",
      "        [ 7.5953e-01,  7.2105e-02],\n",
      "        [ 8.8756e-01,  1.8597e-02],\n",
      "        [ 7.8696e-01,  2.6799e-02],\n",
      "        [ 3.4036e-01,  3.0708e-02],\n",
      "        [ 5.5145e-01,  3.0511e-03],\n",
      "        [ 6.4514e-01,  1.9317e-02],\n",
      "        [ 5.0218e-01,  8.9690e-02],\n",
      "        [ 4.7955e-01, -4.7350e-02],\n",
      "        [ 5.7148e-01,  7.3966e-02],\n",
      "        [ 2.5778e-01,  1.0330e-01],\n",
      "        [ 4.1102e-01, -3.2906e-02],\n",
      "        [ 9.7932e-01,  8.4930e-02],\n",
      "        [ 7.2868e-01,  1.7934e-01],\n",
      "        [ 9.5536e-01,  2.2718e-01],\n",
      "        [ 6.4697e-01, -4.2470e-02],\n",
      "        [ 6.8062e-01,  1.0538e-01],\n",
      "        [ 9.4889e-01,  2.9940e-01],\n",
      "        [ 7.5379e-01,  2.1263e-01],\n",
      "        [ 4.8941e-01,  1.3716e-01],\n",
      "        [ 1.1747e+00,  3.2415e-01]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.8913, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6741,  0.1703],\n",
      "        [ 0.9368,  0.3134],\n",
      "        [ 0.8631,  0.0445],\n",
      "        [ 0.9039,  0.1512],\n",
      "        [ 1.0093,  0.0530],\n",
      "        [ 0.5826,  0.2729],\n",
      "        [ 0.8901,  0.1489],\n",
      "        [ 0.4644, -0.0527],\n",
      "        [ 0.8479,  0.3709],\n",
      "        [ 0.5641, -0.2604],\n",
      "        [ 0.7914,  0.2472],\n",
      "        [ 0.7523,  0.2347],\n",
      "        [ 0.6239, -0.0688],\n",
      "        [ 0.7732,  0.0906],\n",
      "        [ 0.6484, -0.0154],\n",
      "        [ 1.0744,  0.0494],\n",
      "        [ 0.8737,  0.2988],\n",
      "        [ 0.7966,  0.0043],\n",
      "        [ 0.3217, -0.2110],\n",
      "        [ 0.7336,  0.0112],\n",
      "        [ 1.1123,  0.0492],\n",
      "        [ 0.9147,  0.0979],\n",
      "        [ 0.7654, -0.1320],\n",
      "        [ 0.5138, -0.0419],\n",
      "        [ 0.7282, -0.0145],\n",
      "        [ 0.6101,  0.1462],\n",
      "        [ 0.6644, -0.1601],\n",
      "        [ 0.8492,  0.0910],\n",
      "        [ 0.5035, -0.0565],\n",
      "        [ 0.7657, -0.1354],\n",
      "        [ 0.6726, -0.0154],\n",
      "        [ 0.6577,  0.0623]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8631, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8151, -0.0069],\n",
      "        [ 1.0330,  0.2008],\n",
      "        [ 0.7024,  0.2150],\n",
      "        [ 0.8567,  0.3100],\n",
      "        [ 0.7418,  0.0492],\n",
      "        [ 0.9144,  0.1886],\n",
      "        [ 0.6416,  0.1670],\n",
      "        [ 0.3027,  0.3133],\n",
      "        [ 0.6041, -0.1095],\n",
      "        [ 0.6104, -0.1464],\n",
      "        [ 0.4823, -0.0199],\n",
      "        [ 0.6479,  0.0702],\n",
      "        [ 0.7295,  0.1137],\n",
      "        [ 0.9685,  0.0531],\n",
      "        [ 0.5601,  0.0119],\n",
      "        [ 1.0533,  0.2229],\n",
      "        [ 0.3546, -0.1834],\n",
      "        [ 0.8685,  0.0992],\n",
      "        [ 0.6961,  0.0792],\n",
      "        [ 0.8672,  0.0200],\n",
      "        [ 0.2302,  0.0279],\n",
      "        [ 0.6808,  0.0864],\n",
      "        [ 0.7198,  0.1274],\n",
      "        [ 0.6545,  0.3028],\n",
      "        [ 0.5594,  0.1426],\n",
      "        [ 0.8708,  0.0437],\n",
      "        [ 0.8535,  0.3627],\n",
      "        [ 0.8039,  0.0216],\n",
      "        [ 0.5611,  0.0086],\n",
      "        [ 0.7348,  0.0211],\n",
      "        [ 0.8529,  0.2085],\n",
      "        [ 0.7118,  0.1607]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9175, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7937,  0.1567],\n",
      "        [ 0.6448,  0.0498],\n",
      "        [ 0.7456, -0.0126],\n",
      "        [ 0.9224, -0.0454],\n",
      "        [ 1.0548,  0.1951],\n",
      "        [ 0.7158,  0.0216],\n",
      "        [ 0.6129, -0.0120],\n",
      "        [ 0.6872, -0.0272],\n",
      "        [ 1.0292,  0.0545],\n",
      "        [ 0.8572,  0.0716],\n",
      "        [ 0.6549, -0.1227],\n",
      "        [ 0.6523,  0.0563],\n",
      "        [ 0.6101, -0.0741],\n",
      "        [ 0.2588,  0.0256],\n",
      "        [ 0.7511,  0.3060],\n",
      "        [ 0.4243, -0.0813],\n",
      "        [ 0.6285,  0.0721],\n",
      "        [ 1.0928,  0.0113],\n",
      "        [ 0.5955, -0.1756],\n",
      "        [ 0.5563,  0.0434],\n",
      "        [ 0.9491, -0.0831],\n",
      "        [ 0.6451,  0.2061],\n",
      "        [ 0.8340,  0.1285],\n",
      "        [ 0.6828,  0.3605],\n",
      "        [ 0.7537, -0.1875],\n",
      "        [ 0.9844, -0.1257],\n",
      "        [ 0.7268,  0.1725],\n",
      "        [ 0.4992,  0.0876],\n",
      "        [ 0.8771,  0.2146],\n",
      "        [ 0.9938, -0.2648],\n",
      "        [ 0.4129,  0.0271],\n",
      "        [ 0.8266,  0.2091]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9285, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8770,  0.3138],\n",
      "        [ 0.6732,  0.0613],\n",
      "        [ 0.7700,  0.1989],\n",
      "        [ 0.6996,  0.4661],\n",
      "        [ 0.7096, -0.0701],\n",
      "        [ 0.6820, -0.2130],\n",
      "        [ 0.9394,  0.1608],\n",
      "        [ 0.9576,  0.0470],\n",
      "        [ 1.0725,  0.2308],\n",
      "        [ 0.6308,  0.1055],\n",
      "        [ 0.5122, -0.2501],\n",
      "        [ 0.7178, -0.0269],\n",
      "        [ 0.7474, -0.0241],\n",
      "        [ 0.8534,  0.0887],\n",
      "        [ 0.5259,  0.1048],\n",
      "        [ 0.4809,  0.0334],\n",
      "        [ 0.7191,  0.1072],\n",
      "        [ 0.7158,  0.0781],\n",
      "        [ 0.7474,  0.1390],\n",
      "        [ 0.6764,  0.1696],\n",
      "        [ 0.4910, -0.0692],\n",
      "        [ 1.0247,  0.1134],\n",
      "        [ 0.8316,  0.2553],\n",
      "        [ 0.7938, -0.0138],\n",
      "        [ 0.9073,  0.1039],\n",
      "        [ 0.2687,  0.0725],\n",
      "        [ 1.0138,  0.1650],\n",
      "        [ 0.7055,  0.1868],\n",
      "        [ 0.6922, -0.0520],\n",
      "        [ 0.6215,  0.1002],\n",
      "        [ 0.9010,  0.0418],\n",
      "        [ 0.7910, -0.0997]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.8718, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8013,  0.0165],\n",
      "        [ 0.7721,  0.1568],\n",
      "        [ 0.5262,  0.1656],\n",
      "        [ 0.6242,  0.1874],\n",
      "        [ 0.6123,  0.1490],\n",
      "        [ 0.6285, -0.0356],\n",
      "        [ 0.7219, -0.1199],\n",
      "        [ 0.4162,  0.0638],\n",
      "        [ 0.7555, -0.1570],\n",
      "        [ 0.7468,  0.1609],\n",
      "        [ 0.7483,  0.1573],\n",
      "        [ 0.8780,  0.0436],\n",
      "        [ 0.5386,  0.1808],\n",
      "        [ 0.8515, -0.0087],\n",
      "        [ 0.9322,  0.0745],\n",
      "        [ 0.8046,  0.1786],\n",
      "        [ 0.4342,  0.3201],\n",
      "        [ 0.5046,  0.1219],\n",
      "        [ 0.6050, -0.0177],\n",
      "        [ 0.5709,  0.0630],\n",
      "        [ 0.6432, -0.1031],\n",
      "        [ 0.6741,  0.3958],\n",
      "        [ 0.7630,  0.1847],\n",
      "        [ 0.6751,  0.2565],\n",
      "        [ 0.6180,  0.1092],\n",
      "        [ 0.8566,  0.0625],\n",
      "        [ 0.8524,  0.1339],\n",
      "        [ 0.5476, -0.0033],\n",
      "        [ 0.6616,  0.1793],\n",
      "        [ 0.7525,  0.0500],\n",
      "        [ 0.6805,  0.2232],\n",
      "        [ 0.7609, -0.0425]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8792, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6995,  0.3508],\n",
      "        [ 0.5297,  0.0304],\n",
      "        [ 0.5179, -0.1625],\n",
      "        [ 0.7310,  0.0404],\n",
      "        [ 0.8278,  0.1636],\n",
      "        [ 0.7861, -0.3735],\n",
      "        [ 0.4604,  0.0344],\n",
      "        [ 0.8780,  0.0678],\n",
      "        [ 0.5673,  0.0554],\n",
      "        [ 0.6042, -0.0865],\n",
      "        [ 0.6752, -0.2315],\n",
      "        [ 0.7800, -0.1249],\n",
      "        [ 0.8246,  0.3395],\n",
      "        [ 0.8227,  0.5587],\n",
      "        [ 1.0196,  0.1002],\n",
      "        [ 0.6541,  0.1389],\n",
      "        [ 0.7558, -0.1458],\n",
      "        [ 1.0718,  0.1769],\n",
      "        [ 0.7346,  0.1927],\n",
      "        [ 0.4301,  0.1521],\n",
      "        [ 0.6890,  0.1037],\n",
      "        [ 0.6132, -0.0624],\n",
      "        [-0.3319,  0.2763],\n",
      "        [ 0.7120, -0.1170],\n",
      "        [ 0.6615, -0.0718],\n",
      "        [ 0.8881,  0.0013],\n",
      "        [ 0.7553, -0.0787],\n",
      "        [ 0.8457,  0.2250],\n",
      "        [ 0.7869,  0.2149],\n",
      "        [ 0.7026,  0.1786],\n",
      "        [ 0.9011,  0.1351],\n",
      "        [ 0.4828,  0.1048]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.7476, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 8.8422e-01,  1.1279e-01],\n",
      "        [ 1.0619e+00,  3.0449e-01],\n",
      "        [ 7.9367e-01,  7.9528e-02],\n",
      "        [ 8.0937e-01,  6.2759e-02],\n",
      "        [ 6.9115e-01,  8.5418e-02],\n",
      "        [ 8.7860e-01,  1.2424e-01],\n",
      "        [ 9.6864e-01,  2.6701e-01],\n",
      "        [ 7.2833e-01,  1.3647e-01],\n",
      "        [ 5.5202e-01,  1.8691e-01],\n",
      "        [ 6.4327e-01,  8.2863e-02],\n",
      "        [ 3.6194e-01,  1.0614e-02],\n",
      "        [ 9.0121e-01, -1.7637e-01],\n",
      "        [ 7.5481e-01, -4.1402e-02],\n",
      "        [ 6.3531e-01,  7.7084e-02],\n",
      "        [ 6.4760e-01,  8.4975e-02],\n",
      "        [ 4.5769e-01,  1.2041e-01],\n",
      "        [ 3.9445e-01,  2.2768e-01],\n",
      "        [ 7.8297e-01,  2.1574e-01],\n",
      "        [ 7.1684e-01,  2.6220e-01],\n",
      "        [ 5.8680e-01, -1.7500e-01],\n",
      "        [ 1.0855e+00,  1.7205e-01],\n",
      "        [ 5.3548e-01, -5.3422e-02],\n",
      "        [ 5.5852e-01, -1.7521e-01],\n",
      "        [ 3.5719e-01, -7.0301e-02],\n",
      "        [ 6.4960e-01, -2.7585e-03],\n",
      "        [ 7.9600e-01,  1.2453e-01],\n",
      "        [ 9.9139e-01,  1.7330e-01],\n",
      "        [ 4.5511e-01,  5.7976e-02],\n",
      "        [ 9.1291e-01,  3.8579e-04],\n",
      "        [ 8.2211e-01, -2.5331e-01],\n",
      "        [ 7.6141e-01, -2.6068e-01],\n",
      "        [ 7.4887e-01, -1.7967e-01]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(1.0148, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9686,  0.0650],\n",
      "        [ 1.0267, -0.1348],\n",
      "        [ 0.8437,  0.0165],\n",
      "        [ 0.4241,  0.1228],\n",
      "        [ 0.6873,  0.1832],\n",
      "        [ 0.4929, -0.0136],\n",
      "        [ 0.8481, -0.0099],\n",
      "        [ 0.6754,  0.1342],\n",
      "        [ 0.7276,  0.2467],\n",
      "        [ 0.7874,  0.1062],\n",
      "        [ 0.3909,  0.2538],\n",
      "        [ 0.9757, -0.0160],\n",
      "        [ 0.6836, -0.0441],\n",
      "        [ 0.4168, -0.0674],\n",
      "        [ 0.4074,  0.1522],\n",
      "        [ 1.0575, -0.0833],\n",
      "        [ 0.8751,  0.0930],\n",
      "        [ 0.7016, -0.0441],\n",
      "        [ 0.5926, -0.0384],\n",
      "        [ 0.7561,  0.4566],\n",
      "        [ 0.6022, -0.1464],\n",
      "        [ 0.5524,  0.0543],\n",
      "        [ 0.7588,  0.0850],\n",
      "        [ 0.7904,  0.0081],\n",
      "        [ 0.8044,  0.1583],\n",
      "        [ 0.9076,  0.0402],\n",
      "        [ 0.7625,  0.1816],\n",
      "        [ 0.8294, -0.0400],\n",
      "        [ 0.8949, -0.2129],\n",
      "        [ 0.6890,  0.2735],\n",
      "        [ 0.6358, -0.0162],\n",
      "        [ 1.0717,  0.2229]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.9334, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1114, -0.1638],\n",
      "        [ 0.6663, -0.0262],\n",
      "        [ 0.8716, -0.1542],\n",
      "        [ 0.8704,  0.4767],\n",
      "        [ 0.7988,  0.0932],\n",
      "        [ 0.5793,  0.2470],\n",
      "        [ 0.5803,  0.1878],\n",
      "        [ 0.9828,  0.0132],\n",
      "        [ 0.4216,  0.1455],\n",
      "        [ 0.5369,  0.2185],\n",
      "        [ 0.9622,  0.3689],\n",
      "        [ 0.7027,  0.0021],\n",
      "        [ 0.8307,  0.0089],\n",
      "        [ 0.4864, -0.0713],\n",
      "        [ 1.0567,  0.2167],\n",
      "        [ 0.2881,  0.0990],\n",
      "        [ 0.7652,  0.3147],\n",
      "        [ 0.1572,  0.1062],\n",
      "        [ 0.7299, -0.0083],\n",
      "        [ 0.7482, -0.0043],\n",
      "        [ 0.4070,  0.1844],\n",
      "        [ 0.5493,  0.0803],\n",
      "        [ 0.3532, -0.0496],\n",
      "        [ 0.5104, -0.1920],\n",
      "        [ 0.2599,  0.1624],\n",
      "        [ 0.7489,  0.1132],\n",
      "        [ 0.7353,  0.0075],\n",
      "        [ 0.9183,  0.1722],\n",
      "        [ 0.8054,  0.2692],\n",
      "        [ 0.5702, -0.0021],\n",
      "        [ 0.5001,  0.2045],\n",
      "        [ 0.6884,  0.1129]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8147, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9097, -0.1665],\n",
      "        [ 0.2608, -0.1285],\n",
      "        [ 0.3435, -0.0382],\n",
      "        [ 0.8780,  0.0509],\n",
      "        [ 0.5946,  0.2232],\n",
      "        [ 0.5684, -0.0279],\n",
      "        [ 0.4595,  0.0901],\n",
      "        [ 0.7419,  0.1195],\n",
      "        [ 0.8918,  0.3523],\n",
      "        [ 0.6077,  0.3485],\n",
      "        [ 0.7058, -0.0623],\n",
      "        [ 0.9694, -0.0329],\n",
      "        [ 0.7959,  0.2488],\n",
      "        [ 0.6545, -0.0029],\n",
      "        [ 0.5352,  0.2479],\n",
      "        [ 0.7993,  0.2118],\n",
      "        [ 0.7942,  0.0524],\n",
      "        [ 0.5202,  0.1354],\n",
      "        [ 0.7496,  0.2884],\n",
      "        [ 0.6601,  0.0285],\n",
      "        [ 0.4040, -0.1995],\n",
      "        [ 0.4445,  0.0880],\n",
      "        [ 0.5727,  0.0847],\n",
      "        [ 0.7425,  0.0682],\n",
      "        [ 0.9084,  0.0302],\n",
      "        [ 0.5330, -0.0705],\n",
      "        [ 0.3709,  0.1823],\n",
      "        [ 0.8048,  0.2171],\n",
      "        [ 0.2092, -0.0064],\n",
      "        [ 0.5985,  0.2159],\n",
      "        [ 0.8083,  0.0660],\n",
      "        [ 0.5608,  0.1218]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8180, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9032,  0.1125],\n",
      "        [ 0.6215, -0.0743],\n",
      "        [ 0.8154,  0.0767],\n",
      "        [ 0.6694,  0.0834],\n",
      "        [ 0.7782, -0.1558],\n",
      "        [ 0.6801,  0.0970],\n",
      "        [ 0.9802,  0.1758],\n",
      "        [ 0.5702, -0.0686],\n",
      "        [ 0.8559,  0.1751],\n",
      "        [ 0.8389,  0.0032],\n",
      "        [ 0.7892, -0.0611],\n",
      "        [ 0.9947,  0.3065],\n",
      "        [ 0.6708,  0.0113],\n",
      "        [ 0.9457, -0.1014],\n",
      "        [ 0.7115,  0.2631],\n",
      "        [ 0.3405, -0.3078],\n",
      "        [ 0.7314,  0.1336],\n",
      "        [ 0.2984,  0.0918],\n",
      "        [ 0.7304,  0.0394],\n",
      "        [ 0.8125,  0.0842],\n",
      "        [ 1.0614,  0.2207],\n",
      "        [ 0.6645,  0.2508],\n",
      "        [ 0.7458, -0.1715],\n",
      "        [ 0.4893, -0.0571],\n",
      "        [ 0.2491,  0.1155],\n",
      "        [ 0.8357,  0.2757],\n",
      "        [ 0.7443, -0.0970],\n",
      "        [ 0.7799,  0.0982],\n",
      "        [ 0.3685,  0.0847],\n",
      "        [ 0.4569, -0.0652],\n",
      "        [ 0.5891,  0.0734],\n",
      "        [ 0.6817,  0.0896]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(1.0326, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9582,  0.1212],\n",
      "        [ 0.8873, -0.1934],\n",
      "        [ 0.3685,  0.0672],\n",
      "        [ 0.6139, -0.0770],\n",
      "        [ 0.4390,  0.1208],\n",
      "        [ 0.5219,  0.0238],\n",
      "        [ 0.8414,  0.1001],\n",
      "        [ 0.8419,  0.1735],\n",
      "        [ 0.5096, -0.0125],\n",
      "        [ 0.6271,  0.1704],\n",
      "        [ 0.9299,  0.1080],\n",
      "        [ 0.9277,  0.2499],\n",
      "        [ 0.3296, -0.0495],\n",
      "        [ 0.4446,  0.1950],\n",
      "        [ 0.5616,  0.0755],\n",
      "        [ 1.0225,  0.1015],\n",
      "        [ 0.7723, -0.1917],\n",
      "        [ 0.7742,  0.0780],\n",
      "        [ 0.5620,  0.1088],\n",
      "        [ 0.8363,  0.0079],\n",
      "        [ 0.7756,  0.0893],\n",
      "        [ 0.5635, -0.0090],\n",
      "        [ 0.8370,  0.2504],\n",
      "        [ 0.6824,  0.1407],\n",
      "        [ 0.5582,  0.0032],\n",
      "        [ 0.5389,  0.1565],\n",
      "        [ 0.5689,  0.0153],\n",
      "        [ 0.8042, -0.0384],\n",
      "        [ 0.8375, -0.0732],\n",
      "        [ 0.8303,  0.2084],\n",
      "        [ 0.7390, -0.0182],\n",
      "        [ 0.6805,  0.0935]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.9883, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0154,  0.0540],\n",
      "        [ 1.1272, -0.2768],\n",
      "        [ 0.8259,  0.2002],\n",
      "        [ 1.0778,  0.1666],\n",
      "        [ 0.5451,  0.0104],\n",
      "        [ 0.5281, -0.0246],\n",
      "        [ 0.8180,  0.0858],\n",
      "        [ 0.4402, -0.1584],\n",
      "        [ 0.5353, -0.1856],\n",
      "        [ 0.5377, -0.0891],\n",
      "        [ 0.5019, -0.0181],\n",
      "        [ 0.6739,  0.3389],\n",
      "        [ 0.8877,  0.3220],\n",
      "        [ 0.6936,  0.0425],\n",
      "        [ 0.5720,  0.0857],\n",
      "        [ 0.9082, -0.0809],\n",
      "        [ 0.6806,  0.1451],\n",
      "        [ 0.7265,  0.0259],\n",
      "        [ 0.8149,  0.1764],\n",
      "        [ 0.8266,  0.2165],\n",
      "        [ 1.0856, -0.0125],\n",
      "        [ 0.7491,  0.2409],\n",
      "        [ 0.9826, -0.2565],\n",
      "        [ 0.4873,  0.0335],\n",
      "        [ 0.5607, -0.2586],\n",
      "        [ 0.6946,  0.0841],\n",
      "        [ 0.6564,  0.1702],\n",
      "        [ 0.4824,  0.1498],\n",
      "        [ 0.6044,  0.0845],\n",
      "        [ 0.8497,  0.0137],\n",
      "        [ 0.8875,  0.0169],\n",
      "        [ 0.4369,  0.0393]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9536, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5339,  0.0484],\n",
      "        [ 0.9101,  0.0153],\n",
      "        [ 0.5378,  0.1735],\n",
      "        [ 0.7005,  0.1764],\n",
      "        [ 0.7376,  0.0855],\n",
      "        [ 0.7359,  0.0707],\n",
      "        [ 0.9615,  0.3527],\n",
      "        [ 0.9659, -0.1025],\n",
      "        [ 0.2162, -0.1185],\n",
      "        [ 0.8028,  0.3899],\n",
      "        [ 1.1408, -0.0177],\n",
      "        [ 1.0975,  0.1608],\n",
      "        [ 0.7819, -0.1116],\n",
      "        [ 0.7100,  0.1655],\n",
      "        [ 0.5065, -0.0833],\n",
      "        [ 0.5797, -0.0105],\n",
      "        [ 0.7253, -0.3015],\n",
      "        [ 0.8139,  0.0902],\n",
      "        [ 0.6912, -0.2511],\n",
      "        [ 0.7083,  0.2911],\n",
      "        [ 0.7876,  0.0138],\n",
      "        [ 0.8330, -0.1562],\n",
      "        [ 0.9793,  0.3756],\n",
      "        [ 0.4987,  0.1907],\n",
      "        [ 0.6600, -0.1323],\n",
      "        [ 0.4464,  0.1621],\n",
      "        [ 0.4993,  0.1110],\n",
      "        [ 0.5668, -0.0202],\n",
      "        [ 0.8697,  0.0763],\n",
      "        [ 0.9609,  0.2769],\n",
      "        [ 0.7317,  0.2154],\n",
      "        [ 0.4573, -0.0272]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8842, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7079,  0.1081],\n",
      "        [ 0.7585,  0.2846],\n",
      "        [ 0.5722,  0.1760],\n",
      "        [ 0.5255,  0.2798],\n",
      "        [ 1.1620,  0.0714],\n",
      "        [ 0.9496,  0.4199],\n",
      "        [ 0.5485,  0.1158],\n",
      "        [ 0.3782, -0.1803],\n",
      "        [ 0.9518,  0.1819],\n",
      "        [ 0.6871,  0.1304],\n",
      "        [ 0.5626,  0.0222],\n",
      "        [ 0.7164, -0.0676],\n",
      "        [ 0.8158,  0.0672],\n",
      "        [ 0.4735, -0.2382],\n",
      "        [ 0.6358,  0.0421],\n",
      "        [ 0.8143,  0.0996],\n",
      "        [ 0.4717, -0.1563],\n",
      "        [ 0.7159,  0.1660],\n",
      "        [ 0.8962,  0.0909],\n",
      "        [ 0.6417,  0.0683],\n",
      "        [ 0.8201,  0.2099],\n",
      "        [ 1.1070,  0.2236],\n",
      "        [ 0.0509, -0.0541],\n",
      "        [ 0.4510, -0.1463],\n",
      "        [ 0.8360,  0.2001],\n",
      "        [ 0.5751,  0.1135],\n",
      "        [ 0.4987,  0.0316],\n",
      "        [ 0.2048, -0.0424],\n",
      "        [ 1.0889,  0.0416],\n",
      "        [ 0.7725,  0.1983],\n",
      "        [ 0.2073,  0.2317],\n",
      "        [ 0.5609, -0.1074]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(1.0103, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4830,  0.0079],\n",
      "        [ 0.5934,  0.0584],\n",
      "        [ 0.7759,  0.2017],\n",
      "        [ 0.7118,  0.1108],\n",
      "        [ 0.7679,  0.0374],\n",
      "        [ 0.6291, -0.1605],\n",
      "        [ 0.9268, -0.1918],\n",
      "        [ 0.9612, -0.0131],\n",
      "        [ 0.8088,  0.1979],\n",
      "        [ 0.8550, -0.2471],\n",
      "        [ 0.6170,  0.1782],\n",
      "        [ 0.7322,  0.2622],\n",
      "        [ 0.6669,  0.0108],\n",
      "        [ 0.9242,  0.0228],\n",
      "        [ 0.5535, -0.1039],\n",
      "        [ 0.4342, -0.1240],\n",
      "        [ 0.6444, -0.2294],\n",
      "        [ 0.6310, -0.1450],\n",
      "        [ 0.6625,  0.2727],\n",
      "        [ 0.9055,  0.1429],\n",
      "        [ 0.3555, -0.0078],\n",
      "        [ 1.1335, -0.0342],\n",
      "        [ 0.7279,  0.3538],\n",
      "        [ 0.9278, -0.2013],\n",
      "        [ 0.6929, -0.0733],\n",
      "        [ 0.7073, -0.1860],\n",
      "        [ 0.3870, -0.0543],\n",
      "        [ 0.6271,  0.1922],\n",
      "        [ 0.5793,  0.0854],\n",
      "        [ 0.7978, -0.0322],\n",
      "        [ 0.9240,  0.0521],\n",
      "        [ 0.7497,  0.0605]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.8996, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8819, -0.0782],\n",
      "        [ 0.8120,  0.2015],\n",
      "        [ 0.8280,  0.1018],\n",
      "        [ 0.5224,  0.2624],\n",
      "        [ 0.6668,  0.0711],\n",
      "        [ 0.9448,  0.0769],\n",
      "        [ 0.8869,  0.0027],\n",
      "        [ 0.7694, -0.0320],\n",
      "        [ 0.7398,  0.2512],\n",
      "        [ 0.9987,  0.0129],\n",
      "        [ 0.5731, -0.1579],\n",
      "        [ 0.6236, -0.1410],\n",
      "        [ 0.3795, -0.0044],\n",
      "        [ 0.5862, -0.1329],\n",
      "        [ 0.5953,  0.1445],\n",
      "        [ 0.7215, -0.0208],\n",
      "        [ 0.8022,  0.0858],\n",
      "        [ 0.8111,  0.1654],\n",
      "        [ 0.8531,  0.1633],\n",
      "        [ 0.8908,  0.0270],\n",
      "        [ 0.6283,  0.1683],\n",
      "        [ 0.9584,  0.1934],\n",
      "        [ 0.3836, -0.2876],\n",
      "        [ 0.7911, -0.0098],\n",
      "        [ 0.8683,  0.1271],\n",
      "        [ 0.7334, -0.0914],\n",
      "        [ 0.7618,  0.2448],\n",
      "        [ 0.7044,  0.0102],\n",
      "        [ 0.7280, -0.0203],\n",
      "        [ 0.8964, -0.0135],\n",
      "        [ 0.6112,  0.2077],\n",
      "        [ 0.6518,  0.0358]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8738, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9007,  0.1299],\n",
      "        [ 0.6878, -0.1115],\n",
      "        [ 0.9328,  0.3775],\n",
      "        [ 0.6410, -0.0462],\n",
      "        [ 0.9385,  0.0411],\n",
      "        [ 0.6634,  0.1790],\n",
      "        [ 0.5070, -0.0733],\n",
      "        [ 1.0229,  0.3615],\n",
      "        [ 0.7027,  0.1114],\n",
      "        [ 1.0080,  0.0289],\n",
      "        [ 0.0659,  0.1441],\n",
      "        [ 0.8591,  0.0345],\n",
      "        [ 0.9936,  0.0172],\n",
      "        [ 0.6087,  0.0635],\n",
      "        [ 0.7861, -0.1498],\n",
      "        [ 0.6279,  0.1760],\n",
      "        [ 0.6968, -0.2021],\n",
      "        [ 1.1132,  0.1837],\n",
      "        [ 0.7222,  0.1034],\n",
      "        [ 0.6795,  0.0861],\n",
      "        [ 0.6562,  0.1641],\n",
      "        [ 0.6371,  0.0375],\n",
      "        [ 0.7289,  0.1074],\n",
      "        [ 0.7399, -0.0846],\n",
      "        [ 1.0539,  0.1611],\n",
      "        [ 0.9031,  0.1483],\n",
      "        [ 0.6873,  0.4562],\n",
      "        [ 0.5351,  0.1408],\n",
      "        [ 0.3590,  0.0757],\n",
      "        [ 1.0173,  0.3359],\n",
      "        [ 0.7141,  0.0415],\n",
      "        [ 0.7904,  0.1523]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8183, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7558, -0.0839],\n",
      "        [ 1.0253,  0.0499],\n",
      "        [ 0.7521,  0.0285],\n",
      "        [ 0.9386, -0.1314],\n",
      "        [ 0.3832, -0.0482],\n",
      "        [ 0.7507,  0.1157],\n",
      "        [ 0.4850,  0.1843],\n",
      "        [ 1.0767,  0.1539],\n",
      "        [ 0.8382,  0.1269],\n",
      "        [ 0.6300,  0.1582],\n",
      "        [ 0.7179,  0.2326],\n",
      "        [ 0.6941,  0.1909],\n",
      "        [ 0.6323, -0.0539],\n",
      "        [ 1.0284,  0.1961],\n",
      "        [ 0.7723,  0.0926],\n",
      "        [ 0.4862,  0.1915],\n",
      "        [ 0.7445,  0.2847],\n",
      "        [ 0.8245,  0.1432],\n",
      "        [ 0.5886,  0.2469],\n",
      "        [ 0.5849, -0.1638],\n",
      "        [ 0.8300,  0.1861],\n",
      "        [ 0.8916,  0.1071],\n",
      "        [ 0.8289, -0.2394],\n",
      "        [ 0.8400,  0.0740],\n",
      "        [ 0.4614,  0.1590],\n",
      "        [ 1.1200,  0.2520],\n",
      "        [ 0.4442, -0.0049],\n",
      "        [ 0.9600,  0.0837],\n",
      "        [ 0.4888,  0.0126],\n",
      "        [ 0.6853, -0.1862],\n",
      "        [ 0.5894,  0.3293],\n",
      "        [ 0.8542,  0.0146]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8981, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8087,  0.1402],\n",
      "        [ 0.8680, -0.1308],\n",
      "        [ 0.5247, -0.4312],\n",
      "        [ 0.6000, -0.2579],\n",
      "        [ 0.7032,  0.0038],\n",
      "        [ 0.6553,  0.1708],\n",
      "        [ 0.6671, -0.0104],\n",
      "        [ 0.8258,  0.1381],\n",
      "        [ 0.6032, -0.1134],\n",
      "        [ 1.0623,  0.2217],\n",
      "        [ 0.6858, -0.0677],\n",
      "        [ 0.6366,  0.0205],\n",
      "        [ 0.8362,  0.2600],\n",
      "        [ 0.7270,  0.3508],\n",
      "        [ 0.9199,  0.2808],\n",
      "        [ 1.1136,  0.0950],\n",
      "        [ 0.5018,  0.2173],\n",
      "        [ 0.7238,  0.1337],\n",
      "        [ 0.6488,  0.1465],\n",
      "        [ 0.7333,  0.1028],\n",
      "        [ 0.6254,  0.0691],\n",
      "        [ 0.7726,  0.1274],\n",
      "        [ 0.8704,  0.0952],\n",
      "        [ 0.3740,  0.0829],\n",
      "        [ 0.4268,  0.1467],\n",
      "        [ 0.8442, -0.0683],\n",
      "        [ 0.8794, -0.1060],\n",
      "        [ 0.4263, -0.3812],\n",
      "        [ 0.5277,  0.0315],\n",
      "        [ 0.5633,  0.2078],\n",
      "        [ 0.7160,  0.1397],\n",
      "        [ 0.6699,  0.1207]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.8977, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7326,  0.0645],\n",
      "        [ 0.5904, -0.0730],\n",
      "        [ 0.9431,  0.0394],\n",
      "        [ 0.7694,  0.1983],\n",
      "        [ 0.6283,  0.0110],\n",
      "        [ 0.9245,  0.1878],\n",
      "        [ 0.7841,  0.3142],\n",
      "        [ 0.9582,  0.1728],\n",
      "        [ 0.8566,  0.2043],\n",
      "        [ 0.7538,  0.0821],\n",
      "        [ 0.7804,  0.1129],\n",
      "        [ 0.9259,  0.2114],\n",
      "        [ 0.6083,  0.1056],\n",
      "        [ 0.6603,  0.1815],\n",
      "        [ 0.7111,  0.1846],\n",
      "        [ 0.7097, -0.0682],\n",
      "        [ 0.9133,  0.2671],\n",
      "        [ 0.9168, -0.0834],\n",
      "        [ 0.9165,  0.0192],\n",
      "        [ 0.9025,  0.3314],\n",
      "        [ 0.9935,  0.1144],\n",
      "        [ 0.3444,  0.0916],\n",
      "        [ 0.7070,  0.0920],\n",
      "        [ 0.4855, -0.0426],\n",
      "        [ 0.4975,  0.1174],\n",
      "        [ 0.8218,  0.2108],\n",
      "        [ 0.6838,  0.2916],\n",
      "        [ 0.8603, -0.0818],\n",
      "        [ 0.8318, -0.1513],\n",
      "        [ 0.6936, -0.0753],\n",
      "        [ 0.5416, -0.1034],\n",
      "        [ 0.6496, -0.0216]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8529, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0927e+00, -2.3854e-02],\n",
      "        [ 5.9672e-01,  1.0621e-01],\n",
      "        [ 9.3928e-01, -1.9805e-02],\n",
      "        [ 7.3418e-01,  1.1220e-01],\n",
      "        [ 7.0571e-01,  9.2481e-02],\n",
      "        [ 5.9601e-01, -1.9877e-02],\n",
      "        [ 9.2794e-01, -1.3025e-01],\n",
      "        [ 8.1046e-01,  7.1923e-02],\n",
      "        [ 6.8023e-01,  1.6520e-01],\n",
      "        [ 1.0071e+00,  5.4522e-02],\n",
      "        [ 9.3040e-01,  1.6067e-01],\n",
      "        [ 7.9358e-01,  1.4260e-01],\n",
      "        [ 6.8470e-01,  1.7067e-02],\n",
      "        [ 6.7027e-01,  1.9682e-01],\n",
      "        [ 4.7631e-01,  2.9513e-01],\n",
      "        [ 1.1657e+00,  3.5896e-02],\n",
      "        [ 6.7525e-01,  6.2264e-02],\n",
      "        [ 7.9227e-01,  1.5988e-01],\n",
      "        [ 5.5666e-01,  5.9290e-02],\n",
      "        [ 7.5329e-01,  9.3747e-02],\n",
      "        [ 6.4643e-01,  1.5466e-01],\n",
      "        [ 8.6742e-01,  4.9877e-02],\n",
      "        [ 8.9866e-01,  4.6939e-02],\n",
      "        [ 5.2025e-01,  2.7047e-01],\n",
      "        [ 8.3605e-01,  7.6984e-02],\n",
      "        [ 5.2803e-01, -2.4614e-03],\n",
      "        [ 3.3318e-01,  5.7033e-02],\n",
      "        [ 5.9818e-01,  3.0490e-01],\n",
      "        [ 6.1474e-01,  1.1612e-01],\n",
      "        [ 9.1561e-01,  1.0063e-01],\n",
      "        [ 4.6585e-01, -2.0224e-03],\n",
      "        [ 6.8185e-01, -6.6742e-05]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8872, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8257,  0.3472],\n",
      "        [ 0.8293,  0.1707],\n",
      "        [ 0.8588,  0.1887],\n",
      "        [ 0.8051, -0.0508],\n",
      "        [ 0.5535, -0.0431],\n",
      "        [ 0.6539,  0.2323],\n",
      "        [ 0.9460,  0.2020],\n",
      "        [ 0.8698,  0.0580],\n",
      "        [ 0.6778,  0.1397],\n",
      "        [ 0.7899,  0.1749],\n",
      "        [ 0.6833,  0.0764],\n",
      "        [ 0.9510,  0.1838],\n",
      "        [ 0.6835,  0.1272],\n",
      "        [ 0.3639,  0.0744],\n",
      "        [ 1.0442,  0.2198],\n",
      "        [ 0.7877,  0.1178],\n",
      "        [ 0.4466, -0.0190],\n",
      "        [ 0.9799,  0.1299],\n",
      "        [ 0.7402, -0.0665],\n",
      "        [ 0.6330,  0.2776],\n",
      "        [ 0.5948,  0.2147],\n",
      "        [ 0.8174,  0.0991],\n",
      "        [ 0.9194,  0.3859],\n",
      "        [ 1.0135,  0.3188],\n",
      "        [ 0.8017, -0.0212],\n",
      "        [ 0.2191,  0.0763],\n",
      "        [ 0.7399,  0.2429],\n",
      "        [ 0.4061,  0.1079],\n",
      "        [ 0.6090,  0.0829],\n",
      "        [ 0.6401,  0.0456],\n",
      "        [ 0.5000,  0.2314],\n",
      "        [ 0.3982,  0.0911]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8097, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7582,  0.0986],\n",
      "        [ 0.5686, -0.2622],\n",
      "        [ 0.6710, -0.0896],\n",
      "        [ 0.5933, -0.0776],\n",
      "        [ 0.6768,  0.1914],\n",
      "        [ 0.4714,  0.0694],\n",
      "        [ 0.4086,  0.0392],\n",
      "        [ 0.5848,  0.0786],\n",
      "        [ 0.5000,  0.1385],\n",
      "        [ 0.9514,  0.2604],\n",
      "        [ 0.4924,  0.1489],\n",
      "        [ 0.7837,  0.1129],\n",
      "        [ 0.7495,  0.4181],\n",
      "        [ 0.5961,  0.1390],\n",
      "        [ 0.6161,  0.1647],\n",
      "        [ 0.7774,  0.1096],\n",
      "        [ 1.0742,  0.1695],\n",
      "        [ 0.8075, -0.1210],\n",
      "        [ 0.5888,  0.1756],\n",
      "        [ 0.6786,  0.0318],\n",
      "        [ 0.7114, -0.0299],\n",
      "        [ 0.8622,  0.1785],\n",
      "        [ 0.5623,  0.2644],\n",
      "        [ 0.5834,  0.2239],\n",
      "        [ 0.4700,  0.2047],\n",
      "        [ 1.1837,  0.0044],\n",
      "        [ 0.9019,  0.1915],\n",
      "        [ 0.8428, -0.0082],\n",
      "        [ 0.7634, -0.1253],\n",
      "        [ 1.1503, -0.0423],\n",
      "        [ 0.9383,  0.1344],\n",
      "        [ 0.7239,  0.1765]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.9047, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.1344, -0.0391],\n",
      "        [ 0.8676, -0.0169],\n",
      "        [ 0.4475, -0.0532],\n",
      "        [ 0.6975, -0.0323],\n",
      "        [ 1.1248,  0.1675],\n",
      "        [ 0.9191,  0.0775],\n",
      "        [ 0.8485, -0.1859],\n",
      "        [ 0.5359, -0.1683],\n",
      "        [ 1.0996,  0.0960],\n",
      "        [ 0.8187,  0.2764],\n",
      "        [ 0.5448,  0.1096],\n",
      "        [ 0.5970, -0.2303],\n",
      "        [ 0.9162,  0.2128],\n",
      "        [ 0.6160, -0.0662],\n",
      "        [ 0.8021,  0.0839],\n",
      "        [ 0.7971,  0.2322],\n",
      "        [ 0.6144, -0.0072],\n",
      "        [ 0.5029,  0.1405],\n",
      "        [ 0.8243, -0.0319],\n",
      "        [ 0.8529,  0.0143],\n",
      "        [ 0.4625,  0.1268],\n",
      "        [ 1.0049,  0.2263],\n",
      "        [ 0.5204,  0.0750],\n",
      "        [ 0.6829, -0.0407],\n",
      "        [ 0.9577,  0.0677],\n",
      "        [ 0.5276,  0.1717],\n",
      "        [ 0.3977, -0.1486],\n",
      "        [ 0.6895,  0.1089],\n",
      "        [ 1.0385,  0.1962],\n",
      "        [ 0.8625,  0.0373],\n",
      "        [ 0.8402,  0.0744],\n",
      "        [ 1.0444,  0.0672]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9303, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8147,  0.0180],\n",
      "        [ 0.7641,  0.0790],\n",
      "        [ 0.6601,  0.2959],\n",
      "        [ 0.8077,  0.1030],\n",
      "        [ 0.9012,  0.0222],\n",
      "        [ 0.8050,  0.2852],\n",
      "        [ 0.8784, -0.2012],\n",
      "        [ 0.8246,  0.4833],\n",
      "        [ 0.4882, -0.2137],\n",
      "        [ 0.5972,  0.1202],\n",
      "        [ 0.8838, -0.0528],\n",
      "        [ 0.1834, -0.0397],\n",
      "        [ 0.9210,  0.0795],\n",
      "        [ 0.5974, -0.0014],\n",
      "        [ 0.4538,  0.2611],\n",
      "        [ 0.3943,  0.0797],\n",
      "        [ 0.4487, -0.0284],\n",
      "        [ 0.8954, -0.1289],\n",
      "        [ 1.0303,  0.1952],\n",
      "        [ 0.7115,  0.1256],\n",
      "        [ 0.4082, -0.0850],\n",
      "        [ 0.6676,  0.1343],\n",
      "        [ 0.9265,  0.0496],\n",
      "        [ 0.8078,  0.3407],\n",
      "        [ 0.5993, -0.1238],\n",
      "        [ 0.5491,  0.1355],\n",
      "        [ 0.8275,  0.2568],\n",
      "        [ 0.7130,  0.2327],\n",
      "        [ 0.7146,  0.0247],\n",
      "        [ 0.6344, -0.2143],\n",
      "        [ 0.8092,  0.3416],\n",
      "        [ 1.0654, -0.0491]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8935, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5326,  0.1193],\n",
      "        [ 0.6665,  0.1452],\n",
      "        [ 0.7341, -0.0563],\n",
      "        [ 0.7121,  0.0460],\n",
      "        [ 0.4015,  0.1200],\n",
      "        [ 0.9743,  0.2029],\n",
      "        [ 1.1641, -0.2423],\n",
      "        [ 0.7853,  0.1452],\n",
      "        [ 1.2480, -0.0095],\n",
      "        [ 0.7700,  0.2281],\n",
      "        [ 0.6162,  0.0473],\n",
      "        [ 0.5859,  0.0803],\n",
      "        [ 0.4768,  0.2129],\n",
      "        [ 0.9212,  0.1919],\n",
      "        [ 0.9639,  0.1506],\n",
      "        [ 1.1595,  0.2691],\n",
      "        [ 0.8391,  0.2798],\n",
      "        [ 1.1065,  0.1459],\n",
      "        [ 0.6883,  0.2103],\n",
      "        [ 1.0751,  0.1570],\n",
      "        [ 0.6434, -0.0930],\n",
      "        [ 0.8192,  0.0259],\n",
      "        [ 0.6675, -0.0076],\n",
      "        [ 0.7188,  0.0282],\n",
      "        [ 0.5637,  0.0627],\n",
      "        [ 1.0954,  0.2230],\n",
      "        [ 0.7209,  0.2107],\n",
      "        [ 0.7308,  0.0211],\n",
      "        [ 0.5359,  0.2802],\n",
      "        [ 0.6856,  0.0358],\n",
      "        [ 0.9392,  0.0871],\n",
      "        [ 0.9172,  0.1314]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8280, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 8.2626e-01, -3.0653e-01],\n",
      "        [ 9.5157e-01,  1.9225e-01],\n",
      "        [ 4.9251e-01,  9.1727e-02],\n",
      "        [ 4.1672e-01,  4.4724e-04],\n",
      "        [ 6.3576e-01,  1.4378e-01],\n",
      "        [ 6.3186e-01,  5.6306e-02],\n",
      "        [ 8.0111e-01,  1.3421e-01],\n",
      "        [ 8.5822e-01,  9.3572e-02],\n",
      "        [ 7.4113e-01,  1.4567e-01],\n",
      "        [ 6.8786e-01, -1.0506e-02],\n",
      "        [ 5.6539e-01, -1.8634e-01],\n",
      "        [ 7.7184e-01, -2.3668e-02],\n",
      "        [ 6.9376e-01, -1.9607e-01],\n",
      "        [ 5.0473e-01,  2.3005e-01],\n",
      "        [ 5.9178e-01, -1.5667e-01],\n",
      "        [ 7.6288e-01,  1.1179e-01],\n",
      "        [ 6.9348e-01,  3.0032e-01],\n",
      "        [ 5.7622e-01,  1.6031e-01],\n",
      "        [ 3.7936e-01, -2.2248e-02],\n",
      "        [ 1.0015e+00,  1.6740e-01],\n",
      "        [ 5.2550e-01,  3.1215e-01],\n",
      "        [ 7.6834e-01,  1.5286e-01],\n",
      "        [ 6.9268e-01,  7.3907e-02],\n",
      "        [ 5.8588e-01,  6.6812e-02],\n",
      "        [ 7.0491e-01,  3.4203e-02],\n",
      "        [ 5.0956e-01, -6.5420e-02],\n",
      "        [ 6.6173e-01,  1.9649e-03],\n",
      "        [ 8.4789e-01, -9.6466e-03],\n",
      "        [ 5.5247e-01, -4.5225e-02],\n",
      "        [ 7.3890e-01,  1.3484e-01],\n",
      "        [ 4.9557e-01, -7.9683e-02],\n",
      "        [ 9.0874e-01,  3.1795e-01]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.8769, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0215,  0.1907],\n",
      "        [ 0.7611,  0.0351],\n",
      "        [ 0.6846,  0.3695],\n",
      "        [ 0.5846,  0.3336],\n",
      "        [ 0.5906,  0.1491],\n",
      "        [ 0.9230,  0.1697],\n",
      "        [ 0.9080,  0.3431],\n",
      "        [ 0.7049,  0.1708],\n",
      "        [ 0.5078,  0.0797],\n",
      "        [ 0.7085, -0.0323],\n",
      "        [ 0.3937,  0.0588],\n",
      "        [ 0.5708,  0.1493],\n",
      "        [ 0.5406,  0.1333],\n",
      "        [ 0.5496, -0.0269],\n",
      "        [ 0.4111, -0.2115],\n",
      "        [ 0.7018,  0.0912],\n",
      "        [ 0.6919,  0.0884],\n",
      "        [ 0.7805, -0.0269],\n",
      "        [ 0.5058, -0.0838],\n",
      "        [ 0.6228,  0.1619],\n",
      "        [ 0.7654, -0.1163],\n",
      "        [ 0.6120,  0.0850],\n",
      "        [ 0.6854, -0.2262],\n",
      "        [ 0.8543, -0.1871],\n",
      "        [ 0.6340, -0.1378],\n",
      "        [ 0.7754, -0.0482],\n",
      "        [ 0.8516,  0.1337],\n",
      "        [ 0.5423, -0.0164],\n",
      "        [ 0.8214,  0.0528],\n",
      "        [ 0.6096,  0.0728],\n",
      "        [ 0.5461, -0.0691],\n",
      "        [ 1.0150,  0.1890]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8845, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9801,  0.0026],\n",
      "        [ 0.7089,  0.2185],\n",
      "        [ 0.3820,  0.0650],\n",
      "        [ 0.9779,  0.0480],\n",
      "        [ 0.9261, -0.1119],\n",
      "        [ 0.7152,  0.1817],\n",
      "        [ 0.9017,  0.1037],\n",
      "        [ 0.6970,  0.1805],\n",
      "        [ 0.9409,  0.1625],\n",
      "        [ 0.7061, -0.1232],\n",
      "        [ 0.7423,  0.0149],\n",
      "        [ 0.7333,  0.0432],\n",
      "        [ 0.7225,  0.0088],\n",
      "        [ 0.7130, -0.0292],\n",
      "        [ 0.8774,  0.3547],\n",
      "        [ 1.0151,  0.0553],\n",
      "        [ 0.4105,  0.3182],\n",
      "        [ 0.7336, -0.1017],\n",
      "        [ 0.6931, -0.0379],\n",
      "        [ 0.5397,  0.0646],\n",
      "        [ 0.7945, -0.0212],\n",
      "        [ 1.0267,  0.2411],\n",
      "        [ 0.8818,  0.1554],\n",
      "        [ 0.9729,  0.1920],\n",
      "        [ 0.5586, -0.1035],\n",
      "        [ 0.5827, -0.0593],\n",
      "        [ 0.8978,  0.3600],\n",
      "        [ 0.4664, -0.1150],\n",
      "        [ 0.8732,  0.0029],\n",
      "        [ 0.8264,  0.0952],\n",
      "        [ 1.0686,  0.1585],\n",
      "        [ 0.6828,  0.0806]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8892, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7724,  0.2255],\n",
      "        [ 0.9244, -0.0147],\n",
      "        [ 0.7807,  0.1935],\n",
      "        [ 0.8025,  0.1646],\n",
      "        [ 0.7083,  0.0229],\n",
      "        [ 1.0276,  0.1191],\n",
      "        [ 0.7632, -0.1765],\n",
      "        [ 0.4356, -0.1838],\n",
      "        [ 0.7980,  0.3500],\n",
      "        [ 0.9602, -0.0650],\n",
      "        [ 0.4603, -0.2999],\n",
      "        [ 0.8169, -0.0600],\n",
      "        [ 0.6937,  0.0564],\n",
      "        [ 0.7901,  0.1498],\n",
      "        [ 1.1376,  0.0772],\n",
      "        [ 0.6555, -0.0607],\n",
      "        [ 1.0524,  0.0654],\n",
      "        [ 0.9603,  0.1482],\n",
      "        [ 0.5282,  0.0538],\n",
      "        [ 0.7681,  0.1161],\n",
      "        [ 0.6993,  0.0811],\n",
      "        [-0.0586,  0.1067],\n",
      "        [ 0.8411,  0.0360],\n",
      "        [ 0.7703,  0.1587],\n",
      "        [ 0.7035,  0.0121],\n",
      "        [ 0.6839,  0.0441],\n",
      "        [ 0.6371,  0.2188],\n",
      "        [ 0.8326,  0.2177],\n",
      "        [ 0.7560,  0.1585],\n",
      "        [ 0.9176,  0.1680],\n",
      "        [ 0.6511,  0.1368],\n",
      "        [ 0.6851,  0.0183]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8759, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8435,  0.3109],\n",
      "        [ 0.5416,  0.0925],\n",
      "        [ 0.6654, -0.0223],\n",
      "        [ 0.7201,  0.1336],\n",
      "        [ 0.8808, -0.0126],\n",
      "        [ 0.7130,  0.0793],\n",
      "        [ 0.8489,  0.1622],\n",
      "        [ 0.9433,  0.0955],\n",
      "        [ 0.9035, -0.1316],\n",
      "        [ 0.7352, -0.0533],\n",
      "        [ 0.5719,  0.0835],\n",
      "        [ 0.3810,  0.1838],\n",
      "        [ 0.7073,  0.0801],\n",
      "        [ 0.5517,  0.1851],\n",
      "        [ 0.6442,  0.2024],\n",
      "        [ 0.8431,  0.1662],\n",
      "        [ 1.0259, -0.0580],\n",
      "        [ 0.8748,  0.0743],\n",
      "        [ 0.6421, -0.0672],\n",
      "        [ 0.8249, -0.0188],\n",
      "        [ 0.5488, -0.0426],\n",
      "        [ 0.7783, -0.0578],\n",
      "        [ 0.8447,  0.2759],\n",
      "        [ 0.8827, -0.0725],\n",
      "        [ 0.3286,  0.1138],\n",
      "        [ 0.7550,  0.2161],\n",
      "        [ 0.6689, -0.0467],\n",
      "        [ 1.0047, -0.0388],\n",
      "        [ 0.7556,  0.2103],\n",
      "        [ 0.8581,  0.4146],\n",
      "        [ 0.6158, -0.1545],\n",
      "        [ 1.0246,  0.1446]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.9327, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5114,  0.0834],\n",
      "        [ 0.7375,  0.0838],\n",
      "        [ 0.7553,  0.1489],\n",
      "        [ 1.1158,  0.2019],\n",
      "        [ 0.6253, -0.0620],\n",
      "        [ 0.7816,  0.0367],\n",
      "        [ 0.6240, -0.1125],\n",
      "        [ 0.7310,  0.0408],\n",
      "        [ 0.6864, -0.0400],\n",
      "        [ 0.7638, -0.0423],\n",
      "        [ 0.8992,  0.2960],\n",
      "        [ 0.6337,  0.0794],\n",
      "        [ 0.6217,  0.0470],\n",
      "        [ 0.6262,  0.4199],\n",
      "        [ 0.8973,  0.1524],\n",
      "        [ 0.5611, -0.0112],\n",
      "        [ 0.6637,  0.1645],\n",
      "        [ 0.3126, -0.0500],\n",
      "        [ 0.8113,  0.1330],\n",
      "        [ 0.6679,  0.1955],\n",
      "        [ 0.8766, -0.0538],\n",
      "        [ 0.5556,  0.2334],\n",
      "        [ 0.7177,  0.1736],\n",
      "        [ 0.5080,  0.0032],\n",
      "        [ 0.5215,  0.0721],\n",
      "        [ 1.0290, -0.0191],\n",
      "        [ 0.6493, -0.0429],\n",
      "        [ 0.8098, -0.1136],\n",
      "        [ 0.6378,  0.0082],\n",
      "        [ 0.4444, -0.0251],\n",
      "        [ 0.7029,  0.0144],\n",
      "        [ 0.6549,  0.2135]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8677, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7129, -0.3588],\n",
      "        [ 0.6628, -0.0789],\n",
      "        [ 0.6798,  0.0750],\n",
      "        [ 0.7374,  0.3125],\n",
      "        [ 1.0319, -0.1179],\n",
      "        [ 0.3992,  0.0770],\n",
      "        [ 0.8106,  0.2045],\n",
      "        [ 0.8175, -0.0653],\n",
      "        [ 0.5575, -0.0418],\n",
      "        [ 0.8469,  0.0281],\n",
      "        [ 0.8422, -0.1966],\n",
      "        [ 0.6203,  0.1321],\n",
      "        [ 0.6778,  0.0775],\n",
      "        [ 0.6117,  0.1435],\n",
      "        [ 0.6486,  0.0735],\n",
      "        [ 0.5961, -0.1945],\n",
      "        [ 0.7879,  0.1580],\n",
      "        [ 0.6748, -0.2780],\n",
      "        [ 0.7740,  0.0402],\n",
      "        [ 0.6820,  0.2747],\n",
      "        [ 0.4655,  0.0046],\n",
      "        [ 0.6930, -0.0424],\n",
      "        [ 0.7399,  0.1733],\n",
      "        [ 0.5360,  0.1880],\n",
      "        [ 0.5892, -0.2997],\n",
      "        [ 0.3436, -0.1342],\n",
      "        [ 0.7098,  0.0547],\n",
      "        [ 0.7052, -0.1881],\n",
      "        [ 0.9577,  0.0251],\n",
      "        [ 0.6311,  0.0727],\n",
      "        [ 0.4979, -0.2067],\n",
      "        [ 0.3420,  0.2490]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9105, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 9.3662e-01,  1.2802e-01],\n",
      "        [ 5.2392e-01,  2.1494e-01],\n",
      "        [ 8.3480e-01, -3.1000e-02],\n",
      "        [ 3.7353e-01,  5.3784e-02],\n",
      "        [ 9.1282e-01, -7.6267e-02],\n",
      "        [ 8.4190e-01,  1.2106e-01],\n",
      "        [ 9.9257e-01, -3.3822e-03],\n",
      "        [ 9.0684e-01,  1.3762e-01],\n",
      "        [ 5.9797e-01, -8.3387e-05],\n",
      "        [ 5.1085e-01, -8.2735e-02],\n",
      "        [ 6.9761e-01,  1.0268e-01],\n",
      "        [ 5.8323e-01, -2.8799e-01],\n",
      "        [ 4.9764e-01,  3.1884e-02],\n",
      "        [ 8.3807e-01,  1.6631e-01],\n",
      "        [ 6.3123e-01,  1.5838e-01],\n",
      "        [ 5.2851e-01,  1.5877e-01],\n",
      "        [ 8.1293e-01,  1.6277e-01],\n",
      "        [ 8.9604e-01, -1.0661e-01],\n",
      "        [ 7.9991e-01,  9.7489e-02],\n",
      "        [ 7.8131e-01, -1.1301e-01],\n",
      "        [ 5.4719e-01,  7.8462e-03],\n",
      "        [ 6.2587e-01,  1.1966e-01],\n",
      "        [ 9.4755e-01,  2.3336e-01],\n",
      "        [ 7.2842e-01,  2.2387e-02],\n",
      "        [ 8.0344e-01,  1.1473e-01],\n",
      "        [ 5.9949e-01, -3.1129e-03],\n",
      "        [ 7.7154e-01,  2.1143e-02],\n",
      "        [ 1.1206e+00,  5.6652e-02],\n",
      "        [ 5.9507e-01, -1.9118e-01],\n",
      "        [ 9.5966e-01,  1.1820e-01],\n",
      "        [ 7.8954e-01,  2.5077e-01],\n",
      "        [ 8.9699e-01, -2.5533e-02]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9064, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9099,  0.2959],\n",
      "        [ 0.6976, -0.0250],\n",
      "        [ 0.5257,  0.3150],\n",
      "        [ 0.1516, -0.0027],\n",
      "        [ 0.4832,  0.1298],\n",
      "        [ 0.8213, -0.0922],\n",
      "        [ 0.5256,  0.1924],\n",
      "        [ 0.8755,  0.1831],\n",
      "        [ 0.6076,  0.0486],\n",
      "        [ 1.2085,  0.1989],\n",
      "        [ 0.6648,  0.1249],\n",
      "        [ 0.6991,  0.2539],\n",
      "        [ 0.7284,  0.0606],\n",
      "        [ 0.7727, -0.1992],\n",
      "        [ 0.8887,  0.0703],\n",
      "        [ 0.2769,  0.1165],\n",
      "        [ 1.0822,  0.0322],\n",
      "        [ 0.6870,  0.0130],\n",
      "        [ 0.7797,  0.2128],\n",
      "        [ 0.8571,  0.1813],\n",
      "        [ 0.4604,  0.2834],\n",
      "        [ 0.7806,  0.1107],\n",
      "        [ 0.5979,  0.2383],\n",
      "        [ 0.7926,  0.3590],\n",
      "        [ 0.8676,  0.0359],\n",
      "        [ 0.1496, -0.0556],\n",
      "        [ 0.6376, -0.1874],\n",
      "        [ 0.9512,  0.0985],\n",
      "        [ 0.4697, -0.0801],\n",
      "        [ 0.9622,  0.1043],\n",
      "        [ 0.6903,  0.0781],\n",
      "        [ 0.5860,  0.2801]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.8177, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6116, -0.0603],\n",
      "        [ 0.9753,  0.1734],\n",
      "        [ 0.6653, -0.0380],\n",
      "        [ 0.6374, -0.0123],\n",
      "        [ 0.8856, -0.0035],\n",
      "        [ 0.6688, -0.0446],\n",
      "        [ 0.5746, -0.1724],\n",
      "        [ 0.8455,  0.2885],\n",
      "        [ 1.0047, -0.0814],\n",
      "        [ 0.6539,  0.3154],\n",
      "        [ 0.4657,  0.0521],\n",
      "        [ 0.6903,  0.1643],\n",
      "        [ 0.8759,  0.0350],\n",
      "        [ 0.5756,  0.1649],\n",
      "        [ 0.4734,  0.0949],\n",
      "        [ 0.5324,  0.1271],\n",
      "        [ 0.6762,  0.1318],\n",
      "        [ 0.6295,  0.2842],\n",
      "        [ 0.7637, -0.0720],\n",
      "        [ 0.6279, -0.1423],\n",
      "        [ 0.9809, -0.0191],\n",
      "        [ 0.9209,  0.2414],\n",
      "        [ 0.7446,  0.1233],\n",
      "        [ 0.8373,  0.3265],\n",
      "        [ 0.7542,  0.2114],\n",
      "        [ 0.8230, -0.2252],\n",
      "        [ 0.6766,  0.1068],\n",
      "        [ 0.7402,  0.0147],\n",
      "        [ 0.7733,  0.0660],\n",
      "        [ 0.6913,  0.0741],\n",
      "        [ 0.5876,  0.0830],\n",
      "        [ 0.6409,  0.0867]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9633, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2726,  0.0828],\n",
      "        [ 0.7967,  0.0507],\n",
      "        [ 0.6163,  0.1942],\n",
      "        [ 0.9622, -0.0287],\n",
      "        [ 0.7168,  0.2851],\n",
      "        [ 0.8487,  0.0141],\n",
      "        [ 0.9097,  0.0927],\n",
      "        [ 0.9335, -0.1533],\n",
      "        [ 0.6290, -0.0449],\n",
      "        [ 1.1219,  0.0737],\n",
      "        [ 0.6750,  0.1316],\n",
      "        [ 0.9028,  0.0861],\n",
      "        [ 0.3522, -0.2311],\n",
      "        [ 1.0255,  0.0881],\n",
      "        [ 0.6279,  0.1965],\n",
      "        [ 0.7044, -0.0798],\n",
      "        [ 0.7665,  0.4009],\n",
      "        [ 0.5305, -0.0460],\n",
      "        [ 0.6825,  0.1855],\n",
      "        [ 0.6991, -0.3066],\n",
      "        [ 0.7342,  0.1413],\n",
      "        [ 0.8537, -0.0625],\n",
      "        [ 0.6432,  0.2131],\n",
      "        [ 0.5491,  0.0026],\n",
      "        [ 0.8638,  0.0331],\n",
      "        [ 0.8422,  0.0195],\n",
      "        [ 0.7507,  0.1622],\n",
      "        [ 1.0583,  0.2171],\n",
      "        [ 0.2509, -0.0430],\n",
      "        [ 0.6276, -0.0121],\n",
      "        [ 0.7932, -0.1696],\n",
      "        [ 0.6649,  0.2054]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.7984, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0243,  0.2072],\n",
      "        [ 0.8914, -0.0945],\n",
      "        [ 0.4835, -0.0069],\n",
      "        [ 0.4860, -0.1913],\n",
      "        [ 0.7882,  0.0671],\n",
      "        [ 0.9574,  0.0843],\n",
      "        [ 0.8921,  0.1195],\n",
      "        [ 0.6863,  0.2025],\n",
      "        [ 0.7967, -0.0473],\n",
      "        [ 0.8000,  0.1972],\n",
      "        [ 0.8404, -0.1085],\n",
      "        [ 0.8821,  0.1221],\n",
      "        [ 0.3430, -0.2888],\n",
      "        [ 0.5216, -0.0043],\n",
      "        [ 1.0034, -0.0645],\n",
      "        [ 0.8069,  0.2299],\n",
      "        [ 0.7704, -0.0629],\n",
      "        [ 0.9406,  0.0337],\n",
      "        [ 0.6020,  0.1229],\n",
      "        [ 0.7246,  0.0697],\n",
      "        [ 1.0096,  0.1296],\n",
      "        [ 0.5445,  0.2129],\n",
      "        [ 0.8418, -0.0085],\n",
      "        [ 0.5018,  0.1813],\n",
      "        [ 0.6239, -0.1297],\n",
      "        [ 0.9443, -0.3356],\n",
      "        [ 0.6025,  0.0740],\n",
      "        [ 0.6864,  0.2540],\n",
      "        [ 0.8186,  0.1576],\n",
      "        [ 0.6908,  0.1194],\n",
      "        [ 0.7104,  0.0524],\n",
      "        [ 0.9034,  0.1246]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8492, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8487,  0.0772],\n",
      "        [ 1.0350, -0.1015],\n",
      "        [ 0.9622,  0.2717],\n",
      "        [ 0.9868,  0.0476],\n",
      "        [ 0.7833, -0.0031],\n",
      "        [ 0.9359,  0.0535],\n",
      "        [ 0.6804,  0.0149],\n",
      "        [ 0.4610,  0.2074],\n",
      "        [ 0.8703,  0.1586],\n",
      "        [ 0.5461,  0.1229],\n",
      "        [ 0.8014, -0.1661],\n",
      "        [ 0.9336,  0.1225],\n",
      "        [ 0.8792,  0.2633],\n",
      "        [ 0.7389,  0.2973],\n",
      "        [ 0.9415, -0.1462],\n",
      "        [ 1.0572,  0.1774],\n",
      "        [ 0.9989,  0.1396],\n",
      "        [ 0.8833,  0.1253],\n",
      "        [ 0.6080,  0.2877],\n",
      "        [ 0.9140, -0.0059],\n",
      "        [ 1.0600, -0.0220],\n",
      "        [ 0.6162,  0.1149],\n",
      "        [ 0.6274,  0.1442],\n",
      "        [ 0.8162,  0.0211],\n",
      "        [ 0.6343,  0.1614],\n",
      "        [ 0.6030, -0.0738],\n",
      "        [ 0.7919,  0.1676],\n",
      "        [ 0.8211, -0.0287],\n",
      "        [ 0.6614,  0.1579],\n",
      "        [ 0.9873, -0.0671],\n",
      "        [ 0.7264,  0.0705],\n",
      "        [ 0.9621,  0.2629]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.8912, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 5.2901e-01, -3.8281e-02],\n",
      "        [ 9.4804e-01, -1.1985e-01],\n",
      "        [ 6.4083e-01,  4.0773e-02],\n",
      "        [ 5.8466e-01,  2.4103e-01],\n",
      "        [ 8.1452e-01,  6.1113e-02],\n",
      "        [ 8.2432e-01,  1.3295e-01],\n",
      "        [ 5.8847e-01, -1.0444e-01],\n",
      "        [ 6.7361e-01,  1.9297e-01],\n",
      "        [ 6.4619e-01, -7.4764e-02],\n",
      "        [ 5.8571e-01,  1.6353e-01],\n",
      "        [ 6.4824e-01,  2.1187e-01],\n",
      "        [ 9.3620e-01,  2.9705e-02],\n",
      "        [ 6.6083e-01, -4.1256e-01],\n",
      "        [ 8.2464e-01,  1.9091e-02],\n",
      "        [ 6.5119e-01,  1.6299e-02],\n",
      "        [ 5.4662e-01,  1.3118e-01],\n",
      "        [ 5.6673e-01,  2.8193e-05],\n",
      "        [ 1.0668e+00,  1.6578e-01],\n",
      "        [ 2.6687e-01, -5.0496e-04],\n",
      "        [ 8.2637e-01,  1.2314e-01],\n",
      "        [ 5.9668e-01,  1.8491e-01],\n",
      "        [ 8.9041e-01,  1.1976e-01],\n",
      "        [ 8.6318e-01,  1.3094e-01],\n",
      "        [ 6.6053e-01,  3.9777e-01],\n",
      "        [ 6.7943e-01, -6.2069e-02],\n",
      "        [ 7.4050e-01, -6.5502e-02],\n",
      "        [ 6.5712e-01, -2.1971e-01],\n",
      "        [ 7.4554e-01,  2.5220e-01],\n",
      "        [ 6.7406e-01,  1.9320e-01],\n",
      "        [ 6.1792e-01, -6.2357e-02],\n",
      "        [ 6.0932e-01, -7.8963e-02],\n",
      "        [ 4.4942e-01, -1.5494e-02]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.7459, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 7.2605e-01, -1.8593e-01],\n",
      "        [ 4.9230e-01,  2.5064e-01],\n",
      "        [ 6.1888e-01, -3.0760e-02],\n",
      "        [ 5.4641e-01, -3.7707e-02],\n",
      "        [ 8.9193e-01,  2.6919e-01],\n",
      "        [ 6.9403e-01,  1.0619e-01],\n",
      "        [ 7.4362e-01,  2.1178e-01],\n",
      "        [ 1.1449e+00,  1.7689e-01],\n",
      "        [ 7.0093e-01,  4.9543e-02],\n",
      "        [ 8.0365e-01,  2.1731e-01],\n",
      "        [ 6.3240e-01, -9.2612e-02],\n",
      "        [ 6.4897e-01,  2.0733e-01],\n",
      "        [ 1.6896e-01, -2.4193e-02],\n",
      "        [ 8.3486e-01,  2.7476e-01],\n",
      "        [ 7.5160e-01,  2.7639e-01],\n",
      "        [ 1.0311e+00,  2.8574e-01],\n",
      "        [ 6.9892e-01,  6.8286e-02],\n",
      "        [ 9.6171e-01,  8.1528e-02],\n",
      "        [ 7.7152e-01,  5.6355e-03],\n",
      "        [ 8.8547e-01, -1.6025e-04],\n",
      "        [ 5.1781e-01,  8.3520e-02],\n",
      "        [ 5.0311e-01,  1.1753e-01],\n",
      "        [ 8.3397e-01,  1.9955e-01],\n",
      "        [ 4.2131e-01,  6.7146e-02],\n",
      "        [ 8.6647e-01,  1.2735e-01],\n",
      "        [ 2.7702e-01,  2.2061e-02],\n",
      "        [ 4.8150e-01,  1.4509e-01],\n",
      "        [ 7.2250e-01, -6.9247e-02],\n",
      "        [ 6.1012e-01,  4.6880e-02],\n",
      "        [ 6.8911e-01, -1.2615e-02],\n",
      "        [ 6.8071e-01,  2.0114e-03],\n",
      "        [ 7.4130e-01, -3.5430e-02]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8831, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9757,  0.2811],\n",
      "        [ 0.3587,  0.0668],\n",
      "        [ 0.7288, -0.0483],\n",
      "        [ 0.2299,  0.1235],\n",
      "        [ 0.6403,  0.1326],\n",
      "        [ 0.8859,  0.2702],\n",
      "        [ 0.3661,  0.0966],\n",
      "        [ 0.6041,  0.1432],\n",
      "        [ 0.7883, -0.0317],\n",
      "        [ 0.7385, -0.0919],\n",
      "        [ 0.9055,  0.1186],\n",
      "        [ 0.5686,  0.0752],\n",
      "        [ 0.5001,  0.3475],\n",
      "        [ 0.9016, -0.0489],\n",
      "        [ 0.8759,  0.0954],\n",
      "        [ 0.7653,  0.1875],\n",
      "        [ 0.7422, -0.0258],\n",
      "        [ 0.9317,  0.0915],\n",
      "        [ 0.6748,  0.0880],\n",
      "        [ 0.8756,  0.0188],\n",
      "        [ 0.7431, -0.0238],\n",
      "        [ 0.8591,  0.1654],\n",
      "        [ 0.8832,  0.4038],\n",
      "        [ 0.2705, -0.1813],\n",
      "        [ 0.9650,  0.2505],\n",
      "        [ 0.5323,  0.2304],\n",
      "        [ 0.5928,  0.0682],\n",
      "        [ 1.0495, -0.0054],\n",
      "        [ 0.3947,  0.1995],\n",
      "        [ 0.8097, -0.0038],\n",
      "        [ 0.8351,  0.2549],\n",
      "        [ 0.7422,  0.2114]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8913, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3699,  0.2253],\n",
      "        [ 0.6969,  0.0805],\n",
      "        [ 1.0184,  0.0574],\n",
      "        [ 0.8791,  0.0730],\n",
      "        [ 0.8226, -0.0284],\n",
      "        [ 0.5708,  0.2346],\n",
      "        [ 0.7725,  0.3448],\n",
      "        [ 0.4879,  0.0744],\n",
      "        [ 0.7809,  0.0381],\n",
      "        [ 0.6960, -0.0151],\n",
      "        [ 0.7813,  0.1475],\n",
      "        [ 0.7790,  0.1926],\n",
      "        [ 0.6857,  0.3308],\n",
      "        [ 0.8765,  0.1577],\n",
      "        [ 0.2305,  0.0337],\n",
      "        [ 0.6397,  0.1505],\n",
      "        [ 0.3598,  0.1430],\n",
      "        [ 0.7223, -0.1082],\n",
      "        [ 0.5805,  0.1761],\n",
      "        [ 0.7977,  0.0576],\n",
      "        [ 0.6318,  0.1177],\n",
      "        [ 0.7365, -0.0517],\n",
      "        [ 0.7154,  0.1977],\n",
      "        [ 0.9498, -0.0916],\n",
      "        [ 0.5330,  0.1701],\n",
      "        [ 0.6706,  0.0287],\n",
      "        [ 0.5075,  0.1837],\n",
      "        [ 0.7036, -0.1235],\n",
      "        [ 0.5640, -0.0563],\n",
      "        [ 0.4090,  0.0883],\n",
      "        [ 0.4658,  0.0275],\n",
      "        [ 0.9343,  0.1439]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.8386, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5965,  0.0724],\n",
      "        [ 0.7626, -0.0381],\n",
      "        [ 0.3541,  0.0017],\n",
      "        [ 0.4979, -0.0878],\n",
      "        [ 0.5124,  0.3211],\n",
      "        [ 0.7916,  0.0479],\n",
      "        [ 0.3828, -0.0311],\n",
      "        [ 0.6076,  0.1449],\n",
      "        [ 0.8373,  0.0624],\n",
      "        [ 0.8096, -0.0344],\n",
      "        [ 0.5645,  0.0729],\n",
      "        [ 0.7931,  0.1882],\n",
      "        [ 0.6361,  0.0873],\n",
      "        [ 0.7741,  0.1555],\n",
      "        [ 0.4029,  0.2069],\n",
      "        [ 0.6594,  0.1453],\n",
      "        [ 0.5113, -0.0449],\n",
      "        [ 1.0216, -0.1808],\n",
      "        [ 0.6500,  0.0605],\n",
      "        [ 0.9580,  0.0056],\n",
      "        [ 0.7847,  0.2056],\n",
      "        [ 0.8339, -0.0236],\n",
      "        [ 0.6600,  0.1297],\n",
      "        [ 0.8205, -0.1580],\n",
      "        [ 0.7702, -0.0371],\n",
      "        [ 0.7879,  0.1325],\n",
      "        [ 0.4576,  0.1350],\n",
      "        [ 1.0805, -0.0348],\n",
      "        [ 0.8066,  0.1085],\n",
      "        [ 0.6103, -0.0037],\n",
      "        [ 0.7293,  0.0515],\n",
      "        [ 1.1807, -0.2248]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8823, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9271,  0.0436],\n",
      "        [ 0.8043, -0.0605],\n",
      "        [ 0.8507, -0.0351],\n",
      "        [ 0.9783,  0.1880],\n",
      "        [ 0.7227,  0.0393],\n",
      "        [ 0.6689,  0.0303],\n",
      "        [ 0.6788,  0.0916],\n",
      "        [ 0.7002,  0.2827],\n",
      "        [ 0.7118, -0.0417],\n",
      "        [ 0.6442,  0.1525],\n",
      "        [ 0.8049,  0.0427],\n",
      "        [ 0.8902,  0.0210],\n",
      "        [ 0.5187, -0.0984],\n",
      "        [ 0.7411,  0.0777],\n",
      "        [ 0.4427,  0.0948],\n",
      "        [ 0.8467,  0.1375],\n",
      "        [ 0.6975,  0.0325],\n",
      "        [ 0.8094,  0.1211],\n",
      "        [ 0.7432,  0.0997],\n",
      "        [ 0.7028,  0.1199],\n",
      "        [ 0.6145, -0.1011],\n",
      "        [ 0.8798,  0.2577],\n",
      "        [ 0.8016,  0.2645],\n",
      "        [ 0.7288,  0.0520],\n",
      "        [ 0.8470,  0.0238],\n",
      "        [ 0.7739,  0.1065],\n",
      "        [ 0.8134,  0.0943],\n",
      "        [ 0.6284,  0.0837],\n",
      "        [ 0.7772,  0.0923],\n",
      "        [ 0.6560, -0.1919],\n",
      "        [ 0.7890,  0.0624],\n",
      "        [ 0.4994,  0.1236]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9212, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 8.8134e-01,  7.8859e-03],\n",
      "        [ 8.3495e-01,  1.3414e-01],\n",
      "        [ 6.7184e-01, -5.0250e-02],\n",
      "        [ 5.0024e-01, -9.6488e-02],\n",
      "        [ 9.7151e-01,  1.5250e-01],\n",
      "        [ 6.4148e-01, -8.8011e-03],\n",
      "        [ 8.3021e-01,  2.2489e-01],\n",
      "        [ 8.3331e-01, -8.5727e-02],\n",
      "        [ 7.6174e-01,  8.6284e-02],\n",
      "        [ 5.6419e-01,  7.9299e-02],\n",
      "        [ 9.8546e-01, -1.1873e-01],\n",
      "        [ 8.1605e-01,  5.1472e-04],\n",
      "        [ 6.5792e-01,  3.3362e-01],\n",
      "        [ 1.2239e+00,  1.8871e-01],\n",
      "        [ 7.4551e-01, -7.8776e-02],\n",
      "        [ 9.4851e-01,  7.8913e-02],\n",
      "        [ 7.5584e-01,  1.8478e-01],\n",
      "        [ 5.6472e-01,  4.6495e-02],\n",
      "        [ 6.9197e-01, -9.0598e-02],\n",
      "        [ 1.0471e+00, -1.5537e-02],\n",
      "        [ 8.9741e-01,  2.3450e-01],\n",
      "        [ 9.6861e-01,  1.4213e-01],\n",
      "        [ 9.0238e-01,  3.1816e-02],\n",
      "        [ 1.2597e+00,  1.7354e-01],\n",
      "        [ 5.9419e-01, -2.8682e-03],\n",
      "        [ 7.7995e-01,  1.0403e-02],\n",
      "        [ 6.3851e-01,  1.6048e-01],\n",
      "        [ 8.6711e-01,  2.4055e-01],\n",
      "        [ 6.3472e-01,  1.5572e-01],\n",
      "        [ 6.9509e-01, -5.3757e-02],\n",
      "        [ 6.8643e-01,  2.6339e-02],\n",
      "        [ 7.5314e-01,  4.1132e-01]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8581, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7262,  0.1065],\n",
      "        [ 0.7015,  0.3459],\n",
      "        [ 0.5132,  0.0449],\n",
      "        [ 0.6834,  0.3000],\n",
      "        [ 0.7561,  0.0775],\n",
      "        [ 0.4365,  0.0741],\n",
      "        [ 0.6070, -0.0497],\n",
      "        [ 0.7808,  0.1426],\n",
      "        [ 0.7951, -0.0314],\n",
      "        [ 0.7475,  0.1142],\n",
      "        [ 0.7977,  0.1771],\n",
      "        [ 0.6453, -0.0043],\n",
      "        [ 0.7571,  0.0246],\n",
      "        [ 0.7417,  0.3901],\n",
      "        [ 0.9211,  0.1370],\n",
      "        [ 0.6367, -0.0160],\n",
      "        [ 0.6302, -0.0032],\n",
      "        [ 0.5168,  0.1216],\n",
      "        [ 0.3722,  0.0330],\n",
      "        [ 0.6718, -0.0118],\n",
      "        [ 0.5758,  0.0600],\n",
      "        [ 0.8487,  0.0495],\n",
      "        [ 1.1183,  0.0456],\n",
      "        [ 0.8516,  0.0698],\n",
      "        [ 0.3883, -0.0330],\n",
      "        [ 0.8416,  0.1464],\n",
      "        [ 0.6913,  0.1366],\n",
      "        [ 1.1476,  0.0559],\n",
      "        [ 0.5651,  0.1507],\n",
      "        [ 0.7344,  0.0474],\n",
      "        [ 0.6103,  0.1241],\n",
      "        [ 1.0671,  0.0310]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.8829, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 6.1622e-01,  7.1860e-02],\n",
      "        [ 6.2543e-01,  3.9688e-02],\n",
      "        [ 7.3078e-01,  4.1522e-02],\n",
      "        [ 7.7229e-01,  4.2979e-02],\n",
      "        [ 7.3689e-01, -8.2457e-02],\n",
      "        [ 7.5863e-01,  5.3932e-04],\n",
      "        [ 9.3950e-01, -3.6670e-02],\n",
      "        [ 7.0246e-01,  1.2101e-01],\n",
      "        [ 9.4806e-01,  3.1183e-01],\n",
      "        [ 7.7320e-01,  2.0410e-01],\n",
      "        [ 9.6442e-01, -5.9754e-02],\n",
      "        [ 6.7515e-01,  1.3025e-01],\n",
      "        [ 7.0456e-01,  1.5444e-01],\n",
      "        [ 5.7400e-01,  2.3947e-01],\n",
      "        [ 5.4267e-01,  1.1250e-01],\n",
      "        [ 9.4476e-01, -7.8788e-02],\n",
      "        [ 8.7332e-01,  2.3671e-01],\n",
      "        [ 7.8376e-01, -5.4763e-02],\n",
      "        [ 8.7265e-01,  2.0808e-01],\n",
      "        [ 5.5896e-01,  1.1233e-01],\n",
      "        [ 8.7515e-01,  2.7334e-01],\n",
      "        [ 6.4312e-01, -1.3958e-01],\n",
      "        [ 5.3403e-01,  4.7582e-02],\n",
      "        [ 5.9638e-01,  3.3218e-02],\n",
      "        [ 6.5045e-01, -1.4873e-02],\n",
      "        [ 4.6066e-01,  2.2011e-01],\n",
      "        [ 9.7234e-01,  2.8565e-01],\n",
      "        [ 7.9601e-01,  2.6468e-01],\n",
      "        [ 9.1019e-01,  2.2517e-01],\n",
      "        [ 5.4893e-01, -3.7565e-02],\n",
      "        [ 6.6869e-01,  6.1329e-02],\n",
      "        [ 4.0254e-01,  1.7526e-03]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(1.0074, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7651,  0.2398],\n",
      "        [ 0.8762,  0.0683],\n",
      "        [ 0.5873,  0.0025],\n",
      "        [ 0.5633,  0.0700],\n",
      "        [ 0.7338,  0.2085],\n",
      "        [ 0.5524,  0.0483],\n",
      "        [ 0.6789, -0.3254],\n",
      "        [ 0.8036,  0.0951],\n",
      "        [ 0.5680, -0.1395],\n",
      "        [ 0.7596,  0.1557],\n",
      "        [ 0.3970,  0.0680],\n",
      "        [ 0.9688, -0.0398],\n",
      "        [ 0.8566, -0.1237],\n",
      "        [ 0.6132,  0.0706],\n",
      "        [ 0.7224, -0.0112],\n",
      "        [ 0.7435,  0.2137],\n",
      "        [ 0.9185,  0.0898],\n",
      "        [ 0.6979,  0.0709],\n",
      "        [ 0.8424, -0.0369],\n",
      "        [ 0.5633,  0.1291],\n",
      "        [ 0.9491,  0.1134],\n",
      "        [ 0.9176,  0.1181],\n",
      "        [ 0.6570,  0.1103],\n",
      "        [ 0.5518,  0.0324],\n",
      "        [ 0.7435, -0.0621],\n",
      "        [ 0.6850, -0.1822],\n",
      "        [ 0.8626,  0.0948],\n",
      "        [ 0.6016,  0.0577],\n",
      "        [ 0.7500, -0.5329],\n",
      "        [ 1.0633, -0.0025],\n",
      "        [ 0.7851,  0.0516],\n",
      "        [ 0.8580,  0.1618]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9924, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9412,  0.3771],\n",
      "        [ 1.0116,  0.2324],\n",
      "        [ 0.9463,  0.0402],\n",
      "        [ 0.5746,  0.2366],\n",
      "        [ 0.8328, -0.1170],\n",
      "        [ 0.4775, -0.1993],\n",
      "        [ 0.7043,  0.0249],\n",
      "        [ 0.5386,  0.1231],\n",
      "        [ 0.5506, -0.0852],\n",
      "        [ 0.9005,  0.0868],\n",
      "        [ 0.9283,  0.2823],\n",
      "        [ 0.7393,  0.2208],\n",
      "        [ 0.5225, -0.1642],\n",
      "        [ 0.6172, -0.0176],\n",
      "        [ 0.8868,  0.0259],\n",
      "        [ 0.5301,  0.1563],\n",
      "        [ 0.8450, -0.0133],\n",
      "        [ 0.9512,  0.1326],\n",
      "        [ 0.7711, -0.1867],\n",
      "        [ 0.5128, -0.0717],\n",
      "        [ 0.7269, -0.1950],\n",
      "        [ 0.8808, -0.0385],\n",
      "        [ 0.7977,  0.1918],\n",
      "        [ 1.0089,  0.0562],\n",
      "        [ 0.5367,  0.0956],\n",
      "        [ 0.7559, -0.0036],\n",
      "        [ 0.6831,  0.2653],\n",
      "        [ 0.7708, -0.0276],\n",
      "        [ 0.9213, -0.0223],\n",
      "        [ 0.5704,  0.2902],\n",
      "        [ 0.8984, -0.0132],\n",
      "        [ 0.6051, -0.2977]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8025, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6036,  0.0846],\n",
      "        [ 0.4651, -0.1906],\n",
      "        [ 0.9050,  0.1404],\n",
      "        [ 0.8867,  0.1584],\n",
      "        [-0.1529,  0.0575],\n",
      "        [ 0.7410,  0.1135],\n",
      "        [ 0.6676,  0.1582],\n",
      "        [ 0.8361,  0.0786],\n",
      "        [ 0.6653, -0.2201],\n",
      "        [ 0.5556, -0.0496],\n",
      "        [ 0.6929, -0.1055],\n",
      "        [ 0.5604,  0.0100],\n",
      "        [ 0.9659, -0.0957],\n",
      "        [ 0.8564,  0.0116],\n",
      "        [ 0.9385,  0.0293],\n",
      "        [ 0.9026,  0.1020],\n",
      "        [ 0.8589,  0.1310],\n",
      "        [ 0.9628,  0.0624],\n",
      "        [ 0.4712, -0.0016],\n",
      "        [ 0.7651, -0.1243],\n",
      "        [ 0.8666, -0.0072],\n",
      "        [ 0.7867,  0.1795],\n",
      "        [ 0.7161,  0.1760],\n",
      "        [ 1.0616,  0.3199],\n",
      "        [ 0.7386,  0.1792],\n",
      "        [ 0.6347,  0.1837],\n",
      "        [ 0.9532, -0.4679],\n",
      "        [ 0.9285, -0.0488],\n",
      "        [ 0.8536,  0.0650],\n",
      "        [ 0.5875, -0.1191],\n",
      "        [ 0.7636,  0.1407],\n",
      "        [ 0.7589,  0.2207]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.7599, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9366,  0.1067],\n",
      "        [ 0.5960,  0.1079],\n",
      "        [ 0.8220,  0.2125],\n",
      "        [ 0.7687,  0.0600],\n",
      "        [ 0.6518,  0.0759],\n",
      "        [ 0.5056,  0.0161],\n",
      "        [ 0.7914,  0.1558],\n",
      "        [ 0.6380,  0.1288],\n",
      "        [ 0.6533,  0.3592],\n",
      "        [ 0.7486,  0.0855],\n",
      "        [ 0.6113, -0.1430],\n",
      "        [ 0.8705,  0.0397],\n",
      "        [ 0.4788,  0.1087],\n",
      "        [ 0.6497,  0.0516],\n",
      "        [ 0.8489,  0.1140],\n",
      "        [ 0.8160,  0.3392],\n",
      "        [ 0.6135,  0.2408],\n",
      "        [ 0.7742, -0.2305],\n",
      "        [ 0.0132, -0.0359],\n",
      "        [ 0.8455,  0.1080],\n",
      "        [ 0.7084,  0.2726],\n",
      "        [ 0.6957,  0.3174],\n",
      "        [ 0.7527,  0.2822],\n",
      "        [ 0.7878,  0.1622],\n",
      "        [ 0.5887, -0.1545],\n",
      "        [ 0.6041, -0.1610],\n",
      "        [ 0.4747,  0.0190],\n",
      "        [ 0.9993,  0.0300],\n",
      "        [ 0.7494,  0.0574],\n",
      "        [ 0.8390, -0.1532],\n",
      "        [ 0.6234,  0.0372],\n",
      "        [ 0.5245, -0.0634]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9036, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9411,  0.0230],\n",
      "        [ 0.9326, -0.0323],\n",
      "        [ 0.7900,  0.2779],\n",
      "        [ 0.8132,  0.1121],\n",
      "        [ 0.6784,  0.1682],\n",
      "        [ 0.7464,  0.0697],\n",
      "        [ 0.7861,  0.1157],\n",
      "        [ 0.7856,  0.0453],\n",
      "        [ 0.5454, -0.0200],\n",
      "        [ 0.7288,  0.0618],\n",
      "        [ 0.5572,  0.1708],\n",
      "        [ 0.6881,  0.0716],\n",
      "        [ 0.6515, -0.0704],\n",
      "        [ 0.7149,  0.0206],\n",
      "        [ 0.6677,  0.0531],\n",
      "        [ 0.3091,  0.0567],\n",
      "        [ 1.0479,  0.0207],\n",
      "        [ 0.4523,  0.0136],\n",
      "        [ 0.6119,  0.0724],\n",
      "        [ 0.7162, -0.1461],\n",
      "        [ 0.6562,  0.0250],\n",
      "        [ 0.5974,  0.0875],\n",
      "        [ 0.5970, -0.3587],\n",
      "        [ 1.0238, -0.0057],\n",
      "        [ 0.8531,  0.1244],\n",
      "        [ 0.4024,  0.1639],\n",
      "        [ 0.9305,  0.0691],\n",
      "        [ 0.9941,  0.2602],\n",
      "        [ 0.9345,  0.1520],\n",
      "        [ 0.7345,  0.1958],\n",
      "        [ 0.4146, -0.0630],\n",
      "        [ 0.7707, -0.0949]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9091, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8037,  0.1147],\n",
      "        [ 0.4987,  0.0992],\n",
      "        [ 0.8455,  0.1111],\n",
      "        [ 0.5061, -0.0445],\n",
      "        [ 0.8459,  0.1032],\n",
      "        [ 0.5758,  0.0325],\n",
      "        [ 0.8125,  0.1088],\n",
      "        [ 0.5915,  0.1199],\n",
      "        [ 0.7754,  0.1888],\n",
      "        [ 0.8439,  0.0611],\n",
      "        [ 0.7247, -0.2945],\n",
      "        [ 0.6900, -0.0877],\n",
      "        [ 0.7993, -0.1039],\n",
      "        [ 0.4115,  0.2091],\n",
      "        [ 0.6349,  0.0542],\n",
      "        [ 0.5861, -0.0068],\n",
      "        [ 0.4795,  0.1521],\n",
      "        [ 1.1683,  0.1429],\n",
      "        [ 0.5132,  0.1675],\n",
      "        [ 0.7972,  0.1910],\n",
      "        [ 1.0450,  0.0552],\n",
      "        [ 0.8705, -0.1620],\n",
      "        [ 0.6437,  0.0416],\n",
      "        [ 0.5962,  0.1607],\n",
      "        [ 0.3868, -0.0859],\n",
      "        [ 0.6412,  0.0269],\n",
      "        [ 0.6916, -0.2363],\n",
      "        [ 0.9062,  0.1440],\n",
      "        [ 1.0655,  0.1406],\n",
      "        [ 0.5476, -0.0109],\n",
      "        [ 0.7312,  0.1204],\n",
      "        [ 0.1259, -0.1982]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9000, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6672, -0.0504],\n",
      "        [ 0.4187, -0.0353],\n",
      "        [ 0.4098, -0.0558],\n",
      "        [ 0.5878, -0.0848],\n",
      "        [ 0.8012, -0.0171],\n",
      "        [ 0.8951,  0.0989],\n",
      "        [ 0.8474,  0.1025],\n",
      "        [ 0.7606,  0.0609],\n",
      "        [ 0.5361, -0.2366],\n",
      "        [ 0.7490,  0.0340],\n",
      "        [ 0.5254,  0.0385],\n",
      "        [ 0.8480,  0.1090],\n",
      "        [ 0.7323,  0.0445],\n",
      "        [ 0.8550,  0.2941],\n",
      "        [ 1.1683, -0.0568],\n",
      "        [ 0.2261,  0.0301],\n",
      "        [ 0.6974,  0.0124],\n",
      "        [ 0.5954,  0.1592],\n",
      "        [ 0.7430, -0.0454],\n",
      "        [ 1.1209,  0.2327],\n",
      "        [ 1.1384,  0.0468],\n",
      "        [ 0.7386,  0.0212],\n",
      "        [ 0.4793,  0.1469],\n",
      "        [ 0.3826, -0.1911],\n",
      "        [ 0.8214, -0.0665],\n",
      "        [ 0.6453,  0.2509],\n",
      "        [ 0.7296,  0.1812],\n",
      "        [ 0.8869,  0.1436],\n",
      "        [ 0.7642,  0.0329],\n",
      "        [ 0.7560,  0.1135],\n",
      "        [ 0.4982,  0.0471],\n",
      "        [ 0.6154, -0.0350]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.8839, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6052,  0.2560],\n",
      "        [-0.1816,  0.0584],\n",
      "        [ 0.9210,  0.1115],\n",
      "        [ 0.8470,  0.1695],\n",
      "        [ 0.9512, -0.0689],\n",
      "        [ 0.6517,  0.0858],\n",
      "        [ 0.9311,  0.2094],\n",
      "        [ 0.6737,  0.1011],\n",
      "        [ 0.7310, -0.1573],\n",
      "        [ 0.8060, -0.0033],\n",
      "        [ 0.8001,  0.0120],\n",
      "        [ 0.3018, -0.0560],\n",
      "        [ 0.7838,  0.1903],\n",
      "        [ 0.5968, -0.1626],\n",
      "        [ 0.7351,  0.0056],\n",
      "        [ 0.5495,  0.0115],\n",
      "        [ 0.5690, -0.0255],\n",
      "        [ 0.4864,  0.1328],\n",
      "        [ 0.7277,  0.2019],\n",
      "        [ 0.5175,  0.1230],\n",
      "        [ 0.5134, -0.1681],\n",
      "        [ 1.1329,  0.3993],\n",
      "        [ 0.8706,  0.2564],\n",
      "        [ 0.8894,  0.1608],\n",
      "        [ 0.8078, -0.1859],\n",
      "        [ 0.6792,  0.1293],\n",
      "        [ 0.1181,  0.1197],\n",
      "        [ 0.6195,  0.2526],\n",
      "        [ 0.8654,  0.1693],\n",
      "        [ 0.3607,  0.0096],\n",
      "        [ 0.7099,  0.0807],\n",
      "        [ 0.4322,  0.2280]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8183, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6739, -0.1881],\n",
      "        [ 0.8969,  0.4568],\n",
      "        [ 0.7996, -0.1468],\n",
      "        [ 0.9699,  0.0742],\n",
      "        [ 0.7437,  0.1774],\n",
      "        [ 0.5285,  0.0890],\n",
      "        [ 0.6913,  0.0942],\n",
      "        [ 0.7819,  0.1746],\n",
      "        [ 0.5325,  0.1776],\n",
      "        [ 0.7317,  0.1304],\n",
      "        [ 0.6707,  0.0260],\n",
      "        [ 0.7910, -0.1591],\n",
      "        [ 0.7316, -0.1061],\n",
      "        [ 0.4978,  0.0063],\n",
      "        [ 0.7053, -0.0740],\n",
      "        [ 0.7683,  0.0169],\n",
      "        [ 0.8597,  0.1243],\n",
      "        [ 0.5915, -0.1092],\n",
      "        [ 0.4161, -0.0159],\n",
      "        [ 1.0773,  0.1219],\n",
      "        [ 0.6314,  0.2543],\n",
      "        [ 0.8169, -0.0437],\n",
      "        [ 0.6273,  0.0932],\n",
      "        [ 0.9006, -0.0250],\n",
      "        [ 1.0336,  0.2419],\n",
      "        [ 0.6965,  0.0039],\n",
      "        [ 1.1015,  0.3112],\n",
      "        [ 0.2374,  0.2152],\n",
      "        [ 1.0676,  0.0858],\n",
      "        [ 0.7020, -0.0403],\n",
      "        [ 0.8127,  0.2755],\n",
      "        [ 0.8479, -0.0669]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8862, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8148, -0.1012],\n",
      "        [ 0.7957, -0.1174],\n",
      "        [ 0.5387,  0.1634],\n",
      "        [ 0.5185,  0.1035],\n",
      "        [ 1.0363,  0.2740],\n",
      "        [ 0.8115,  0.0700],\n",
      "        [ 0.7407,  0.0513],\n",
      "        [ 0.7861, -0.0505],\n",
      "        [ 0.7956, -0.0829],\n",
      "        [ 0.8540,  0.2247],\n",
      "        [ 0.5447,  0.1409],\n",
      "        [ 0.7674, -0.0191],\n",
      "        [ 0.5498, -0.1605],\n",
      "        [ 0.6741,  0.0958],\n",
      "        [ 0.6775,  0.1597],\n",
      "        [ 0.5266,  0.2352],\n",
      "        [ 0.8803,  0.1164],\n",
      "        [ 0.5294, -0.1054],\n",
      "        [ 0.8341, -0.0564],\n",
      "        [ 0.7621,  0.1569],\n",
      "        [ 0.8540,  0.0265],\n",
      "        [ 0.9109,  0.0637],\n",
      "        [ 1.0212,  0.1551],\n",
      "        [ 0.6781, -0.4365],\n",
      "        [ 0.7039,  0.1461],\n",
      "        [ 0.9019, -0.0267],\n",
      "        [ 0.4497,  0.2070],\n",
      "        [ 0.4135,  0.0215],\n",
      "        [ 0.6966,  0.1014],\n",
      "        [ 0.8196,  0.1565],\n",
      "        [ 0.8670, -0.0274],\n",
      "        [ 0.7420,  0.3124]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8484, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6361,  0.1712],\n",
      "        [ 0.4702, -0.0907],\n",
      "        [ 0.7951,  0.3048],\n",
      "        [ 0.6825,  0.2260],\n",
      "        [ 0.5502, -0.1089],\n",
      "        [ 0.1849,  0.1192],\n",
      "        [ 0.4331, -0.0366],\n",
      "        [ 0.6041, -0.0253],\n",
      "        [ 0.6081,  0.1283],\n",
      "        [ 0.7326,  0.2117],\n",
      "        [ 0.5618, -0.1895],\n",
      "        [ 0.7536,  0.2607],\n",
      "        [ 0.3256, -0.0949],\n",
      "        [ 0.6716,  0.2600],\n",
      "        [ 0.6555,  0.2160],\n",
      "        [ 1.0302, -0.0866],\n",
      "        [ 0.7536, -0.1396],\n",
      "        [ 0.3952, -0.1866],\n",
      "        [ 0.9034, -0.1167],\n",
      "        [ 0.5495, -0.0138],\n",
      "        [ 0.5942, -0.0323],\n",
      "        [ 0.8901,  0.1369],\n",
      "        [ 0.6187,  0.0534],\n",
      "        [ 0.7768,  0.1914],\n",
      "        [ 0.7716, -0.1774],\n",
      "        [ 0.6547, -0.0228],\n",
      "        [ 0.4984,  0.0166],\n",
      "        [ 0.7188,  0.1377],\n",
      "        [ 0.7527,  0.1643],\n",
      "        [ 0.6553, -0.0836],\n",
      "        [ 0.6831,  0.0092],\n",
      "        [ 0.6992,  0.1428]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.9124, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9959,  0.0241],\n",
      "        [ 0.8831, -0.0512],\n",
      "        [ 0.9899,  0.0997],\n",
      "        [ 0.6591,  0.0202],\n",
      "        [ 0.8138,  0.1608],\n",
      "        [ 0.4531,  0.1875],\n",
      "        [ 0.1102,  0.2400],\n",
      "        [ 0.7016,  0.2253],\n",
      "        [ 0.9105,  0.0098],\n",
      "        [ 0.6411,  0.0532],\n",
      "        [ 0.8010, -0.0635],\n",
      "        [ 0.8293,  0.2114],\n",
      "        [ 0.8926,  0.0171],\n",
      "        [ 0.7414,  0.0314],\n",
      "        [ 0.7664,  0.1326],\n",
      "        [ 0.5008, -0.0437],\n",
      "        [ 0.9503,  0.1950],\n",
      "        [ 0.8697, -0.1388],\n",
      "        [ 0.7331,  0.2952],\n",
      "        [ 0.4313,  0.3388],\n",
      "        [ 0.8643,  0.0910],\n",
      "        [ 0.6689, -0.0481],\n",
      "        [ 0.7654, -0.0457],\n",
      "        [ 0.7896,  0.1005],\n",
      "        [ 0.7959,  0.0503],\n",
      "        [ 0.9284,  0.0047],\n",
      "        [ 0.8666,  0.1644],\n",
      "        [ 0.9412,  0.0837],\n",
      "        [ 0.7122,  0.1881],\n",
      "        [ 0.8329,  0.2377],\n",
      "        [ 0.7671, -0.0079],\n",
      "        [ 0.3047,  0.1701]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8034, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6836, -0.1474],\n",
      "        [ 0.6954,  0.2026],\n",
      "        [ 0.8386,  0.1876],\n",
      "        [ 0.7805,  0.1023],\n",
      "        [ 0.6443, -0.0681],\n",
      "        [ 0.8191,  0.0665],\n",
      "        [ 0.4629,  0.0377],\n",
      "        [ 0.6335,  0.3063],\n",
      "        [ 0.7495,  0.1075],\n",
      "        [ 0.8100, -0.2025],\n",
      "        [ 0.5886,  0.1006],\n",
      "        [ 0.8152,  0.1359],\n",
      "        [ 0.7387, -0.0013],\n",
      "        [ 0.8154,  0.2764],\n",
      "        [ 0.6515,  0.1825],\n",
      "        [ 0.8477,  0.0537],\n",
      "        [ 0.5265,  0.0940],\n",
      "        [ 0.9783,  0.1522],\n",
      "        [ 0.9052,  0.0581],\n",
      "        [ 0.5393, -0.0291],\n",
      "        [ 0.9590,  0.2626],\n",
      "        [ 0.7056, -0.0748],\n",
      "        [ 0.9275,  0.1596],\n",
      "        [ 1.1475, -0.0526],\n",
      "        [ 0.3257, -0.0632],\n",
      "        [ 0.6546,  0.1160],\n",
      "        [ 0.7161,  0.1420],\n",
      "        [ 0.9236,  0.0257],\n",
      "        [ 0.4995,  0.1599],\n",
      "        [ 0.8611,  0.0643],\n",
      "        [ 0.3433, -0.0390],\n",
      "        [ 0.9395,  0.1574]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9268, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3991,  0.2148],\n",
      "        [ 0.8734,  0.0498],\n",
      "        [ 0.6465, -0.0591],\n",
      "        [ 0.8252,  0.0146],\n",
      "        [ 0.2846, -0.0321],\n",
      "        [ 0.6729,  0.1557],\n",
      "        [ 0.9214,  0.0944],\n",
      "        [ 0.6032,  0.0725],\n",
      "        [ 0.6739,  0.0162],\n",
      "        [ 0.8078,  0.3616],\n",
      "        [ 0.5714, -0.0137],\n",
      "        [ 0.5415,  0.2948],\n",
      "        [ 0.5685,  0.0703],\n",
      "        [ 0.8218, -0.0289],\n",
      "        [ 1.0761,  0.0244],\n",
      "        [ 0.4677,  0.1880],\n",
      "        [ 0.7284,  0.1438],\n",
      "        [ 0.6115,  0.2094],\n",
      "        [ 0.9946,  0.0258],\n",
      "        [ 0.6217,  0.1692],\n",
      "        [ 0.9379,  0.1465],\n",
      "        [ 0.7333,  0.0630],\n",
      "        [ 0.7208, -0.0243],\n",
      "        [ 0.5508,  0.1674],\n",
      "        [ 0.6529,  0.0532],\n",
      "        [ 0.7837,  0.0221],\n",
      "        [ 0.8059, -0.0470],\n",
      "        [ 0.9605,  0.1048],\n",
      "        [ 0.7041,  0.0241],\n",
      "        [ 0.5688, -0.1610],\n",
      "        [ 0.8227,  0.2125],\n",
      "        [ 0.7529,  0.0421]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9116, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9014,  0.2951],\n",
      "        [ 0.6133,  0.1154],\n",
      "        [ 0.6266,  0.1145],\n",
      "        [ 0.9253, -0.1876],\n",
      "        [ 0.6673, -0.0535],\n",
      "        [ 0.6310, -0.0409],\n",
      "        [ 0.8738, -0.3570],\n",
      "        [ 0.7888,  0.0281],\n",
      "        [ 0.8261,  0.0987],\n",
      "        [ 0.6371,  0.0213],\n",
      "        [ 0.7492,  0.1164],\n",
      "        [ 0.7654,  0.1140],\n",
      "        [ 0.9865, -0.1172],\n",
      "        [ 0.8834,  0.1505],\n",
      "        [ 0.9691,  0.3028],\n",
      "        [ 0.6626,  0.1354],\n",
      "        [ 0.9394,  0.3314],\n",
      "        [ 0.8799,  0.1625],\n",
      "        [ 0.4855,  0.3357],\n",
      "        [ 0.7929,  0.3179],\n",
      "        [ 0.4296,  0.2151],\n",
      "        [ 0.5938, -0.2014],\n",
      "        [ 0.7656, -0.0590],\n",
      "        [ 0.7938,  0.0809],\n",
      "        [ 0.6846,  0.0594],\n",
      "        [ 0.8142, -0.0807],\n",
      "        [ 0.7146,  0.1521],\n",
      "        [ 0.5142, -0.0555],\n",
      "        [ 0.7657,  0.2394],\n",
      "        [ 1.0107,  0.0084],\n",
      "        [ 0.7768, -0.0107],\n",
      "        [ 0.5332,  0.3445]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.7725, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 4.8593e-01, -1.2171e-01],\n",
      "        [ 6.0558e-01, -1.2421e-01],\n",
      "        [ 9.9449e-01,  1.3980e-01],\n",
      "        [ 3.5634e-01,  8.5191e-02],\n",
      "        [ 6.5631e-01, -1.5156e-01],\n",
      "        [ 6.9280e-01,  2.9282e-01],\n",
      "        [ 5.1887e-01,  4.0215e-03],\n",
      "        [ 7.4318e-01,  1.0884e-01],\n",
      "        [ 6.5842e-01,  6.0759e-02],\n",
      "        [ 7.7404e-01,  2.9138e-01],\n",
      "        [ 6.4089e-01,  1.8385e-01],\n",
      "        [ 8.6076e-01,  2.8918e-01],\n",
      "        [ 5.5186e-01, -2.4098e-01],\n",
      "        [ 6.8491e-01, -1.1396e-01],\n",
      "        [ 4.5696e-01,  2.0664e-01],\n",
      "        [ 7.1125e-01,  4.1054e-01],\n",
      "        [ 8.9868e-01,  4.7923e-02],\n",
      "        [ 5.0533e-01,  3.5782e-02],\n",
      "        [ 5.1677e-01,  1.6987e-01],\n",
      "        [ 8.9699e-01,  8.5952e-03],\n",
      "        [ 5.0033e-01, -5.0210e-03],\n",
      "        [ 8.9762e-01,  2.6074e-01],\n",
      "        [ 6.0424e-01, -7.4439e-02],\n",
      "        [ 8.3287e-01, -2.3243e-02],\n",
      "        [ 3.4152e-01,  2.2907e-01],\n",
      "        [ 6.7026e-01,  5.8619e-02],\n",
      "        [ 6.9124e-01,  1.1231e-02],\n",
      "        [ 8.8949e-01,  1.6765e-02],\n",
      "        [ 5.1182e-01,  3.9198e-02],\n",
      "        [ 6.2086e-01, -6.3472e-02],\n",
      "        [ 1.0026e+00, -1.6215e-04],\n",
      "        [ 7.4398e-01,  3.4678e-01]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8510, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8761,  0.0925],\n",
      "        [ 0.6726, -0.0971],\n",
      "        [ 0.7452,  0.0555],\n",
      "        [ 1.1199,  0.0098],\n",
      "        [ 0.9463,  0.0290],\n",
      "        [ 0.6049,  0.1188],\n",
      "        [ 0.5818,  0.1185],\n",
      "        [ 0.7470,  0.2461],\n",
      "        [ 0.7899, -0.1030],\n",
      "        [ 0.6933,  0.2396],\n",
      "        [ 0.7329,  0.3209],\n",
      "        [ 0.2309, -0.0162],\n",
      "        [ 1.0113,  0.2667],\n",
      "        [ 1.0203,  0.0159],\n",
      "        [ 0.7077, -0.2168],\n",
      "        [ 0.9904,  0.1027],\n",
      "        [ 0.8907,  0.1443],\n",
      "        [ 0.7747, -0.1318],\n",
      "        [ 0.7728,  0.0299],\n",
      "        [ 0.5332, -0.0213],\n",
      "        [ 0.6944,  0.2393],\n",
      "        [ 0.9278,  0.2715],\n",
      "        [ 0.7288, -0.0492],\n",
      "        [ 0.7844, -0.0482],\n",
      "        [ 1.0059,  0.1967],\n",
      "        [ 0.5946, -0.1532],\n",
      "        [ 1.0846,  0.2823],\n",
      "        [ 0.8371, -0.0267],\n",
      "        [ 0.8886, -0.1544],\n",
      "        [ 0.6213,  0.1515],\n",
      "        [ 0.7997,  0.0842],\n",
      "        [ 0.9712,  0.3361]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8973, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8245,  0.0403],\n",
      "        [ 0.8050,  0.3279],\n",
      "        [ 0.9372, -0.0429],\n",
      "        [ 0.4721,  0.2335],\n",
      "        [ 0.7999,  0.1559],\n",
      "        [ 1.0550,  0.2607],\n",
      "        [ 0.9982,  0.0532],\n",
      "        [ 0.7401, -0.0529],\n",
      "        [ 0.7589,  0.4014],\n",
      "        [ 0.8280,  0.2183],\n",
      "        [ 0.9873,  0.1942],\n",
      "        [ 0.4805,  0.1203],\n",
      "        [ 1.0677, -0.0343],\n",
      "        [ 0.5837,  0.0046],\n",
      "        [ 0.5594,  0.1603],\n",
      "        [ 0.7437,  0.1264],\n",
      "        [ 0.8767, -0.1097],\n",
      "        [ 0.8493,  0.3201],\n",
      "        [ 0.5127, -0.1358],\n",
      "        [ 0.6616, -0.0800],\n",
      "        [ 0.5479,  0.0754],\n",
      "        [ 0.6503,  0.3360],\n",
      "        [ 0.8627,  0.2428],\n",
      "        [ 0.8007, -0.1378],\n",
      "        [ 0.6572,  0.1621],\n",
      "        [ 0.3224, -0.1222],\n",
      "        [ 0.4620, -0.2690],\n",
      "        [ 0.5716, -0.1974],\n",
      "        [ 1.0804,  0.1587],\n",
      "        [ 1.0082,  0.0972],\n",
      "        [ 0.8315,  0.1092],\n",
      "        [ 0.7203,  0.2213]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9100, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8073,  0.0806],\n",
      "        [ 1.0287,  0.0365],\n",
      "        [ 0.5259, -0.1112],\n",
      "        [ 0.7669,  0.0519],\n",
      "        [ 1.0660,  0.2697],\n",
      "        [ 1.0181,  0.0145],\n",
      "        [ 0.6602,  0.0577],\n",
      "        [ 0.7960,  0.0340],\n",
      "        [ 0.8770,  0.2917],\n",
      "        [ 0.7026, -0.0156],\n",
      "        [ 0.6264,  0.0610],\n",
      "        [ 0.6809, -0.1563],\n",
      "        [ 0.4979, -0.1078],\n",
      "        [ 0.7802, -0.0265],\n",
      "        [ 0.6891, -0.1703],\n",
      "        [ 1.0033,  0.0273],\n",
      "        [ 0.8955, -0.0358],\n",
      "        [ 0.8332,  0.2599],\n",
      "        [ 0.5671,  0.0813],\n",
      "        [ 0.7451,  0.0646],\n",
      "        [ 0.6143, -0.0016],\n",
      "        [ 0.6076, -0.0488],\n",
      "        [ 0.9248,  0.2054],\n",
      "        [ 1.1390, -0.0644],\n",
      "        [ 0.4319,  0.2511],\n",
      "        [ 0.5911,  0.0927],\n",
      "        [ 0.4000,  0.0450],\n",
      "        [ 0.9631, -0.0705],\n",
      "        [ 0.9825,  0.2499],\n",
      "        [ 0.3704,  0.0714],\n",
      "        [ 0.6922,  0.1163],\n",
      "        [ 0.6154,  0.0580]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.8152, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7592,  0.0077],\n",
      "        [ 0.5636, -0.0299],\n",
      "        [ 0.5927,  0.0562],\n",
      "        [ 0.7210,  0.1014],\n",
      "        [ 0.4998,  0.1281],\n",
      "        [ 0.6073,  0.0436],\n",
      "        [ 0.8387,  0.1962],\n",
      "        [ 0.7453,  0.0065],\n",
      "        [ 0.5471,  0.0111],\n",
      "        [ 0.5895,  0.1185],\n",
      "        [ 0.5834, -0.1092],\n",
      "        [ 0.9185,  0.3783],\n",
      "        [ 0.7943,  0.3428],\n",
      "        [ 0.6078,  0.0441],\n",
      "        [ 0.4745,  0.1000],\n",
      "        [ 0.8485,  0.3832],\n",
      "        [ 0.4043, -0.0209],\n",
      "        [ 0.3500, -0.0725],\n",
      "        [ 0.5673,  0.3154],\n",
      "        [ 0.8272,  0.1185],\n",
      "        [ 0.6915,  0.0461],\n",
      "        [ 0.9447,  0.0393],\n",
      "        [ 0.7710,  0.2677],\n",
      "        [ 0.7315,  0.0744],\n",
      "        [ 0.7355,  0.0576],\n",
      "        [ 0.4813,  0.1265],\n",
      "        [ 0.5256, -0.2428],\n",
      "        [ 0.6504, -0.0775],\n",
      "        [ 0.8394,  0.1790],\n",
      "        [ 0.6614,  0.1932],\n",
      "        [ 0.8525,  0.1314],\n",
      "        [ 0.5539,  0.0235]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9241, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.1351e+00, -2.0927e-01],\n",
      "        [ 7.1399e-01,  6.4906e-02],\n",
      "        [ 6.4442e-01,  1.5587e-01],\n",
      "        [ 6.6742e-01,  7.4503e-02],\n",
      "        [ 4.3751e-01, -1.7310e-01],\n",
      "        [ 9.2956e-01,  2.6087e-01],\n",
      "        [ 7.8466e-01, -2.2556e-02],\n",
      "        [ 1.0503e+00,  1.0866e-01],\n",
      "        [ 6.5966e-01,  1.0418e-01],\n",
      "        [ 6.9268e-01,  3.6962e-03],\n",
      "        [ 6.3973e-01,  2.6755e-01],\n",
      "        [ 6.8400e-01, -1.6514e-01],\n",
      "        [ 8.1941e-01,  1.9594e-01],\n",
      "        [ 9.2491e-01,  2.2653e-01],\n",
      "        [ 9.1387e-01, -1.2324e-01],\n",
      "        [ 7.7995e-01, -4.3095e-02],\n",
      "        [ 5.6245e-01, -1.0598e-01],\n",
      "        [ 7.5221e-01,  1.1317e-01],\n",
      "        [ 8.7898e-01,  2.1402e-02],\n",
      "        [ 7.6243e-01,  2.1607e-01],\n",
      "        [ 1.4616e-01, -1.4707e-01],\n",
      "        [ 5.0200e-01,  1.0372e-01],\n",
      "        [ 6.8437e-01,  7.6985e-02],\n",
      "        [ 5.2541e-01,  1.0118e-01],\n",
      "        [ 9.6489e-01,  1.7827e-01],\n",
      "        [ 8.2522e-01,  1.2981e-01],\n",
      "        [ 7.3666e-01,  2.5927e-01],\n",
      "        [ 5.5449e-01,  8.3396e-04],\n",
      "        [ 7.7123e-01,  1.4379e-01],\n",
      "        [ 4.5125e-01,  2.5378e-01],\n",
      "        [ 6.2070e-01,  8.3591e-02],\n",
      "        [ 1.0000e+00,  2.7685e-01]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8254, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6691, -0.1262],\n",
      "        [ 0.8664, -0.0668],\n",
      "        [ 0.6650,  0.2647],\n",
      "        [ 0.7281,  0.1445],\n",
      "        [ 0.6268,  0.0319],\n",
      "        [ 0.9722,  0.2735],\n",
      "        [ 0.6830,  0.0675],\n",
      "        [ 0.6765,  0.0482],\n",
      "        [ 0.8945,  0.2878],\n",
      "        [ 0.7075,  0.0766],\n",
      "        [ 0.7665,  0.0964],\n",
      "        [ 0.8528,  0.0977],\n",
      "        [ 0.8281,  0.1947],\n",
      "        [ 0.6851,  0.0111],\n",
      "        [ 0.4254,  0.1139],\n",
      "        [ 0.9871,  0.0260],\n",
      "        [-0.0703,  0.2100],\n",
      "        [ 0.7375, -0.1848],\n",
      "        [ 0.7694,  0.0351],\n",
      "        [ 0.6433,  0.1113],\n",
      "        [ 0.9783, -0.2940],\n",
      "        [ 1.0284, -0.1296],\n",
      "        [ 0.1960, -0.1422],\n",
      "        [ 0.7626,  0.4923],\n",
      "        [ 0.5941, -0.0452],\n",
      "        [ 0.8258,  0.0919],\n",
      "        [ 0.6910,  0.0166],\n",
      "        [ 0.7952,  0.1225],\n",
      "        [ 0.8623,  0.2082],\n",
      "        [ 0.7485,  0.0974],\n",
      "        [ 0.8271,  0.3482],\n",
      "        [ 0.6238,  0.2096]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8452, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2150,  0.0696],\n",
      "        [ 0.6797,  0.1276],\n",
      "        [ 0.8465, -0.0045],\n",
      "        [ 1.0412, -0.0124],\n",
      "        [ 0.7921, -0.1351],\n",
      "        [ 0.7659,  0.2639],\n",
      "        [ 0.6621,  0.0528],\n",
      "        [ 0.7004,  0.0224],\n",
      "        [ 0.3453,  0.0496],\n",
      "        [ 0.7676, -0.1733],\n",
      "        [ 0.7535, -0.0093],\n",
      "        [ 0.7923,  0.0946],\n",
      "        [ 0.8320,  0.4237],\n",
      "        [ 0.6297,  0.1121],\n",
      "        [ 0.7742, -0.0873],\n",
      "        [ 0.6734,  0.0462],\n",
      "        [ 0.6013, -0.0598],\n",
      "        [ 0.9926,  0.2002],\n",
      "        [ 0.6309, -0.0997],\n",
      "        [ 0.7554,  0.0748],\n",
      "        [ 0.6573, -0.0459],\n",
      "        [ 0.5221,  0.0773],\n",
      "        [ 0.7705, -0.1398],\n",
      "        [ 0.5659, -0.1429],\n",
      "        [ 0.5752,  0.1083],\n",
      "        [ 0.8156,  0.2224],\n",
      "        [ 0.5738, -0.1351],\n",
      "        [ 0.9791,  0.1679],\n",
      "        [ 0.7933,  0.2125],\n",
      "        [ 1.0295,  0.2176],\n",
      "        [ 0.7924,  0.1340],\n",
      "        [ 0.8011, -0.0551]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.8781, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7307,  0.0201],\n",
      "        [ 0.6001, -0.3164],\n",
      "        [ 0.8186,  0.3299],\n",
      "        [ 0.7981,  0.1039],\n",
      "        [ 0.8707,  0.0424],\n",
      "        [ 0.4210,  0.1113],\n",
      "        [ 0.8835,  0.0055],\n",
      "        [ 0.7022,  0.1369],\n",
      "        [ 0.6870,  0.3434],\n",
      "        [ 0.2432,  0.2077],\n",
      "        [ 0.5204, -0.0848],\n",
      "        [ 0.9229,  0.3924],\n",
      "        [ 0.6960, -0.1730],\n",
      "        [ 0.6109, -0.0808],\n",
      "        [ 0.6201,  0.0344],\n",
      "        [ 0.8712,  0.1909],\n",
      "        [ 0.5532,  0.0992],\n",
      "        [ 0.6744,  0.1111],\n",
      "        [ 0.9221,  0.2364],\n",
      "        [ 1.0020, -0.3108],\n",
      "        [ 0.5752,  0.0042],\n",
      "        [ 0.3539, -0.0199],\n",
      "        [ 0.7491,  0.0343],\n",
      "        [ 0.8523,  0.1052],\n",
      "        [ 0.5503, -0.0411],\n",
      "        [ 0.5172,  0.0574],\n",
      "        [ 0.8682,  0.0566],\n",
      "        [ 0.7776,  0.2747],\n",
      "        [ 0.4920, -0.2421],\n",
      "        [ 0.8437,  0.3826],\n",
      "        [ 0.5411, -0.0483],\n",
      "        [ 0.8392,  0.1196]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9202, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 9.4053e-01,  2.2695e-01],\n",
      "        [ 5.1397e-01,  2.2588e-01],\n",
      "        [ 8.7887e-01,  1.2367e-01],\n",
      "        [ 8.2081e-01,  2.2694e-01],\n",
      "        [ 5.9490e-01, -9.4604e-02],\n",
      "        [ 8.0906e-01, -1.4274e-01],\n",
      "        [ 6.4017e-01, -1.2504e-02],\n",
      "        [ 6.2431e-01,  2.0483e-01],\n",
      "        [ 1.0383e+00, -5.8821e-02],\n",
      "        [ 7.7362e-01,  5.4064e-02],\n",
      "        [ 8.3759e-01,  1.3549e-01],\n",
      "        [ 8.1447e-01,  1.5745e-01],\n",
      "        [ 4.3667e-01,  3.3879e-01],\n",
      "        [ 8.7561e-01,  3.2119e-01],\n",
      "        [ 5.4562e-01,  2.2203e-01],\n",
      "        [ 8.6845e-01,  7.7277e-02],\n",
      "        [ 7.8015e-01, -1.0018e-02],\n",
      "        [ 7.2197e-01,  1.5908e-01],\n",
      "        [ 8.0267e-01,  1.4870e-01],\n",
      "        [ 8.9736e-01,  1.8248e-02],\n",
      "        [ 5.3386e-01,  2.2624e-04],\n",
      "        [ 8.6704e-01,  1.9802e-01],\n",
      "        [ 8.5282e-01,  1.8597e-01],\n",
      "        [ 6.2555e-01,  1.2968e-01],\n",
      "        [ 5.8092e-01, -1.3535e-01],\n",
      "        [ 2.8493e-01,  1.7325e-01],\n",
      "        [ 7.9213e-01,  3.3089e-01],\n",
      "        [ 7.5488e-01,  1.2350e-01],\n",
      "        [ 6.3409e-01,  2.2122e-01],\n",
      "        [ 7.6578e-01, -9.6783e-02],\n",
      "        [ 1.0543e+00, -5.1570e-02],\n",
      "        [ 4.4004e-01,  1.2559e-01]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8508, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6526,  0.0770],\n",
      "        [ 0.8888, -0.0331],\n",
      "        [ 0.8376, -0.0194],\n",
      "        [ 0.4886,  0.3021],\n",
      "        [ 0.7858,  0.2619],\n",
      "        [ 0.7459,  0.1092],\n",
      "        [ 0.7402,  0.0650],\n",
      "        [ 0.6458,  0.0496],\n",
      "        [ 0.5516, -0.0660],\n",
      "        [ 0.3751,  0.1485],\n",
      "        [ 0.6061, -0.0458],\n",
      "        [ 0.4345, -0.0975],\n",
      "        [ 0.6505, -0.0429],\n",
      "        [ 0.5036,  0.1305],\n",
      "        [ 1.1284,  0.0808],\n",
      "        [ 0.7469,  0.2935],\n",
      "        [ 0.6335,  0.3108],\n",
      "        [ 0.8309,  0.0635],\n",
      "        [ 0.9430,  0.1127],\n",
      "        [ 0.8340,  0.0912],\n",
      "        [ 1.0439,  0.1806],\n",
      "        [ 0.8597,  0.2427],\n",
      "        [ 0.3657, -0.1876],\n",
      "        [ 0.7722,  0.2117],\n",
      "        [ 1.1191,  0.3949],\n",
      "        [ 0.7585,  0.1403],\n",
      "        [ 0.7827,  0.1047],\n",
      "        [ 0.9858, -0.1364],\n",
      "        [ 0.8273, -0.0470],\n",
      "        [ 0.9893,  0.1511],\n",
      "        [ 0.6183,  0.1898],\n",
      "        [ 0.7789,  0.1354]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9035, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6982,  0.1347],\n",
      "        [ 0.6927,  0.1271],\n",
      "        [ 0.2476, -0.0768],\n",
      "        [ 0.3795,  0.0022],\n",
      "        [ 0.3195, -0.0407],\n",
      "        [ 0.7961,  0.0658],\n",
      "        [ 0.8866,  0.1856],\n",
      "        [ 0.4998,  0.0995],\n",
      "        [ 0.8377,  0.2133],\n",
      "        [ 0.8052, -0.0720],\n",
      "        [ 0.6116,  0.2068],\n",
      "        [ 0.8811,  0.1919],\n",
      "        [ 0.5610,  0.0209],\n",
      "        [ 0.5708, -0.0509],\n",
      "        [ 0.9302,  0.2757],\n",
      "        [ 0.7085,  0.2296],\n",
      "        [ 0.9692,  0.1662],\n",
      "        [ 0.5040,  0.1035],\n",
      "        [ 0.9885,  0.0502],\n",
      "        [ 0.6190,  0.1098],\n",
      "        [ 0.5413,  0.1282],\n",
      "        [ 0.9496,  0.1891],\n",
      "        [ 0.9779, -0.1152],\n",
      "        [ 0.8467,  0.0664],\n",
      "        [ 0.6938,  0.2740],\n",
      "        [ 0.7193,  0.0462],\n",
      "        [ 0.5238,  0.0919],\n",
      "        [ 0.7358,  0.1082],\n",
      "        [ 0.7759, -0.0840],\n",
      "        [ 0.7381,  0.1331],\n",
      "        [ 0.6190, -0.0580],\n",
      "        [ 0.7941,  0.0989]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.9224, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8538,  0.3536],\n",
      "        [ 0.6441,  0.1323],\n",
      "        [ 0.5382,  0.2227],\n",
      "        [ 0.5970,  0.2252],\n",
      "        [ 0.6995,  0.1221],\n",
      "        [ 0.6866,  0.1426],\n",
      "        [ 0.9552, -0.1100],\n",
      "        [ 0.6112,  0.0974],\n",
      "        [ 0.7678,  0.4062],\n",
      "        [ 0.2360,  0.1582],\n",
      "        [ 0.7410,  0.2054],\n",
      "        [ 0.7617, -0.0916],\n",
      "        [ 0.8213,  0.1213],\n",
      "        [ 1.0966,  0.1316],\n",
      "        [ 0.1082,  0.2849],\n",
      "        [ 0.7495, -0.1232],\n",
      "        [ 0.5722,  0.1603],\n",
      "        [ 0.7571, -0.0441],\n",
      "        [ 0.6981, -0.0725],\n",
      "        [ 0.4587, -0.0687],\n",
      "        [ 0.4576, -0.0822],\n",
      "        [ 0.4906,  0.1752],\n",
      "        [ 1.1325,  0.2204],\n",
      "        [ 0.7802, -0.0505],\n",
      "        [ 0.5479,  0.0682],\n",
      "        [ 0.5840,  0.1117],\n",
      "        [ 0.8587, -0.0063],\n",
      "        [ 0.6105, -0.0304],\n",
      "        [ 0.7348,  0.0928],\n",
      "        [ 0.8305,  0.3008],\n",
      "        [ 0.5793,  0.1637],\n",
      "        [ 1.0230,  0.0964]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8727, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 7.6927e-01, -1.4231e-01],\n",
      "        [ 8.7869e-01,  3.5298e-02],\n",
      "        [ 7.9138e-01,  6.1106e-02],\n",
      "        [ 8.7498e-01, -4.5552e-02],\n",
      "        [ 4.3454e-01,  8.3365e-04],\n",
      "        [ 8.7261e-01, -2.0353e-01],\n",
      "        [ 7.6706e-01,  6.7352e-02],\n",
      "        [ 3.2651e-01, -1.3035e-01],\n",
      "        [ 4.1257e-01,  2.3664e-01],\n",
      "        [ 6.1294e-01,  1.7595e-01],\n",
      "        [ 6.6303e-01,  3.0492e-01],\n",
      "        [ 9.7335e-01,  2.7733e-01],\n",
      "        [ 4.4369e-01, -4.1649e-02],\n",
      "        [ 1.0089e+00, -1.9997e-01],\n",
      "        [ 6.1703e-01,  7.8599e-02],\n",
      "        [ 7.5475e-01,  2.6898e-02],\n",
      "        [ 7.8921e-01,  1.5729e-01],\n",
      "        [ 7.1879e-01,  2.8447e-01],\n",
      "        [ 7.7350e-01,  3.3051e-01],\n",
      "        [ 6.4393e-01,  2.6333e-01],\n",
      "        [ 8.9691e-01,  2.2392e-02],\n",
      "        [ 5.2530e-01, -1.7025e-01],\n",
      "        [ 5.9892e-01,  2.0231e-01],\n",
      "        [ 8.6143e-01,  4.5733e-01],\n",
      "        [ 7.1492e-01,  2.7226e-01],\n",
      "        [ 6.7089e-01,  2.8888e-02],\n",
      "        [ 8.4061e-01,  9.6266e-02],\n",
      "        [ 7.4303e-01, -9.8849e-02],\n",
      "        [ 7.8264e-01,  1.7037e-01],\n",
      "        [ 8.7119e-01,  1.5877e-01],\n",
      "        [ 5.5681e-01,  5.5156e-02],\n",
      "        [ 6.6080e-01,  8.8255e-02]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.7721, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6073,  0.1274],\n",
      "        [ 0.7402,  0.2206],\n",
      "        [ 0.6712,  0.2940],\n",
      "        [ 0.5877,  0.0318],\n",
      "        [ 0.7184, -0.0100],\n",
      "        [ 0.8604,  0.3310],\n",
      "        [ 0.3732,  0.0149],\n",
      "        [ 0.8626, -0.0296],\n",
      "        [ 0.8366,  0.1996],\n",
      "        [ 0.6389,  0.0433],\n",
      "        [ 0.7101,  0.1739],\n",
      "        [ 0.6201,  0.0094],\n",
      "        [ 1.0015, -0.0862],\n",
      "        [ 0.8170,  0.0938],\n",
      "        [ 0.7787,  0.1571],\n",
      "        [ 0.9001,  0.0777],\n",
      "        [ 0.8714,  0.0904],\n",
      "        [ 0.8843,  0.0646],\n",
      "        [ 0.5072,  0.1823],\n",
      "        [ 0.7598,  0.0346],\n",
      "        [ 0.8041,  0.3178],\n",
      "        [ 0.4406,  0.1251],\n",
      "        [ 0.8105, -0.0124],\n",
      "        [ 0.7896,  0.0991],\n",
      "        [ 0.8337,  0.1385],\n",
      "        [ 0.7665,  0.2244],\n",
      "        [ 1.0491,  0.0017],\n",
      "        [ 0.7310,  0.2142],\n",
      "        [ 0.6511,  0.2404],\n",
      "        [ 0.3224,  0.1537],\n",
      "        [ 0.8147,  0.0142],\n",
      "        [ 0.7928,  0.0697]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8636, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 7.9642e-01,  1.4865e-01],\n",
      "        [ 8.9765e-01,  2.0608e-01],\n",
      "        [ 7.9513e-01, -5.7740e-02],\n",
      "        [ 6.6409e-01,  2.0001e-02],\n",
      "        [ 6.8138e-01,  3.6540e-01],\n",
      "        [ 6.3007e-01,  1.9961e-01],\n",
      "        [ 1.0527e+00,  4.0789e-02],\n",
      "        [ 8.1633e-01,  1.4189e-01],\n",
      "        [ 5.9204e-01, -6.2593e-02],\n",
      "        [ 7.0886e-01, -8.5366e-02],\n",
      "        [ 1.2521e+00,  4.6593e-02],\n",
      "        [ 6.4912e-01,  2.3183e-01],\n",
      "        [ 1.0026e+00,  1.6163e-01],\n",
      "        [ 7.3073e-01,  2.2565e-01],\n",
      "        [ 6.9655e-01,  1.1841e-01],\n",
      "        [ 6.0309e-01,  2.4954e-01],\n",
      "        [ 5.0306e-01,  1.6604e-01],\n",
      "        [ 9.7945e-01,  1.3853e-01],\n",
      "        [ 7.2606e-01,  1.4016e-01],\n",
      "        [ 7.7007e-01,  1.2758e-01],\n",
      "        [ 8.9063e-01,  5.5104e-02],\n",
      "        [ 8.9227e-01, -1.7590e-02],\n",
      "        [ 7.7244e-01,  8.1689e-02],\n",
      "        [ 5.4165e-01,  1.8814e-02],\n",
      "        [ 9.4841e-01,  8.9668e-02],\n",
      "        [ 6.0416e-01, -7.1769e-02],\n",
      "        [ 1.0605e+00,  8.9147e-04],\n",
      "        [ 6.2482e-01,  1.1825e-01],\n",
      "        [ 5.6832e-01,  2.4851e-01],\n",
      "        [ 8.8300e-01, -1.5279e-03],\n",
      "        [ 6.5923e-01,  1.5260e-01],\n",
      "        [ 8.2174e-01,  2.6219e-01]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.9099, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6074,  0.1241],\n",
      "        [ 0.7113,  0.1452],\n",
      "        [ 0.3622,  0.1334],\n",
      "        [ 0.4096,  0.3398],\n",
      "        [ 0.8274, -0.0616],\n",
      "        [ 0.5075,  0.1339],\n",
      "        [ 0.8782,  0.1534],\n",
      "        [ 0.5803, -0.0800],\n",
      "        [ 0.7305,  0.0641],\n",
      "        [ 0.7992,  0.0902],\n",
      "        [ 0.5799,  0.1263],\n",
      "        [ 0.4500,  0.4402],\n",
      "        [ 0.5796,  0.0267],\n",
      "        [ 0.8461,  0.1993],\n",
      "        [ 0.6926, -0.0846],\n",
      "        [ 0.5877,  0.2095],\n",
      "        [ 0.8724,  0.2426],\n",
      "        [ 0.8135, -0.0322],\n",
      "        [ 0.2648,  0.2165],\n",
      "        [ 0.6750,  0.2563],\n",
      "        [ 0.9215,  0.2031],\n",
      "        [ 0.5686,  0.1719],\n",
      "        [ 0.7159,  0.0221],\n",
      "        [ 0.9071,  0.1114],\n",
      "        [ 0.6251,  0.1764],\n",
      "        [ 0.7763,  0.2526],\n",
      "        [ 0.8906,  0.2163],\n",
      "        [ 0.5163, -0.0176],\n",
      "        [ 0.6630,  0.0536],\n",
      "        [ 0.5916,  0.0829],\n",
      "        [ 0.5763,  0.1814],\n",
      "        [ 0.8754,  0.0993]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8378, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7891,  0.1662],\n",
      "        [ 0.7346,  0.2888],\n",
      "        [ 0.7852,  0.0888],\n",
      "        [ 0.5781,  0.1907],\n",
      "        [ 0.7497, -0.0684],\n",
      "        [ 0.5626, -0.1650],\n",
      "        [ 0.5591, -0.1368],\n",
      "        [ 1.2067,  0.3229],\n",
      "        [ 0.9445,  0.0582],\n",
      "        [ 0.3248,  0.0120],\n",
      "        [ 1.0442,  0.0808],\n",
      "        [ 0.7896,  0.1115],\n",
      "        [ 0.7015,  0.2692],\n",
      "        [ 0.7458,  0.2681],\n",
      "        [ 0.7064,  0.0857],\n",
      "        [ 0.6979,  0.0970],\n",
      "        [ 1.0245, -0.1668],\n",
      "        [ 0.9308, -0.0574],\n",
      "        [ 0.5819,  0.0519],\n",
      "        [ 0.5427,  0.1997],\n",
      "        [ 0.8157,  0.1302],\n",
      "        [ 0.5664, -0.0630],\n",
      "        [ 0.4055,  0.0149],\n",
      "        [ 0.6981,  0.0033],\n",
      "        [ 0.9093, -0.2026],\n",
      "        [ 0.6530,  0.0548],\n",
      "        [ 0.8459,  0.2599],\n",
      "        [ 0.9696,  0.0564],\n",
      "        [ 0.8896,  0.1582],\n",
      "        [ 0.8346,  0.0870],\n",
      "        [ 0.8895,  0.2260],\n",
      "        [ 0.5543, -0.0967]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8824, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7314,  0.3436],\n",
      "        [ 0.7232,  0.0585],\n",
      "        [ 0.6943,  0.0112],\n",
      "        [ 0.5884,  0.3060],\n",
      "        [ 0.7038,  0.0530],\n",
      "        [ 0.8167,  0.3503],\n",
      "        [ 0.5290,  0.1624],\n",
      "        [ 0.6353,  0.2168],\n",
      "        [ 0.9041, -0.0189],\n",
      "        [ 0.6407,  0.0699],\n",
      "        [ 0.5559,  0.1558],\n",
      "        [ 0.4065, -0.0191],\n",
      "        [ 0.6018,  0.0724],\n",
      "        [ 0.9670,  0.0811],\n",
      "        [ 0.8575,  0.1593],\n",
      "        [ 0.8260, -0.1348],\n",
      "        [ 0.6006, -0.0468],\n",
      "        [ 0.7912,  0.0263],\n",
      "        [ 0.8395, -0.0966],\n",
      "        [ 0.7581,  0.1967],\n",
      "        [ 0.7515, -0.0357],\n",
      "        [ 0.7215,  0.1194],\n",
      "        [ 0.6766,  0.1581],\n",
      "        [ 0.5357,  0.0449],\n",
      "        [ 0.6090,  0.2185],\n",
      "        [ 1.0491,  0.1782],\n",
      "        [ 0.4914,  0.0872],\n",
      "        [ 0.6886,  0.2269],\n",
      "        [ 0.7335,  0.0196],\n",
      "        [ 0.6982, -0.0447],\n",
      "        [ 0.7531,  0.0660],\n",
      "        [ 0.8776, -0.1303]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8862, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5004, -0.0592],\n",
      "        [ 0.5319, -0.3386],\n",
      "        [ 0.4493, -0.0196],\n",
      "        [ 0.8182, -0.1088],\n",
      "        [ 0.7981,  0.2735],\n",
      "        [ 0.3069,  0.0738],\n",
      "        [ 0.7769,  0.0864],\n",
      "        [ 0.8030,  0.2416],\n",
      "        [ 1.0070,  0.0709],\n",
      "        [ 0.5018,  0.1100],\n",
      "        [ 0.9441, -0.0087],\n",
      "        [ 0.6214,  0.2431],\n",
      "        [ 0.3643,  0.1437],\n",
      "        [ 0.9028,  0.0152],\n",
      "        [ 0.4573,  0.1515],\n",
      "        [ 0.8342,  0.3112],\n",
      "        [ 0.7331,  0.0940],\n",
      "        [ 0.5718,  0.1573],\n",
      "        [ 0.4037,  0.0085],\n",
      "        [ 1.2326,  0.3160],\n",
      "        [ 0.3279, -0.1875],\n",
      "        [ 0.7522, -0.0470],\n",
      "        [ 0.7437, -0.4101],\n",
      "        [ 0.9446,  0.1450],\n",
      "        [ 0.6212,  0.1776],\n",
      "        [-0.2259,  0.2626],\n",
      "        [ 0.3813,  0.0055],\n",
      "        [ 0.6822,  0.1648],\n",
      "        [ 0.6544,  0.0379],\n",
      "        [ 0.9083,  0.4685],\n",
      "        [ 0.6948,  0.2216],\n",
      "        [ 0.2976,  0.0799]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.9560, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4369,  0.1151],\n",
      "        [ 0.6591, -0.0231],\n",
      "        [ 0.4936, -0.0555],\n",
      "        [ 0.8585, -0.1955],\n",
      "        [ 0.6960,  0.2000],\n",
      "        [ 0.7402,  0.0167],\n",
      "        [ 0.3702, -0.3514],\n",
      "        [ 0.6854,  0.1387],\n",
      "        [ 0.8792,  0.1333],\n",
      "        [ 0.5952, -0.1746],\n",
      "        [ 0.4953,  0.0961],\n",
      "        [ 1.2374,  0.0602],\n",
      "        [ 0.4632, -0.0441],\n",
      "        [ 0.6931,  0.0895],\n",
      "        [ 0.6281, -0.0018],\n",
      "        [ 0.4461,  0.1298],\n",
      "        [ 0.5768,  0.1452],\n",
      "        [ 0.6523,  0.1162],\n",
      "        [ 0.7827, -0.1571],\n",
      "        [ 0.9590, -0.2485],\n",
      "        [ 0.7191, -0.0137],\n",
      "        [ 0.8257,  0.2056],\n",
      "        [ 0.9226, -0.0834],\n",
      "        [ 0.7039,  0.1998],\n",
      "        [ 0.8465,  0.3023],\n",
      "        [ 0.5923, -0.0795],\n",
      "        [ 0.7434,  0.0068],\n",
      "        [ 0.9323, -0.1969],\n",
      "        [ 0.4844,  0.0678],\n",
      "        [ 0.8600,  0.0937],\n",
      "        [ 0.6294,  0.2630],\n",
      "        [ 0.9536,  0.0619]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8991, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8383, -0.0048],\n",
      "        [ 0.8612,  0.0726],\n",
      "        [ 0.6314,  0.0998],\n",
      "        [ 1.0451,  0.0026],\n",
      "        [ 0.6847,  0.2813],\n",
      "        [ 0.8705,  0.2018],\n",
      "        [ 0.6792,  0.1949],\n",
      "        [ 0.5235, -0.1219],\n",
      "        [ 0.7606,  0.1750],\n",
      "        [ 0.5124,  0.1494],\n",
      "        [ 0.6050, -0.0546],\n",
      "        [ 0.7618, -0.1824],\n",
      "        [ 0.7104, -0.1454],\n",
      "        [ 0.8594,  0.2425],\n",
      "        [ 0.9174,  0.0430],\n",
      "        [ 0.4187,  0.1111],\n",
      "        [ 0.9201,  0.1262],\n",
      "        [ 0.8788,  0.1522],\n",
      "        [ 0.7641,  0.0588],\n",
      "        [ 0.5963,  0.0869],\n",
      "        [ 0.8566,  0.0383],\n",
      "        [ 1.0314,  0.0723],\n",
      "        [ 0.7301, -0.0498],\n",
      "        [ 0.6512,  0.0474],\n",
      "        [ 0.9232,  0.0499],\n",
      "        [ 0.6344,  0.2790],\n",
      "        [ 0.5105, -0.2501],\n",
      "        [ 0.9841,  0.2705],\n",
      "        [ 0.7774,  0.0917],\n",
      "        [ 0.5752,  0.1246],\n",
      "        [ 0.7132,  0.1851],\n",
      "        [ 0.4642,  0.0021]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8712, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7281,  0.1491],\n",
      "        [ 0.9354,  0.0172],\n",
      "        [ 0.8311,  0.2110],\n",
      "        [ 0.7032,  0.0725],\n",
      "        [ 0.7516,  0.0300],\n",
      "        [ 0.8127,  0.0046],\n",
      "        [ 0.9454,  0.0879],\n",
      "        [ 0.6821,  0.0415],\n",
      "        [ 0.6150, -0.1934],\n",
      "        [ 0.5581, -0.2609],\n",
      "        [ 0.7063, -0.0410],\n",
      "        [ 0.4474,  0.3106],\n",
      "        [ 0.5499,  0.0439],\n",
      "        [ 0.7264,  0.0629],\n",
      "        [ 0.6415,  0.1132],\n",
      "        [ 0.4769, -0.0792],\n",
      "        [ 0.8184,  0.0566],\n",
      "        [ 0.8129,  0.2594],\n",
      "        [ 0.4554,  0.2134],\n",
      "        [ 0.7283,  0.1416],\n",
      "        [ 0.4531, -0.1430],\n",
      "        [ 0.4347, -0.1342],\n",
      "        [ 0.7448,  0.2347],\n",
      "        [ 0.6395, -0.0238],\n",
      "        [ 0.8384,  0.1487],\n",
      "        [ 0.8083, -0.0055],\n",
      "        [ 0.5086,  0.1290],\n",
      "        [ 0.6708,  0.0976],\n",
      "        [ 0.9094,  0.1343],\n",
      "        [ 0.7446,  0.2233],\n",
      "        [ 1.0151,  0.0189],\n",
      "        [ 0.8436,  0.0987]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8307, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5818,  0.2152],\n",
      "        [ 0.4554,  0.0037],\n",
      "        [ 0.6746,  0.1696],\n",
      "        [ 0.8988,  0.1817],\n",
      "        [ 0.7295,  0.1320],\n",
      "        [ 1.0187,  0.1442],\n",
      "        [ 0.5867, -0.0897],\n",
      "        [ 0.9674,  0.1392],\n",
      "        [ 1.0199,  0.3211],\n",
      "        [ 0.3565, -0.0075],\n",
      "        [ 0.5074,  0.2165],\n",
      "        [ 0.4939,  0.0931],\n",
      "        [ 1.1214,  0.1918],\n",
      "        [ 0.9173,  0.0402],\n",
      "        [ 0.7202,  0.2089],\n",
      "        [ 0.7396, -0.0901],\n",
      "        [ 0.8440,  0.0380],\n",
      "        [ 0.7931, -0.1129],\n",
      "        [ 0.6309,  0.0328],\n",
      "        [ 0.9199,  0.1757],\n",
      "        [ 0.8089, -0.0623],\n",
      "        [ 0.7596, -0.0794],\n",
      "        [ 0.6447,  0.1324],\n",
      "        [ 0.5386, -0.0527],\n",
      "        [ 0.9582,  0.4165],\n",
      "        [ 0.7618,  0.1487],\n",
      "        [ 0.4159,  0.1331],\n",
      "        [ 0.4407,  0.0624],\n",
      "        [ 0.5104, -0.0071],\n",
      "        [ 0.7292,  0.1071],\n",
      "        [ 0.4697,  0.0837],\n",
      "        [ 0.7495,  0.0326]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.9496, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3608,  0.0129],\n",
      "        [ 0.6937,  0.0701],\n",
      "        [ 0.7520,  0.1981],\n",
      "        [ 0.3987,  0.1300],\n",
      "        [ 0.3444,  0.1677],\n",
      "        [ 0.8752,  0.1993],\n",
      "        [ 0.7192, -0.5560],\n",
      "        [ 0.7340,  0.3411],\n",
      "        [ 0.6779,  0.2379],\n",
      "        [ 1.0999,  0.0061],\n",
      "        [ 0.8595,  0.3814],\n",
      "        [ 0.5421,  0.0899],\n",
      "        [ 0.5097, -0.1236],\n",
      "        [ 0.8405,  0.0773],\n",
      "        [ 0.4800, -0.0578],\n",
      "        [ 0.8213,  0.0266],\n",
      "        [ 0.3522, -0.0341],\n",
      "        [ 0.0465, -0.0748],\n",
      "        [ 0.3437, -0.0124],\n",
      "        [ 0.9037, -0.1093],\n",
      "        [ 0.4607, -0.0186],\n",
      "        [ 0.5001,  0.1758],\n",
      "        [ 0.6009, -0.0806],\n",
      "        [ 1.0558,  0.0084],\n",
      "        [ 0.7687, -0.2785],\n",
      "        [ 0.7748,  0.1457],\n",
      "        [ 0.9937,  0.2033],\n",
      "        [ 0.5135,  0.0814],\n",
      "        [ 0.5626,  0.0334],\n",
      "        [-0.2489,  0.0732],\n",
      "        [ 0.6664, -0.0209],\n",
      "        [ 1.0041, -0.0059]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.7777, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7951,  0.1269],\n",
      "        [ 0.9010,  0.0013],\n",
      "        [ 0.6768,  0.1214],\n",
      "        [ 1.1246,  0.0309],\n",
      "        [ 0.8720,  0.2303],\n",
      "        [ 0.7182, -0.1449],\n",
      "        [ 0.6239,  0.0316],\n",
      "        [ 1.1567,  0.2225],\n",
      "        [ 0.5052, -0.0930],\n",
      "        [ 0.4994,  0.0130],\n",
      "        [ 0.5927,  0.0101],\n",
      "        [ 0.8266,  0.0144],\n",
      "        [ 0.9956,  0.1524],\n",
      "        [ 0.8868, -0.1453],\n",
      "        [ 0.7936, -0.0202],\n",
      "        [ 0.7186,  0.2986],\n",
      "        [ 0.9618,  0.0819],\n",
      "        [ 0.6665,  0.1777],\n",
      "        [ 0.7936,  0.2755],\n",
      "        [ 0.5548,  0.1353],\n",
      "        [ 1.0467,  0.0969],\n",
      "        [ 0.6874,  0.0919],\n",
      "        [ 0.9148,  0.2074],\n",
      "        [ 0.6022,  0.1652],\n",
      "        [ 0.6089,  0.2235],\n",
      "        [ 0.6203,  0.0489],\n",
      "        [ 0.7784, -0.1253],\n",
      "        [ 0.8111, -0.0863],\n",
      "        [ 0.6420,  0.0641],\n",
      "        [ 0.5426,  0.0886],\n",
      "        [ 0.8184,  0.3195],\n",
      "        [ 0.5376,  0.2960]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8662, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7164,  0.0374],\n",
      "        [ 0.5565,  0.1711],\n",
      "        [ 0.7608,  0.1551],\n",
      "        [ 0.6486, -0.1121],\n",
      "        [ 0.6760, -0.1138],\n",
      "        [ 0.6537,  0.1783],\n",
      "        [ 0.7084,  0.0764],\n",
      "        [ 0.8037, -0.1690],\n",
      "        [ 0.7182,  0.1370],\n",
      "        [ 0.9128, -0.2001],\n",
      "        [ 0.5302,  0.1214],\n",
      "        [ 0.9604, -0.1422],\n",
      "        [ 0.6856,  0.0882],\n",
      "        [ 0.7828,  0.1687],\n",
      "        [ 0.7122,  0.0273],\n",
      "        [ 0.3023,  0.1253],\n",
      "        [ 0.7303,  0.4555],\n",
      "        [ 1.0439, -0.0327],\n",
      "        [ 0.8266, -0.0079],\n",
      "        [ 0.6640,  0.2218],\n",
      "        [ 0.8909, -0.0476],\n",
      "        [ 0.5073,  0.0978],\n",
      "        [ 0.7513,  0.0014],\n",
      "        [ 0.7739, -0.1862],\n",
      "        [ 0.6809,  0.0324],\n",
      "        [ 0.9102,  0.0217],\n",
      "        [ 0.9765,  0.0921],\n",
      "        [ 0.6536,  0.1226],\n",
      "        [ 0.7335,  0.0097],\n",
      "        [ 0.4146,  0.1217],\n",
      "        [ 0.8569,  0.1517],\n",
      "        [ 0.6768,  0.1493]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8749, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0470, -0.0860],\n",
      "        [ 1.0887,  0.0083],\n",
      "        [ 0.9396, -0.0346],\n",
      "        [ 0.7363,  0.1377],\n",
      "        [ 0.6630,  0.0590],\n",
      "        [ 0.5191,  0.1370],\n",
      "        [ 0.7581, -0.0742],\n",
      "        [ 1.1135,  0.0419],\n",
      "        [ 0.8453,  0.2084],\n",
      "        [ 0.9800,  0.0976],\n",
      "        [ 0.9503,  0.1463],\n",
      "        [ 0.8794,  0.1938],\n",
      "        [ 0.7391,  0.0681],\n",
      "        [ 0.4873, -0.1949],\n",
      "        [ 0.5290,  0.2790],\n",
      "        [ 0.3669,  0.0760],\n",
      "        [ 1.0399,  0.5687],\n",
      "        [ 0.7606, -0.0179],\n",
      "        [ 0.6348, -0.0467],\n",
      "        [ 0.4043, -0.1006],\n",
      "        [ 0.7899, -0.0365],\n",
      "        [ 0.8490,  0.2291],\n",
      "        [ 0.1304,  0.2801],\n",
      "        [ 0.6821,  0.0731],\n",
      "        [ 0.9971,  0.0450],\n",
      "        [ 0.8304, -0.0315],\n",
      "        [ 0.8985,  0.3027],\n",
      "        [ 0.7915,  0.1919],\n",
      "        [ 0.9036, -0.0669],\n",
      "        [ 0.5530,  0.0541],\n",
      "        [ 0.6644, -0.1805],\n",
      "        [ 0.6487,  0.2249]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.8109, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 5.0479e-01,  1.5521e-02],\n",
      "        [ 8.8947e-01, -3.2422e-02],\n",
      "        [ 5.8626e-01, -7.6267e-02],\n",
      "        [ 7.7864e-01,  1.2062e-01],\n",
      "        [ 6.9787e-01,  5.7763e-02],\n",
      "        [ 5.8390e-01,  1.2058e-01],\n",
      "        [ 5.0750e-01,  1.2849e-01],\n",
      "        [ 5.9040e-01,  2.6257e-02],\n",
      "        [ 9.7084e-01,  3.4711e-02],\n",
      "        [ 5.7289e-01,  1.9223e-01],\n",
      "        [ 6.4002e-01,  3.2677e-01],\n",
      "        [ 9.7977e-01, -1.0686e-01],\n",
      "        [ 7.3613e-01, -3.9263e-02],\n",
      "        [ 8.2983e-01, -8.4656e-04],\n",
      "        [ 6.5438e-01,  1.8400e-01],\n",
      "        [ 8.3243e-01, -2.4774e-02],\n",
      "        [ 5.7499e-01,  2.6476e-01],\n",
      "        [ 6.7166e-01,  3.0294e-02],\n",
      "        [ 7.3358e-01,  1.6906e-01],\n",
      "        [ 8.4095e-01,  2.5172e-01],\n",
      "        [ 6.8120e-01,  9.3918e-02],\n",
      "        [ 6.6408e-01,  1.3609e-01],\n",
      "        [ 7.8565e-01, -2.7183e-02],\n",
      "        [ 6.7563e-01,  2.5419e-01],\n",
      "        [ 3.7400e-01,  1.7818e-01],\n",
      "        [ 9.0222e-01, -2.6129e-02],\n",
      "        [ 4.1923e-01, -1.3309e-01],\n",
      "        [ 5.7471e-01,  4.0944e-02],\n",
      "        [ 6.3749e-01, -5.6096e-02],\n",
      "        [ 2.4363e-01,  4.3210e-02],\n",
      "        [ 5.5507e-01,  5.8447e-02],\n",
      "        [ 5.1930e-01, -6.4241e-02]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(1.0020, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 6.7873e-01,  1.9285e-02],\n",
      "        [ 8.1208e-01,  1.9850e-01],\n",
      "        [ 6.6983e-01,  8.7456e-02],\n",
      "        [ 5.9526e-01, -3.4378e-01],\n",
      "        [ 7.8674e-01, -2.8916e-02],\n",
      "        [ 1.0492e+00,  2.8530e-01],\n",
      "        [ 4.3077e-01,  5.4735e-02],\n",
      "        [ 5.2685e-01, -4.6895e-02],\n",
      "        [ 7.7838e-01, -3.6721e-02],\n",
      "        [ 6.3857e-01, -7.3588e-02],\n",
      "        [ 6.3419e-01,  1.4542e-01],\n",
      "        [ 5.8289e-01,  9.8839e-02],\n",
      "        [ 1.0267e+00, -1.1216e-01],\n",
      "        [ 8.5244e-01, -1.2710e-02],\n",
      "        [ 5.1360e-01, -2.7124e-01],\n",
      "        [ 9.0412e-01,  8.3933e-02],\n",
      "        [ 8.1400e-01,  1.4541e-01],\n",
      "        [ 5.6880e-01,  7.2999e-02],\n",
      "        [ 4.2507e-01, -8.5789e-02],\n",
      "        [ 7.7024e-01, -2.4557e-01],\n",
      "        [ 7.8113e-01,  3.5795e-03],\n",
      "        [ 1.0060e+00,  1.7617e-01],\n",
      "        [ 8.1011e-01,  1.3725e-01],\n",
      "        [ 1.0826e+00,  1.3083e-01],\n",
      "        [ 7.9503e-01,  1.2827e-01],\n",
      "        [ 9.0045e-01,  6.4983e-03],\n",
      "        [ 8.6674e-01,  1.2760e-01],\n",
      "        [ 3.8149e-01,  6.8830e-02],\n",
      "        [ 6.6250e-01,  8.2240e-05],\n",
      "        [ 9.3634e-01,  2.4198e-01],\n",
      "        [ 4.4338e-01,  9.2537e-02],\n",
      "        [ 1.0335e+00,  1.9187e-01]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.7325, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8090,  0.0369],\n",
      "        [ 0.6130,  0.2594],\n",
      "        [ 0.7859, -0.0241],\n",
      "        [ 0.7433,  0.2121],\n",
      "        [ 0.8759,  0.0224],\n",
      "        [ 1.0182, -0.0626],\n",
      "        [ 0.5088, -0.1381],\n",
      "        [ 0.7239, -0.2157],\n",
      "        [ 0.5901,  0.2673],\n",
      "        [ 0.5400,  0.2459],\n",
      "        [ 0.5066,  0.0195],\n",
      "        [ 0.5434, -0.0607],\n",
      "        [ 0.8516, -0.0763],\n",
      "        [ 0.6256,  0.1934],\n",
      "        [ 0.9843,  0.0552],\n",
      "        [ 0.4991,  0.5544],\n",
      "        [ 0.3963,  0.1405],\n",
      "        [ 0.8232, -0.0943],\n",
      "        [ 0.6084, -0.0905],\n",
      "        [ 0.9068, -0.1388],\n",
      "        [ 0.9534,  0.1119],\n",
      "        [ 0.7421,  0.2030],\n",
      "        [ 0.6615, -0.1642],\n",
      "        [ 0.3035,  0.1432],\n",
      "        [ 0.7224, -0.1564],\n",
      "        [ 0.9115,  0.0337],\n",
      "        [ 0.9521,  0.1332],\n",
      "        [ 0.7108,  0.1323],\n",
      "        [ 0.5900, -0.0237],\n",
      "        [ 0.8399,  0.0830],\n",
      "        [ 0.8604, -0.0086],\n",
      "        [ 0.5801,  0.1845]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8340, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 4.3942e-01,  1.0851e-01],\n",
      "        [ 5.6447e-01,  1.6589e-01],\n",
      "        [ 7.4098e-01,  5.5361e-02],\n",
      "        [ 6.5599e-01,  8.6070e-02],\n",
      "        [ 6.4930e-01,  6.4372e-02],\n",
      "        [ 8.3676e-01,  8.8539e-02],\n",
      "        [ 1.0567e+00,  7.0071e-02],\n",
      "        [ 5.3317e-01,  9.8054e-02],\n",
      "        [ 6.6407e-01,  1.4212e-01],\n",
      "        [ 6.4341e-01, -3.5672e-02],\n",
      "        [ 4.5658e-01, -1.6987e-01],\n",
      "        [ 8.0184e-01,  2.2880e-02],\n",
      "        [ 9.6402e-01,  1.9873e-01],\n",
      "        [ 7.6005e-01,  4.5202e-02],\n",
      "        [ 6.6937e-01,  6.5000e-02],\n",
      "        [ 6.3305e-01,  1.7618e-01],\n",
      "        [ 7.4405e-01,  4.4008e-03],\n",
      "        [ 6.4220e-01,  7.7222e-02],\n",
      "        [ 6.0304e-01,  1.4943e-01],\n",
      "        [ 1.2465e+00, -1.5820e-01],\n",
      "        [ 5.8085e-01,  7.5922e-02],\n",
      "        [-1.6765e-01,  5.3047e-02],\n",
      "        [ 4.9289e-01,  8.8396e-02],\n",
      "        [ 4.7929e-01, -5.9163e-02],\n",
      "        [ 7.4526e-01,  2.8164e-01],\n",
      "        [ 6.3898e-01, -2.0991e-02],\n",
      "        [ 6.0262e-01,  1.3895e-01],\n",
      "        [ 7.4245e-01,  9.2561e-02],\n",
      "        [ 6.5408e-01,  2.7488e-01],\n",
      "        [ 7.0572e-01, -3.9090e-04],\n",
      "        [ 5.2920e-01, -1.2313e-01],\n",
      "        [ 6.3343e-01, -1.5506e-02]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.9202, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6071, -0.2246],\n",
      "        [ 0.4554,  0.0588],\n",
      "        [ 0.6871,  0.0497],\n",
      "        [ 0.5794, -0.2755],\n",
      "        [ 0.9332, -0.0641],\n",
      "        [ 1.1378,  0.0557],\n",
      "        [ 0.6918,  0.0592],\n",
      "        [ 1.0826,  0.1337],\n",
      "        [ 0.8292, -0.0057],\n",
      "        [ 0.6812,  0.2155],\n",
      "        [ 0.4515,  0.0452],\n",
      "        [ 0.5141,  0.1322],\n",
      "        [ 0.5683, -0.1017],\n",
      "        [ 0.5783, -0.1445],\n",
      "        [ 0.7518,  0.1303],\n",
      "        [ 0.5547,  0.2305],\n",
      "        [ 0.7014,  0.1598],\n",
      "        [ 0.6108, -0.0103],\n",
      "        [ 0.7278,  0.0344],\n",
      "        [ 0.8883,  0.0926],\n",
      "        [ 0.6164,  0.0265],\n",
      "        [ 0.8420,  0.0825],\n",
      "        [ 0.6604,  0.0038],\n",
      "        [ 0.7616,  0.1585],\n",
      "        [ 0.5801,  0.0198],\n",
      "        [ 0.8235,  0.1315],\n",
      "        [ 0.7118,  0.2497],\n",
      "        [ 0.7348,  0.0398],\n",
      "        [ 0.6430,  0.0425],\n",
      "        [ 0.7981,  0.0702],\n",
      "        [ 0.8408,  0.1045],\n",
      "        [ 0.7105,  0.0351]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8564, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 6.6223e-01,  7.9292e-02],\n",
      "        [ 6.2797e-01, -2.4774e-01],\n",
      "        [ 1.3425e-01, -2.8176e-02],\n",
      "        [ 7.6076e-01,  1.1180e-01],\n",
      "        [ 1.0747e+00, -8.1046e-02],\n",
      "        [ 8.3568e-01,  1.4381e-01],\n",
      "        [ 9.9071e-01,  1.1404e-01],\n",
      "        [ 9.8376e-01,  1.0523e-01],\n",
      "        [ 7.1426e-01, -3.3341e-01],\n",
      "        [ 6.8662e-01,  8.0599e-03],\n",
      "        [ 7.4748e-01, -8.9980e-02],\n",
      "        [ 8.2513e-01,  5.4033e-02],\n",
      "        [ 8.5702e-01, -1.2206e-01],\n",
      "        [ 8.1778e-01, -2.0185e-01],\n",
      "        [ 7.5954e-01, -3.0893e-02],\n",
      "        [ 7.8557e-01,  3.5315e-01],\n",
      "        [ 7.2564e-01,  1.2204e-02],\n",
      "        [ 5.9546e-01,  9.4383e-02],\n",
      "        [ 1.0181e+00,  1.7170e-01],\n",
      "        [ 6.1406e-01,  2.1300e-01],\n",
      "        [ 8.1028e-01,  1.4672e-01],\n",
      "        [ 8.1689e-01,  2.3789e-01],\n",
      "        [ 4.1057e-01,  1.1301e-01],\n",
      "        [ 9.1128e-01, -6.5245e-05],\n",
      "        [ 6.9602e-01,  7.2939e-02],\n",
      "        [ 8.3298e-01,  1.2915e-01],\n",
      "        [ 7.9839e-01, -6.8545e-03],\n",
      "        [ 7.4109e-01, -7.0476e-02],\n",
      "        [ 6.0866e-01,  8.0900e-02],\n",
      "        [ 7.3345e-01, -1.7326e-01],\n",
      "        [ 7.9704e-01,  6.9401e-02],\n",
      "        [ 4.0806e-01, -1.6492e-01]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9732, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6901,  0.2635],\n",
      "        [ 0.8988,  0.0188],\n",
      "        [ 0.5104, -0.0746],\n",
      "        [ 0.7074, -0.1503],\n",
      "        [ 0.6462,  0.0123],\n",
      "        [ 0.8536,  0.2596],\n",
      "        [ 0.7692,  0.0559],\n",
      "        [ 0.7370, -0.0476],\n",
      "        [ 0.7510,  0.2229],\n",
      "        [ 0.7916,  0.2732],\n",
      "        [ 0.8423, -0.0322],\n",
      "        [ 0.7586,  0.2020],\n",
      "        [ 0.7158,  0.0910],\n",
      "        [ 0.8452,  0.0049],\n",
      "        [ 0.8998, -0.2636],\n",
      "        [ 0.5768,  0.0890],\n",
      "        [ 0.7125, -0.1101],\n",
      "        [ 0.4128,  0.1762],\n",
      "        [ 0.9941, -0.3708],\n",
      "        [ 0.7838, -0.1520],\n",
      "        [ 0.6445,  0.1326],\n",
      "        [ 0.8186,  0.0534],\n",
      "        [ 0.9124,  0.0642],\n",
      "        [ 0.8285,  0.0719],\n",
      "        [ 0.9609,  0.0017],\n",
      "        [ 0.7585,  0.4251],\n",
      "        [ 0.7795,  0.1605],\n",
      "        [ 0.5819,  0.0560],\n",
      "        [ 0.8879,  0.1778],\n",
      "        [ 0.8582,  0.1769],\n",
      "        [ 0.7913, -0.0271],\n",
      "        [ 0.8299,  0.1759]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8935, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7712,  0.1298],\n",
      "        [ 0.6386, -0.3648],\n",
      "        [ 0.9954,  0.0847],\n",
      "        [ 0.6175,  0.1236],\n",
      "        [ 0.8574, -0.0823],\n",
      "        [ 1.0442,  0.3962],\n",
      "        [ 0.6589, -0.0012],\n",
      "        [ 0.7455,  0.3641],\n",
      "        [ 0.7936, -0.0076],\n",
      "        [ 0.6363,  0.2899],\n",
      "        [ 0.8921,  0.1327],\n",
      "        [ 0.8409, -0.0324],\n",
      "        [ 0.5065, -0.2942],\n",
      "        [ 0.9225, -0.1777],\n",
      "        [ 0.7344, -0.0686],\n",
      "        [ 0.9307,  0.1719],\n",
      "        [ 0.8340,  0.0070],\n",
      "        [ 0.7960, -0.0605],\n",
      "        [ 0.8624,  0.1112],\n",
      "        [ 0.8751,  0.3095],\n",
      "        [ 0.7887,  0.2089],\n",
      "        [ 0.7802,  0.0558],\n",
      "        [ 0.4233, -0.0069],\n",
      "        [ 0.6613, -0.0844],\n",
      "        [ 0.6355,  0.0347],\n",
      "        [ 0.7968,  0.0076],\n",
      "        [ 0.7915, -0.2641],\n",
      "        [ 0.9279, -0.0966],\n",
      "        [-0.1714,  0.0641],\n",
      "        [ 0.8830,  0.1904],\n",
      "        [ 0.7175,  0.0280],\n",
      "        [ 0.5538, -0.0594]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.7439, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 9.7247e-01, -2.7992e-02],\n",
      "        [ 6.8343e-01,  1.4520e-01],\n",
      "        [ 4.8800e-01,  2.4791e-01],\n",
      "        [ 9.2142e-01,  3.6532e-02],\n",
      "        [ 7.4188e-01,  2.8301e-02],\n",
      "        [ 9.4048e-01,  1.8298e-01],\n",
      "        [ 3.4853e-01,  2.3636e-01],\n",
      "        [ 8.1687e-01,  6.9084e-02],\n",
      "        [ 8.8453e-01, -4.6557e-02],\n",
      "        [ 8.2548e-01, -1.8013e-01],\n",
      "        [ 6.1245e-01, -4.4421e-02],\n",
      "        [ 9.4811e-01, -5.1683e-04],\n",
      "        [ 5.9158e-01,  1.2501e-01],\n",
      "        [ 7.7510e-01,  3.1443e-01],\n",
      "        [ 4.0183e-01,  1.9423e-01],\n",
      "        [ 9.0979e-01,  2.4973e-01],\n",
      "        [ 7.6953e-01,  2.6811e-03],\n",
      "        [ 4.3062e-01,  2.5756e-03],\n",
      "        [ 1.0927e+00,  2.8873e-01],\n",
      "        [ 7.5524e-01, -6.9609e-02],\n",
      "        [ 6.6579e-01,  3.1233e-01],\n",
      "        [ 1.1717e+00,  1.4322e-01],\n",
      "        [ 5.1316e-01,  3.4667e-02],\n",
      "        [ 9.8884e-01,  3.7953e-01],\n",
      "        [ 9.0732e-01, -3.4857e-02],\n",
      "        [ 3.9709e-01, -2.0638e-01],\n",
      "        [ 7.1398e-01,  1.6463e-01],\n",
      "        [ 7.9530e-01, -2.6193e-02],\n",
      "        [ 5.2949e-01,  1.3141e-01],\n",
      "        [ 1.1944e+00,  2.9356e-03],\n",
      "        [ 7.0797e-01,  3.1174e-02],\n",
      "        [ 6.5174e-01,  2.2770e-01]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9396, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9772,  0.1432],\n",
      "        [ 0.6817,  0.0825],\n",
      "        [ 0.8409, -0.1019],\n",
      "        [ 0.5722,  0.3492],\n",
      "        [ 0.5907, -0.0754],\n",
      "        [ 0.6669,  0.0047],\n",
      "        [ 1.3011, -0.1222],\n",
      "        [ 0.9982,  0.1345],\n",
      "        [ 0.6371,  0.1276],\n",
      "        [ 0.6762,  0.0709],\n",
      "        [ 0.8510,  0.0361],\n",
      "        [ 0.4390,  0.0799],\n",
      "        [ 0.7308,  0.1929],\n",
      "        [ 0.9185, -0.1494],\n",
      "        [ 0.8520,  0.0409],\n",
      "        [ 0.6940, -0.0128],\n",
      "        [ 0.7414,  0.0762],\n",
      "        [ 0.8745,  0.2754],\n",
      "        [ 0.7294,  0.2556],\n",
      "        [ 0.6549,  0.1326],\n",
      "        [ 0.6477,  0.0083],\n",
      "        [ 0.7051,  0.0303],\n",
      "        [ 0.8904,  0.0047],\n",
      "        [ 0.7204, -0.2158],\n",
      "        [ 1.0239,  0.0362],\n",
      "        [ 0.6546,  0.0966],\n",
      "        [ 0.5623,  0.1247],\n",
      "        [ 0.7666,  0.0449],\n",
      "        [ 0.5190,  0.1262],\n",
      "        [ 0.8280, -0.1459],\n",
      "        [ 0.7982,  0.0129],\n",
      "        [ 0.5997,  0.1669]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8355, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9950, -0.0767],\n",
      "        [ 0.6583,  0.0628],\n",
      "        [ 1.0191,  0.0095],\n",
      "        [ 0.5778,  0.1919],\n",
      "        [ 0.6726,  0.1284],\n",
      "        [ 0.9514,  0.2044],\n",
      "        [ 0.7039,  0.0109],\n",
      "        [ 0.5348, -0.0802],\n",
      "        [ 0.6706,  0.1663],\n",
      "        [ 0.8857,  0.1293],\n",
      "        [ 0.6035,  0.2637],\n",
      "        [ 0.7856,  0.0487],\n",
      "        [ 0.5693, -0.0156],\n",
      "        [ 0.5321, -0.0522],\n",
      "        [ 0.6869,  0.2826],\n",
      "        [ 0.6449,  0.2783],\n",
      "        [ 0.4064, -0.0192],\n",
      "        [ 0.6243,  0.0350],\n",
      "        [ 0.4839,  0.2669],\n",
      "        [ 0.6669, -0.0092],\n",
      "        [ 0.5110,  0.0754],\n",
      "        [ 0.6723, -0.0431],\n",
      "        [ 0.7762,  0.2521],\n",
      "        [ 0.9315,  0.1223],\n",
      "        [ 0.7088,  0.0769],\n",
      "        [ 0.4585,  0.1620],\n",
      "        [ 0.7497, -0.1661],\n",
      "        [ 0.8969,  0.0124],\n",
      "        [ 0.4589,  0.1404],\n",
      "        [ 0.3422,  0.0936],\n",
      "        [ 0.8001,  0.1037],\n",
      "        [ 0.6045,  0.0151]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.7741, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7255,  0.2799],\n",
      "        [ 0.7185,  0.3522],\n",
      "        [ 0.6523,  0.3343],\n",
      "        [ 0.6874,  0.2200],\n",
      "        [ 0.6256,  0.2412],\n",
      "        [ 0.8781,  0.3207],\n",
      "        [ 0.5449, -0.2318],\n",
      "        [ 0.8402,  0.1300],\n",
      "        [ 0.6813,  0.0563],\n",
      "        [ 0.4239, -0.0371],\n",
      "        [ 0.6551,  0.0508],\n",
      "        [ 0.7002,  0.1451],\n",
      "        [ 0.6895, -0.0340],\n",
      "        [ 0.7983,  0.2268],\n",
      "        [ 0.3449,  0.1708],\n",
      "        [ 0.5228,  0.0087],\n",
      "        [ 0.4368,  0.1999],\n",
      "        [ 0.4729,  0.3653],\n",
      "        [ 0.7258,  0.1225],\n",
      "        [ 0.9523,  0.2121],\n",
      "        [ 0.8624,  0.0410],\n",
      "        [ 0.8626, -0.0250],\n",
      "        [ 0.9833,  0.2104],\n",
      "        [ 0.7453, -0.2981],\n",
      "        [ 0.5607, -0.0562],\n",
      "        [ 0.7786, -0.0704],\n",
      "        [ 0.4531,  0.0668],\n",
      "        [ 0.5380, -0.0088],\n",
      "        [ 0.5906, -0.0506],\n",
      "        [ 0.5666,  0.2170],\n",
      "        [ 0.9225, -0.0665],\n",
      "        [ 0.5439,  0.0099]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.9564, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6914,  0.1474],\n",
      "        [ 0.7095, -0.1869],\n",
      "        [ 0.6035, -0.1810],\n",
      "        [ 0.6907,  0.1991],\n",
      "        [ 0.6467, -0.1541],\n",
      "        [ 0.8460,  0.1217],\n",
      "        [ 0.6728,  0.0063],\n",
      "        [ 0.9631, -0.0283],\n",
      "        [ 0.7549,  0.0184],\n",
      "        [ 0.5634, -0.1090],\n",
      "        [ 0.7370, -0.0512],\n",
      "        [ 0.7521,  0.0502],\n",
      "        [ 0.5977, -0.0571],\n",
      "        [ 0.6186,  0.2161],\n",
      "        [ 0.6130,  0.1653],\n",
      "        [ 0.6427,  0.1156],\n",
      "        [ 1.0198, -0.0265],\n",
      "        [ 1.0592,  0.2526],\n",
      "        [ 0.5155,  0.0822],\n",
      "        [ 0.7754,  0.0732],\n",
      "        [ 0.9023,  0.0405],\n",
      "        [ 0.5806, -0.0724],\n",
      "        [ 0.7194,  0.0203],\n",
      "        [ 0.9355,  0.4364],\n",
      "        [ 0.7760,  0.0521],\n",
      "        [ 0.8527, -0.1575],\n",
      "        [ 0.6747,  0.1726],\n",
      "        [ 0.6919,  0.0985],\n",
      "        [ 0.9148,  0.1317],\n",
      "        [ 0.8539, -0.0581],\n",
      "        [ 0.9266,  0.1444],\n",
      "        [ 0.7448,  0.1746]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8994, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5308,  0.0467],\n",
      "        [ 0.6511,  0.0903],\n",
      "        [ 0.8874,  0.0942],\n",
      "        [ 0.5398, -0.0183],\n",
      "        [ 0.5408, -0.0332],\n",
      "        [ 0.7987,  0.2097],\n",
      "        [ 0.6747, -0.0192],\n",
      "        [ 0.8939,  0.3989],\n",
      "        [ 0.5808,  0.0896],\n",
      "        [ 0.5867,  0.0726],\n",
      "        [ 0.5079,  0.0925],\n",
      "        [ 0.9992,  0.1988],\n",
      "        [ 0.3884, -0.0129],\n",
      "        [ 0.9481,  0.1675],\n",
      "        [ 0.8648,  0.1118],\n",
      "        [ 0.8353,  0.2372],\n",
      "        [ 0.4225,  0.2625],\n",
      "        [ 0.3943, -0.0837],\n",
      "        [ 0.6928,  0.2006],\n",
      "        [ 0.8714,  0.0876],\n",
      "        [ 0.7404, -0.3636],\n",
      "        [ 0.7742, -0.0148],\n",
      "        [ 0.7358, -0.1618],\n",
      "        [ 0.6307, -0.0849],\n",
      "        [ 0.7996,  0.0891],\n",
      "        [ 0.6851,  0.1535],\n",
      "        [ 0.6776, -0.3537],\n",
      "        [ 0.6966,  0.1596],\n",
      "        [ 0.5266,  0.0599],\n",
      "        [ 0.7195, -0.0309],\n",
      "        [ 0.4944, -0.1113],\n",
      "        [ 0.6054, -0.0834]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9727, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8374,  0.1662],\n",
      "        [ 0.5232,  0.0268],\n",
      "        [ 0.2221, -0.2677],\n",
      "        [ 1.0612,  0.0406],\n",
      "        [ 0.8815, -0.0152],\n",
      "        [-0.0033,  0.0453],\n",
      "        [ 0.4764,  0.1599],\n",
      "        [ 0.5337,  0.1173],\n",
      "        [ 0.9494, -0.1380],\n",
      "        [ 0.8500,  0.2572],\n",
      "        [ 0.8697,  0.1043],\n",
      "        [ 0.4287, -0.4324],\n",
      "        [ 0.6466, -0.1517],\n",
      "        [ 1.0516,  0.2120],\n",
      "        [ 0.8681,  0.0681],\n",
      "        [ 0.6708, -0.0234],\n",
      "        [ 0.7431,  0.1112],\n",
      "        [ 0.8954,  0.1409],\n",
      "        [ 0.5226,  0.1162],\n",
      "        [ 0.7387,  0.0340],\n",
      "        [ 0.9487,  0.0174],\n",
      "        [ 0.7777,  0.3392],\n",
      "        [ 0.6414,  0.1487],\n",
      "        [ 0.9278,  0.2194],\n",
      "        [ 0.8896, -0.2596],\n",
      "        [ 0.4057,  0.0959],\n",
      "        [ 1.0551,  0.0792],\n",
      "        [ 0.7456,  0.1558],\n",
      "        [ 0.7067, -0.1451],\n",
      "        [ 0.6768, -0.0108],\n",
      "        [ 0.4659,  0.2505],\n",
      "        [ 0.9386,  0.2560]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8748, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4513, -0.0120],\n",
      "        [ 0.5804, -0.0341],\n",
      "        [ 0.5650, -0.2417],\n",
      "        [ 0.7323,  0.0383],\n",
      "        [ 0.7335, -0.0640],\n",
      "        [ 0.6785,  0.2686],\n",
      "        [ 0.9739,  0.1374],\n",
      "        [ 0.7813,  0.2510],\n",
      "        [ 0.5194, -0.1260],\n",
      "        [ 0.9226,  0.0388],\n",
      "        [ 0.8106, -0.0360],\n",
      "        [ 0.4373,  0.1579],\n",
      "        [ 0.6159, -0.0325],\n",
      "        [ 0.3788, -0.0041],\n",
      "        [ 0.8176,  0.1521],\n",
      "        [ 0.6900, -0.3027],\n",
      "        [ 0.6587,  0.2450],\n",
      "        [ 0.5606,  0.1006],\n",
      "        [ 0.7663,  0.0932],\n",
      "        [ 0.8177,  0.0943],\n",
      "        [ 0.8219, -0.1026],\n",
      "        [ 0.4424,  0.2027],\n",
      "        [ 0.9084,  0.1779],\n",
      "        [ 0.7801,  0.0939],\n",
      "        [ 0.7805,  0.1555],\n",
      "        [ 0.5163,  0.0703],\n",
      "        [ 0.7089,  0.0372],\n",
      "        [ 1.0102, -0.1555],\n",
      "        [ 0.4730,  0.0640],\n",
      "        [ 0.7931,  0.1470],\n",
      "        [ 0.9848,  0.0129],\n",
      "        [ 0.8680,  0.1914]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.8983, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 7.7473e-01,  1.6250e-01],\n",
      "        [ 5.9785e-01,  3.4578e-02],\n",
      "        [ 7.7245e-01,  6.3097e-02],\n",
      "        [ 9.4134e-01,  1.0594e-01],\n",
      "        [ 9.1840e-01,  1.3960e-01],\n",
      "        [ 6.3979e-01, -1.7935e-01],\n",
      "        [ 6.7922e-01,  6.4047e-02],\n",
      "        [ 8.1841e-01,  1.0338e-01],\n",
      "        [ 8.9003e-01,  4.1057e-01],\n",
      "        [ 9.8216e-01,  3.2142e-01],\n",
      "        [ 1.0084e+00,  1.9910e-02],\n",
      "        [ 7.3874e-01, -4.8803e-02],\n",
      "        [ 4.7799e-01, -9.0591e-02],\n",
      "        [ 7.4542e-01,  3.0621e-01],\n",
      "        [ 5.6576e-01,  3.9168e-02],\n",
      "        [ 7.9072e-01, -4.7861e-03],\n",
      "        [ 5.1774e-01,  5.1303e-02],\n",
      "        [ 7.4237e-01,  1.2244e-01],\n",
      "        [ 4.8870e-01,  1.4777e-01],\n",
      "        [ 5.4173e-01,  5.7241e-02],\n",
      "        [ 7.5711e-01,  2.6616e-01],\n",
      "        [ 4.2568e-01, -8.5048e-02],\n",
      "        [ 7.5651e-01, -4.4292e-04],\n",
      "        [ 8.8488e-01,  3.8255e-01],\n",
      "        [ 6.4109e-01, -2.0033e-01],\n",
      "        [ 4.9070e-01,  1.6479e-01],\n",
      "        [ 7.5729e-01,  1.3925e-01],\n",
      "        [ 8.8086e-01, -8.5752e-02],\n",
      "        [ 7.7594e-01,  9.1702e-02],\n",
      "        [ 6.0080e-01,  1.0857e-01],\n",
      "        [ 9.8658e-01, -8.8940e-02],\n",
      "        [ 8.5593e-01,  1.3488e-01]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8710, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7373, -0.0795],\n",
      "        [ 0.6874,  0.2572],\n",
      "        [ 0.8023, -0.1240],\n",
      "        [ 0.5674, -0.1715],\n",
      "        [ 0.8446, -0.0025],\n",
      "        [ 0.6029,  0.0690],\n",
      "        [ 0.4031, -0.0400],\n",
      "        [ 0.6665,  0.1843],\n",
      "        [ 0.4605, -0.2583],\n",
      "        [ 0.9799, -0.1188],\n",
      "        [ 0.7080,  0.1889],\n",
      "        [ 0.5060, -0.0544],\n",
      "        [ 0.7740,  0.1038],\n",
      "        [ 0.5038,  0.0335],\n",
      "        [ 0.5885,  0.1879],\n",
      "        [ 0.7354,  0.2907],\n",
      "        [ 0.8822,  0.0734],\n",
      "        [ 0.7987,  0.0801],\n",
      "        [ 0.5816,  0.0486],\n",
      "        [ 0.2276, -0.2620],\n",
      "        [ 0.7848,  0.2055],\n",
      "        [ 0.9192,  0.0624],\n",
      "        [ 0.7784, -0.1702],\n",
      "        [ 0.9318, -0.2132],\n",
      "        [ 1.0819,  0.1229],\n",
      "        [ 0.5281,  0.0812],\n",
      "        [ 0.5535,  0.3334],\n",
      "        [ 0.6509, -0.2081],\n",
      "        [ 0.8331,  0.0685],\n",
      "        [ 0.7398, -0.1536],\n",
      "        [ 0.5250,  0.0388],\n",
      "        [ 0.7338, -0.0039]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9073, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5179,  0.0236],\n",
      "        [ 0.5959,  0.0743],\n",
      "        [ 0.9693,  0.1288],\n",
      "        [ 0.3236,  0.0020],\n",
      "        [ 0.7419, -0.0711],\n",
      "        [ 0.9783,  0.2896],\n",
      "        [ 0.4905,  0.1603],\n",
      "        [ 0.6734,  0.1429],\n",
      "        [ 0.7000, -0.0297],\n",
      "        [ 0.5334,  0.1222],\n",
      "        [ 0.6046, -0.0570],\n",
      "        [ 0.6619,  0.1167],\n",
      "        [ 0.6676, -0.0589],\n",
      "        [ 0.2901, -0.0276],\n",
      "        [ 0.5843, -0.0022],\n",
      "        [ 0.7091, -0.1087],\n",
      "        [ 0.5080, -0.0257],\n",
      "        [ 0.7957,  0.1121],\n",
      "        [ 0.9119,  0.0395],\n",
      "        [ 0.7534,  0.0434],\n",
      "        [ 1.1113, -0.0803],\n",
      "        [ 0.6597,  0.2963],\n",
      "        [ 0.1302,  0.0894],\n",
      "        [ 0.8789,  0.1347],\n",
      "        [ 0.8526,  0.2384],\n",
      "        [ 0.2846,  0.0050],\n",
      "        [ 0.6705, -0.0304],\n",
      "        [ 0.8836,  0.0862],\n",
      "        [ 0.5664,  0.0033],\n",
      "        [ 0.9273,  0.2440],\n",
      "        [ 0.7889, -0.0073],\n",
      "        [ 0.8183,  0.0566]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8362, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6071,  0.2612],\n",
      "        [ 0.7881,  0.2257],\n",
      "        [ 0.6322,  0.0888],\n",
      "        [ 0.8680,  0.0702],\n",
      "        [ 0.7230,  0.1452],\n",
      "        [ 0.7682,  0.0633],\n",
      "        [ 0.7746, -0.1600],\n",
      "        [ 0.7358,  0.2003],\n",
      "        [ 0.3658,  0.1909],\n",
      "        [ 0.4855,  0.0286],\n",
      "        [ 1.1638, -0.2699],\n",
      "        [ 0.8273, -0.0717],\n",
      "        [ 1.0409,  0.2120],\n",
      "        [ 0.9563, -0.0232],\n",
      "        [ 0.6774,  0.1511],\n",
      "        [ 0.9151, -0.1595],\n",
      "        [ 0.5853, -0.0312],\n",
      "        [ 0.7484,  0.1566],\n",
      "        [ 0.7810,  0.2659],\n",
      "        [ 0.8849,  0.2741],\n",
      "        [ 0.6492,  0.3106],\n",
      "        [ 0.7857, -0.1162],\n",
      "        [ 0.6853,  0.0852],\n",
      "        [ 0.9267,  0.0456],\n",
      "        [ 0.7407,  0.1785],\n",
      "        [ 0.7253,  0.3009],\n",
      "        [ 0.8543,  0.1302],\n",
      "        [ 1.1976, -0.2194],\n",
      "        [ 0.7310,  0.1644],\n",
      "        [ 0.7631,  0.0501],\n",
      "        [ 0.7151,  0.1573],\n",
      "        [-0.0440,  0.1489]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.9826, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4913, -0.0788],\n",
      "        [ 0.7763,  0.1229],\n",
      "        [ 0.6966,  0.2805],\n",
      "        [ 0.7774,  0.2935],\n",
      "        [ 0.4788, -0.0610],\n",
      "        [ 0.6606,  0.1626],\n",
      "        [ 0.5947, -0.0406],\n",
      "        [ 0.9269,  0.3196],\n",
      "        [ 0.4412,  0.0218],\n",
      "        [ 1.3086,  0.1915],\n",
      "        [ 0.7406, -0.0669],\n",
      "        [ 0.9172, -0.0215],\n",
      "        [ 0.7437, -0.0097],\n",
      "        [ 0.9213,  0.1246],\n",
      "        [ 0.5704,  0.0819],\n",
      "        [ 0.4528,  0.0741],\n",
      "        [ 1.0683,  0.2501],\n",
      "        [ 0.8693, -0.0078],\n",
      "        [ 0.6269,  0.0219],\n",
      "        [ 0.5642, -0.0956],\n",
      "        [ 0.9746, -0.0343],\n",
      "        [ 0.8257,  0.2313],\n",
      "        [ 1.1246,  0.1321],\n",
      "        [ 0.8463,  0.2522],\n",
      "        [ 0.6652,  0.0340],\n",
      "        [ 0.9545, -0.0985],\n",
      "        [ 0.6585,  0.1966],\n",
      "        [ 0.5545,  0.2550],\n",
      "        [ 0.7135,  0.1475],\n",
      "        [ 0.6191,  0.0547],\n",
      "        [ 1.0719,  0.2068],\n",
      "        [ 0.3619, -0.0663]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8392, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4685, -0.0779],\n",
      "        [ 0.3948,  0.1579],\n",
      "        [ 0.6411, -0.0763],\n",
      "        [ 1.1104,  0.2877],\n",
      "        [ 0.7388,  0.2714],\n",
      "        [ 0.7652, -0.1952],\n",
      "        [ 0.8369,  0.2073],\n",
      "        [ 0.0442,  0.2254],\n",
      "        [ 0.6933,  0.0794],\n",
      "        [ 0.6568,  0.0647],\n",
      "        [ 0.5454,  0.1157],\n",
      "        [ 0.4833, -0.1029],\n",
      "        [ 0.8442,  0.0599],\n",
      "        [ 0.7905,  0.1314],\n",
      "        [ 0.7423,  0.1464],\n",
      "        [ 0.4754,  0.0402],\n",
      "        [ 0.8260,  0.0364],\n",
      "        [ 0.7593, -0.1293],\n",
      "        [ 0.5549, -0.0357],\n",
      "        [ 0.2145,  0.0432],\n",
      "        [ 0.7281, -0.0362],\n",
      "        [ 0.7684,  0.1270],\n",
      "        [ 0.9764, -0.0317],\n",
      "        [ 0.9071,  0.1760],\n",
      "        [ 0.5260, -0.1293],\n",
      "        [ 0.8286, -0.0815],\n",
      "        [ 0.6669,  0.0793],\n",
      "        [ 1.0133, -0.0486],\n",
      "        [ 1.0532,  0.2585],\n",
      "        [ 0.8463,  0.0494],\n",
      "        [ 0.3609,  0.0262],\n",
      "        [ 0.7918, -0.0407]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8771, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5820,  0.1360],\n",
      "        [ 0.7203, -0.0173],\n",
      "        [ 0.5458, -0.1838],\n",
      "        [ 1.1556,  0.2914],\n",
      "        [ 1.0675,  0.1252],\n",
      "        [ 0.8842,  0.2231],\n",
      "        [ 0.7366,  0.4970],\n",
      "        [ 0.7022,  0.1792],\n",
      "        [ 0.6812, -0.0789],\n",
      "        [ 0.5512,  0.0283],\n",
      "        [ 0.4268,  0.1588],\n",
      "        [ 1.0937,  0.1700],\n",
      "        [ 0.5839,  0.3189],\n",
      "        [ 0.5477,  0.0372],\n",
      "        [ 0.3730, -0.1015],\n",
      "        [ 0.6071,  0.0693],\n",
      "        [ 0.4267,  0.1220],\n",
      "        [ 0.6793,  0.0797],\n",
      "        [ 0.5100, -0.1354],\n",
      "        [ 0.7589,  0.0589],\n",
      "        [ 0.7671,  0.0280],\n",
      "        [ 0.5255, -0.0286],\n",
      "        [ 1.1296,  0.3105],\n",
      "        [ 0.6948,  0.2800],\n",
      "        [ 0.8272,  0.2103],\n",
      "        [ 0.5656, -0.2074],\n",
      "        [ 0.7345, -0.0039],\n",
      "        [ 0.2864,  0.2156],\n",
      "        [ 0.9038,  0.0640],\n",
      "        [ 0.8809,  0.0572],\n",
      "        [ 0.7050, -0.0732],\n",
      "        [ 0.5782,  0.2778]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8026, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6169,  0.0276],\n",
      "        [ 0.7463,  0.3159],\n",
      "        [ 0.8104,  0.1024],\n",
      "        [ 0.8064,  0.2476],\n",
      "        [ 0.3526,  0.0829],\n",
      "        [ 0.7862, -0.1795],\n",
      "        [ 0.6179, -0.1539],\n",
      "        [ 0.8417,  0.0247],\n",
      "        [ 0.7323,  0.1629],\n",
      "        [ 0.6624,  0.0430],\n",
      "        [ 0.7541,  0.1326],\n",
      "        [ 0.4860,  0.0756],\n",
      "        [ 0.5716,  0.0179],\n",
      "        [ 0.4833, -0.0479],\n",
      "        [ 0.6585,  0.3737],\n",
      "        [ 0.3863, -0.0353],\n",
      "        [ 0.6256, -0.0961],\n",
      "        [ 0.7769, -0.1212],\n",
      "        [ 0.9257, -0.0264],\n",
      "        [ 0.6947,  0.1094],\n",
      "        [ 0.7745,  0.2874],\n",
      "        [ 0.8616, -0.0739],\n",
      "        [ 0.7414, -0.0196],\n",
      "        [ 0.7678,  0.0068],\n",
      "        [ 0.8740,  0.4231],\n",
      "        [ 0.7444,  0.1636],\n",
      "        [ 0.8290,  0.3601],\n",
      "        [ 0.7089,  0.2554],\n",
      "        [ 0.8874,  0.1170],\n",
      "        [ 1.0081,  0.0212],\n",
      "        [ 0.7426, -0.1015],\n",
      "        [ 0.9641, -0.0783]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.8381, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6859,  0.1267],\n",
      "        [ 0.5162, -0.1520],\n",
      "        [ 0.8282,  0.0452],\n",
      "        [ 1.1902,  0.2601],\n",
      "        [ 0.7432,  0.2144],\n",
      "        [ 0.9481, -0.0762],\n",
      "        [ 0.9167,  0.0726],\n",
      "        [ 0.7135, -0.1072],\n",
      "        [ 0.0491,  0.2124],\n",
      "        [ 0.6312,  0.0214],\n",
      "        [ 0.6854,  0.1167],\n",
      "        [ 0.7303, -0.0646],\n",
      "        [ 0.8946,  0.0933],\n",
      "        [ 0.8788, -0.0605],\n",
      "        [ 0.4057, -0.1383],\n",
      "        [ 0.5954, -0.2236],\n",
      "        [ 0.8224, -0.1224],\n",
      "        [ 0.6109, -0.0385],\n",
      "        [ 0.7364,  0.3232],\n",
      "        [ 0.3032, -0.1649],\n",
      "        [ 1.0576,  0.1255],\n",
      "        [ 0.6446,  0.3132],\n",
      "        [ 0.6516,  0.1127],\n",
      "        [ 0.7250,  0.2758],\n",
      "        [ 0.5856,  0.0418],\n",
      "        [ 0.6101, -0.0294],\n",
      "        [ 0.7755,  0.1558],\n",
      "        [ 0.7224,  0.0740],\n",
      "        [ 0.6634,  0.2164],\n",
      "        [ 0.8939, -0.0724],\n",
      "        [ 0.8864,  0.1870],\n",
      "        [ 0.6173,  0.0695]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9319, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5649,  0.2605],\n",
      "        [ 0.6991, -0.0316],\n",
      "        [ 0.6783,  0.1658],\n",
      "        [ 0.8406, -0.0131],\n",
      "        [ 0.2447, -0.2518],\n",
      "        [ 0.9500,  0.0026],\n",
      "        [ 0.8818,  0.1625],\n",
      "        [ 0.3955,  0.1832],\n",
      "        [ 0.8387,  0.4731],\n",
      "        [ 0.6654,  0.1982],\n",
      "        [ 0.7286,  0.0520],\n",
      "        [ 1.0817,  0.0989],\n",
      "        [ 0.9932, -0.0257],\n",
      "        [ 0.6104,  0.2162],\n",
      "        [ 0.6504,  0.3125],\n",
      "        [ 0.6040,  0.0656],\n",
      "        [ 0.5763,  0.1836],\n",
      "        [ 0.9064,  0.0258],\n",
      "        [ 0.9578, -0.0777],\n",
      "        [ 0.7327, -0.2057],\n",
      "        [ 0.8821,  0.0892],\n",
      "        [ 0.2787,  0.0436],\n",
      "        [ 0.8780, -0.0555],\n",
      "        [ 0.4059,  0.4003],\n",
      "        [ 0.6662,  0.1715],\n",
      "        [ 0.5092, -0.1115],\n",
      "        [ 1.1442,  0.1994],\n",
      "        [ 0.7347, -0.1560],\n",
      "        [ 0.8348,  0.3376],\n",
      "        [ 0.8590,  0.2600],\n",
      "        [ 1.3525,  0.2064],\n",
      "        [ 0.5643, -0.1652]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8976, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4023,  0.0280],\n",
      "        [ 0.5719, -0.0389],\n",
      "        [ 0.3111,  0.0393],\n",
      "        [ 0.4733,  0.0308],\n",
      "        [ 0.9523,  0.1277],\n",
      "        [ 0.7172, -0.0352],\n",
      "        [ 0.5417,  0.1701],\n",
      "        [ 0.7608,  0.1958],\n",
      "        [ 0.7019,  0.1011],\n",
      "        [ 0.8148,  0.1213],\n",
      "        [ 0.6703,  0.1489],\n",
      "        [ 0.5694, -0.0459],\n",
      "        [ 0.0198, -0.1217],\n",
      "        [ 0.9985,  0.1841],\n",
      "        [ 0.3747, -0.0061],\n",
      "        [ 0.5355,  0.0067],\n",
      "        [ 0.5888,  0.1133],\n",
      "        [ 0.5189,  0.0592],\n",
      "        [ 0.2962, -0.1060],\n",
      "        [ 0.6429,  0.1057],\n",
      "        [ 0.5723,  0.1479],\n",
      "        [ 0.9585,  0.0728],\n",
      "        [ 1.0622,  0.2473],\n",
      "        [ 0.5412,  0.1330],\n",
      "        [ 0.6137, -0.0173],\n",
      "        [ 0.6864,  0.0489],\n",
      "        [ 0.5781,  0.0450],\n",
      "        [ 0.5915,  0.1762],\n",
      "        [ 0.3671,  0.1396],\n",
      "        [ 0.8331,  0.2712],\n",
      "        [ 0.5988,  0.1802],\n",
      "        [ 0.4828,  0.0513]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8688, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 7.3999e-01, -1.7299e-01],\n",
      "        [ 9.3267e-01,  1.7561e-01],\n",
      "        [ 4.1420e-01,  2.0123e-01],\n",
      "        [ 5.9832e-01,  1.3085e-01],\n",
      "        [ 5.0545e-01, -4.2296e-02],\n",
      "        [ 5.4358e-01, -1.6068e-03],\n",
      "        [ 8.2376e-01,  7.6193e-02],\n",
      "        [ 6.3335e-01,  3.5827e-02],\n",
      "        [ 1.0126e+00,  2.3652e-01],\n",
      "        [ 9.8940e-01,  6.3931e-02],\n",
      "        [ 8.0737e-01,  4.8741e-02],\n",
      "        [ 1.0415e+00,  1.2089e-01],\n",
      "        [ 3.7926e-01,  2.0390e-01],\n",
      "        [ 7.8030e-01, -4.4963e-02],\n",
      "        [ 6.9458e-01,  2.1952e-01],\n",
      "        [ 1.0236e+00, -1.0860e-02],\n",
      "        [ 7.6982e-01,  2.9976e-02],\n",
      "        [ 1.0962e+00,  2.2152e-01],\n",
      "        [ 7.7710e-01,  4.7401e-02],\n",
      "        [ 8.4383e-01,  1.1649e-01],\n",
      "        [ 4.3647e-01, -6.1319e-02],\n",
      "        [ 8.1877e-01, -6.8731e-02],\n",
      "        [ 7.6119e-01,  3.5879e-02],\n",
      "        [ 5.9876e-01,  2.8873e-02],\n",
      "        [ 5.7000e-01, -8.5960e-02],\n",
      "        [ 4.6039e-01,  3.0578e-02],\n",
      "        [ 8.5392e-01,  4.1042e-01],\n",
      "        [ 6.8199e-01,  7.0110e-02],\n",
      "        [ 7.6814e-01, -1.1036e-04],\n",
      "        [ 6.7268e-01, -1.5199e-01],\n",
      "        [ 6.8295e-01, -1.7411e-01],\n",
      "        [ 1.1547e+00,  2.0329e-01]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.9194, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6053,  0.0115],\n",
      "        [ 0.6395,  0.1914],\n",
      "        [ 0.7046, -0.0449],\n",
      "        [ 0.7587,  0.1354],\n",
      "        [ 1.0913,  0.0935],\n",
      "        [ 0.8097, -0.0048],\n",
      "        [ 0.6407, -0.0519],\n",
      "        [ 0.6120, -0.1288],\n",
      "        [ 0.6322, -0.1145],\n",
      "        [ 0.6218,  0.2000],\n",
      "        [ 0.5104,  0.2419],\n",
      "        [ 0.5551, -0.0191],\n",
      "        [ 0.7004,  0.0715],\n",
      "        [ 0.3879, -0.1879],\n",
      "        [ 0.4395,  0.1924],\n",
      "        [ 0.7727, -0.0431],\n",
      "        [ 0.9479, -0.0964],\n",
      "        [ 0.6883,  0.0149],\n",
      "        [ 0.7542, -0.1653],\n",
      "        [ 1.0737,  0.0109],\n",
      "        [ 0.7488, -0.0355],\n",
      "        [ 0.7661,  0.0094],\n",
      "        [ 0.6234,  0.2360],\n",
      "        [ 0.7013, -0.1374],\n",
      "        [ 0.5984,  0.2944],\n",
      "        [ 0.6624,  0.2358],\n",
      "        [ 0.6344, -0.0551],\n",
      "        [ 0.6896,  0.0339],\n",
      "        [ 0.5407,  0.0339],\n",
      "        [ 0.7214,  0.2341],\n",
      "        [ 0.6485,  0.3192],\n",
      "        [ 0.7527, -0.2350]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9402, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 5.9956e-01,  1.5495e-01],\n",
      "        [ 7.9413e-01,  7.0627e-02],\n",
      "        [ 7.1357e-01, -2.4716e-04],\n",
      "        [ 7.5252e-01,  7.7622e-02],\n",
      "        [ 7.7196e-01, -4.9689e-02],\n",
      "        [ 4.7423e-01,  6.9816e-03],\n",
      "        [ 1.1689e+00,  3.2149e-01],\n",
      "        [ 9.3744e-01,  1.2471e-01],\n",
      "        [ 9.6965e-01, -2.1928e-01],\n",
      "        [ 7.3548e-01, -5.9287e-02],\n",
      "        [ 5.3636e-01,  1.6328e-01],\n",
      "        [ 4.9901e-01,  2.7147e-03],\n",
      "        [ 5.4444e-01, -1.3107e-01],\n",
      "        [ 7.4026e-01,  8.4116e-02],\n",
      "        [ 7.3016e-01,  7.8829e-03],\n",
      "        [ 9.4429e-01,  1.2191e-01],\n",
      "        [ 8.3662e-01,  1.9002e-01],\n",
      "        [ 6.1143e-01,  2.1340e-01],\n",
      "        [ 6.6285e-01,  4.1236e-01],\n",
      "        [ 8.8163e-01, -2.1003e-02],\n",
      "        [ 6.8943e-01,  1.4896e-01],\n",
      "        [ 6.0823e-01,  9.5529e-03],\n",
      "        [ 4.1476e-01, -4.0101e-02],\n",
      "        [ 6.6877e-01,  9.0000e-02],\n",
      "        [ 6.2820e-01,  7.0471e-02],\n",
      "        [ 8.7419e-01,  4.6631e-02],\n",
      "        [ 9.4992e-01,  2.8921e-01],\n",
      "        [ 7.0294e-01,  2.0646e-01],\n",
      "        [ 6.0778e-01,  1.8415e-01],\n",
      "        [ 4.2911e-01, -4.0026e-02],\n",
      "        [ 9.2930e-01,  1.7786e-02],\n",
      "        [ 8.9084e-01,  1.1728e-01]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8902, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5022, -0.0234],\n",
      "        [ 1.0016,  0.0637],\n",
      "        [ 0.4417,  0.1310],\n",
      "        [-0.2079, -0.0863],\n",
      "        [ 0.3423, -0.0934],\n",
      "        [ 0.7209,  0.0026],\n",
      "        [ 0.1875,  0.0154],\n",
      "        [ 0.9181, -0.0052],\n",
      "        [ 0.9941,  0.2349],\n",
      "        [ 0.6391,  0.1127],\n",
      "        [ 0.5927,  0.0305],\n",
      "        [ 0.7094, -0.0057],\n",
      "        [ 0.7002,  0.1798],\n",
      "        [ 0.7763,  0.1230],\n",
      "        [ 0.9597, -0.0845],\n",
      "        [ 0.7623, -0.0461],\n",
      "        [ 0.7120,  0.0090],\n",
      "        [ 0.7744,  0.1245],\n",
      "        [ 0.8152,  0.0217],\n",
      "        [ 0.7555,  0.1854],\n",
      "        [ 0.7227,  0.1098],\n",
      "        [ 0.7501,  0.1114],\n",
      "        [ 0.6552,  0.2198],\n",
      "        [ 0.8432, -0.0790],\n",
      "        [ 0.9106, -0.0504],\n",
      "        [ 0.7348, -0.0382],\n",
      "        [ 0.6272, -0.0459],\n",
      "        [ 0.5711, -0.0289],\n",
      "        [ 0.9822,  0.0169],\n",
      "        [ 0.9412, -0.3894],\n",
      "        [ 0.8385,  0.1496],\n",
      "        [ 0.6401,  0.1063]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8861, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6740,  0.2312],\n",
      "        [ 0.0746, -0.0689],\n",
      "        [ 0.3246,  0.1381],\n",
      "        [ 0.5478, -0.0072],\n",
      "        [ 0.9708, -0.0218],\n",
      "        [ 0.6099,  0.1508],\n",
      "        [ 0.8422,  0.0317],\n",
      "        [ 0.8521,  0.2408],\n",
      "        [ 0.7046,  0.3372],\n",
      "        [ 0.9710,  0.0821],\n",
      "        [ 0.9712,  0.1901],\n",
      "        [ 0.4016,  0.1318],\n",
      "        [ 0.5560, -0.1930],\n",
      "        [ 0.3776,  0.0395],\n",
      "        [ 0.6632, -0.0790],\n",
      "        [ 0.4670, -0.3446],\n",
      "        [ 0.8168,  0.2360],\n",
      "        [ 0.7680,  0.0698],\n",
      "        [ 0.9332,  0.1000],\n",
      "        [ 0.8085,  0.0823],\n",
      "        [ 0.7978, -0.0118],\n",
      "        [ 0.7948,  0.1452],\n",
      "        [ 0.6810,  0.0057],\n",
      "        [ 0.4478,  0.1819],\n",
      "        [ 0.7473,  0.0159],\n",
      "        [ 0.9600,  0.2148],\n",
      "        [ 0.6683,  0.0285],\n",
      "        [-0.1733,  0.2381],\n",
      "        [ 0.4038,  0.0359],\n",
      "        [ 0.7726, -0.0583],\n",
      "        [ 0.9056,  0.1661],\n",
      "        [ 1.0915,  0.1639]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.8725, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8411,  0.2398],\n",
      "        [ 0.3338, -0.2260],\n",
      "        [ 0.6696, -0.1278],\n",
      "        [ 0.6975,  0.1977],\n",
      "        [ 0.3630,  0.1611],\n",
      "        [ 0.8471,  0.2897],\n",
      "        [ 0.7931,  0.1909],\n",
      "        [ 1.0068, -0.0494],\n",
      "        [ 0.8754,  0.2296],\n",
      "        [ 0.3777,  0.0863],\n",
      "        [ 0.7856,  0.1121],\n",
      "        [ 0.8662,  0.1105],\n",
      "        [ 0.3941,  0.1016],\n",
      "        [ 0.5630,  0.0250],\n",
      "        [ 0.7418,  0.2932],\n",
      "        [ 0.8779,  0.2242],\n",
      "        [ 0.5228,  0.0498],\n",
      "        [ 0.6028,  0.1816],\n",
      "        [ 0.4021,  0.0471],\n",
      "        [ 0.9716,  0.2895],\n",
      "        [ 0.8218,  0.0073],\n",
      "        [ 0.9529,  0.2939],\n",
      "        [ 0.8093,  0.0929],\n",
      "        [ 0.5400,  0.1552],\n",
      "        [ 0.8981,  0.0098],\n",
      "        [ 0.8082,  0.4665],\n",
      "        [ 0.5623,  0.2475],\n",
      "        [ 0.7708,  0.2782],\n",
      "        [ 0.9609,  0.2829],\n",
      "        [ 0.5955,  0.0980],\n",
      "        [ 0.5867,  0.0764],\n",
      "        [ 0.6743,  0.0144]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9796, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8672,  0.1788],\n",
      "        [ 0.6529,  0.0809],\n",
      "        [-0.2305,  0.1533],\n",
      "        [ 0.9577,  0.0480],\n",
      "        [ 0.8357, -0.0696],\n",
      "        [ 0.2465, -0.0745],\n",
      "        [ 0.5594, -0.1012],\n",
      "        [ 0.8233,  0.1902],\n",
      "        [ 0.7364,  0.2574],\n",
      "        [ 1.0548,  0.0791],\n",
      "        [ 0.7906,  0.0895],\n",
      "        [ 0.5360, -0.1195],\n",
      "        [ 0.4924, -0.0429],\n",
      "        [ 0.7753,  0.4026],\n",
      "        [ 0.4019, -0.0082],\n",
      "        [ 0.5973,  0.1034],\n",
      "        [ 0.9871,  0.0941],\n",
      "        [ 0.5280, -0.2087],\n",
      "        [ 0.6518,  0.1213],\n",
      "        [ 0.9260,  0.0313],\n",
      "        [ 0.6822, -0.0462],\n",
      "        [ 0.7083,  0.0999],\n",
      "        [ 0.6447,  0.0243],\n",
      "        [ 1.0807, -0.0344],\n",
      "        [ 0.7936,  0.1818],\n",
      "        [ 0.5425,  0.0105],\n",
      "        [ 0.7819, -0.0773],\n",
      "        [ 0.6375, -0.2367],\n",
      "        [ 0.7741, -0.1789],\n",
      "        [ 0.8887,  0.0648],\n",
      "        [ 0.8044, -0.0949],\n",
      "        [ 0.7037,  0.1321]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9185, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6540,  0.0982],\n",
      "        [ 0.7537,  0.0571],\n",
      "        [ 0.7006,  0.0654],\n",
      "        [ 0.5704,  0.0881],\n",
      "        [ 0.9314,  0.0966],\n",
      "        [ 0.7530,  0.1795],\n",
      "        [ 0.3388, -0.0913],\n",
      "        [ 1.0027, -0.0632],\n",
      "        [ 1.1218,  0.0467],\n",
      "        [ 0.6544,  0.2142],\n",
      "        [ 0.3644, -0.1599],\n",
      "        [ 0.8401, -0.1215],\n",
      "        [ 0.7978,  0.1433],\n",
      "        [ 0.5828,  0.1605],\n",
      "        [ 0.8490,  0.1217],\n",
      "        [ 0.7421,  0.1526],\n",
      "        [ 1.1741,  0.0070],\n",
      "        [ 0.4966,  0.0393],\n",
      "        [ 0.1494, -0.0518],\n",
      "        [ 0.5060,  0.0651],\n",
      "        [ 0.7588, -0.0327],\n",
      "        [ 0.2550, -0.0953],\n",
      "        [ 0.7408, -0.0761],\n",
      "        [ 0.8703,  0.3345],\n",
      "        [ 0.7392,  0.1956],\n",
      "        [ 0.5541,  0.0814],\n",
      "        [ 0.3325,  0.0839],\n",
      "        [ 1.1509,  0.0443],\n",
      "        [ 0.6125, -0.0818],\n",
      "        [ 0.9004,  0.0985],\n",
      "        [ 1.0768,  0.0026],\n",
      "        [ 0.7607,  0.1737]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8593, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6258, -0.2627],\n",
      "        [ 0.6967, -0.0164],\n",
      "        [ 0.8464,  0.0226],\n",
      "        [ 0.7867, -0.0083],\n",
      "        [ 0.6015,  0.1791],\n",
      "        [ 0.7209,  0.1138],\n",
      "        [ 0.9110,  0.0950],\n",
      "        [ 0.9204,  0.0613],\n",
      "        [ 0.4654, -0.0361],\n",
      "        [ 0.3409, -0.0800],\n",
      "        [ 0.7541,  0.1161],\n",
      "        [ 0.8577,  0.0798],\n",
      "        [ 0.4870, -0.0593],\n",
      "        [ 0.4394,  0.0340],\n",
      "        [ 0.6510,  0.1911],\n",
      "        [ 0.6464,  0.0554],\n",
      "        [ 0.9859,  0.0893],\n",
      "        [ 0.7394,  0.1962],\n",
      "        [ 0.7677, -0.0084],\n",
      "        [ 0.6519, -0.0290],\n",
      "        [ 0.5576,  0.1244],\n",
      "        [ 1.0656,  0.0847],\n",
      "        [ 0.7520,  0.0627],\n",
      "        [ 0.8390, -0.3431],\n",
      "        [ 0.6686, -0.0139],\n",
      "        [ 1.1126,  0.1059],\n",
      "        [ 0.5300,  0.0785],\n",
      "        [ 0.6422,  0.2408],\n",
      "        [ 0.7518,  0.0699],\n",
      "        [ 1.0065,  0.0783],\n",
      "        [ 0.4319, -0.0022],\n",
      "        [ 0.4940, -0.1726]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.9771, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4614, -0.1359],\n",
      "        [ 0.7462,  0.0670],\n",
      "        [ 0.6825,  0.1275],\n",
      "        [ 0.7645, -0.1514],\n",
      "        [ 0.6589,  0.0672],\n",
      "        [ 0.6231,  0.2183],\n",
      "        [ 0.4791,  0.0514],\n",
      "        [ 0.6783,  0.0060],\n",
      "        [ 0.4550, -0.0059],\n",
      "        [ 0.6232,  0.1285],\n",
      "        [ 0.6183, -0.0870],\n",
      "        [ 0.6835,  0.1391],\n",
      "        [ 0.9102,  0.2168],\n",
      "        [ 0.3228,  0.1361],\n",
      "        [ 0.4519, -0.1388],\n",
      "        [ 0.6655, -0.1170],\n",
      "        [ 0.4462,  0.0902],\n",
      "        [ 0.5859,  0.0612],\n",
      "        [ 0.8951,  0.1896],\n",
      "        [ 0.3376, -0.0433],\n",
      "        [ 0.9635, -0.0146],\n",
      "        [ 0.7995,  0.1641],\n",
      "        [ 0.6052,  0.1003],\n",
      "        [ 0.6643, -0.1316],\n",
      "        [ 0.9983,  0.1829],\n",
      "        [ 0.5553,  0.0861],\n",
      "        [ 0.7559,  0.2063],\n",
      "        [ 1.0408,  0.1681],\n",
      "        [ 0.4228, -0.0585],\n",
      "        [ 0.5994,  0.1295],\n",
      "        [ 0.7695, -0.1874],\n",
      "        [ 0.6437,  0.2705]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8612, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.2419, -0.2615],\n",
      "        [ 0.7248, -0.0293],\n",
      "        [ 0.5873,  0.0247],\n",
      "        [ 1.0104,  0.0406],\n",
      "        [ 0.9554,  0.1180],\n",
      "        [ 0.8347,  0.1176],\n",
      "        [ 0.5821, -0.0874],\n",
      "        [-0.1557,  0.0932],\n",
      "        [ 0.5228, -0.0564],\n",
      "        [ 0.7694, -0.0632],\n",
      "        [ 0.6516,  0.0658],\n",
      "        [ 0.8455,  0.0318],\n",
      "        [ 0.8966, -0.2034],\n",
      "        [ 0.7655,  0.0014],\n",
      "        [ 0.6721, -0.1947],\n",
      "        [ 0.8616, -0.0916],\n",
      "        [ 0.5630,  0.1777],\n",
      "        [ 0.8391,  0.1389],\n",
      "        [ 0.6726,  0.0036],\n",
      "        [ 0.6096,  0.2671],\n",
      "        [ 0.8008, -0.4116],\n",
      "        [ 1.1767,  0.2368],\n",
      "        [ 0.6314,  0.0768],\n",
      "        [ 0.3078, -0.0494],\n",
      "        [ 0.8446,  0.4056],\n",
      "        [ 1.0235,  0.1610],\n",
      "        [ 0.7285,  0.0970],\n",
      "        [ 0.6849, -0.0316],\n",
      "        [ 1.0146,  0.1050],\n",
      "        [ 0.5943, -0.0027],\n",
      "        [ 0.4446, -0.0426],\n",
      "        [ 0.5470,  0.1615]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.7889, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 5.1703e-01,  2.2131e-01],\n",
      "        [ 8.4065e-01,  3.6963e-01],\n",
      "        [ 5.4758e-01, -1.7469e-01],\n",
      "        [ 5.0638e-01,  6.6758e-02],\n",
      "        [ 8.8409e-01, -3.3040e-04],\n",
      "        [ 6.6824e-01,  9.1926e-03],\n",
      "        [ 5.4871e-01,  7.2713e-02],\n",
      "        [ 5.6367e-01,  3.1441e-02],\n",
      "        [ 5.6319e-01,  8.8170e-04],\n",
      "        [ 5.0884e-01,  8.2327e-03],\n",
      "        [ 8.0861e-01,  1.1295e-01],\n",
      "        [ 7.6027e-01,  2.1579e-01],\n",
      "        [ 7.1446e-01, -5.8494e-02],\n",
      "        [ 5.8103e-01,  8.7809e-02],\n",
      "        [ 9.0560e-01, -4.0513e-02],\n",
      "        [ 5.4858e-01,  5.9943e-02],\n",
      "        [ 9.2091e-01,  2.3592e-01],\n",
      "        [ 4.7647e-01,  1.6906e-01],\n",
      "        [ 5.3797e-01,  2.2866e-01],\n",
      "        [ 1.1040e+00,  5.1107e-02],\n",
      "        [ 3.5966e-01, -5.0092e-02],\n",
      "        [ 6.0106e-01,  1.8273e-01],\n",
      "        [ 3.9781e-01,  1.4322e-01],\n",
      "        [ 7.5434e-01,  1.9229e-01],\n",
      "        [ 6.3618e-01,  1.4411e-01],\n",
      "        [ 6.8289e-01,  1.8183e-01],\n",
      "        [ 8.1953e-01,  1.1973e-01],\n",
      "        [ 7.5419e-01,  3.6828e-01],\n",
      "        [ 1.0730e+00, -2.1700e-02],\n",
      "        [ 6.3045e-01, -5.1264e-02],\n",
      "        [ 8.2348e-01,  6.2279e-02],\n",
      "        [ 8.8613e-01, -7.7577e-02]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8244, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 6.5304e-01,  6.5669e-02],\n",
      "        [ 8.8761e-01,  5.4024e-02],\n",
      "        [ 4.8084e-01, -2.7195e-01],\n",
      "        [ 6.6148e-01,  1.4391e-01],\n",
      "        [ 6.0029e-01,  3.4887e-01],\n",
      "        [ 4.2745e-01, -6.1000e-02],\n",
      "        [ 8.2448e-01,  1.7854e-01],\n",
      "        [ 9.3033e-01, -6.9816e-02],\n",
      "        [ 4.0275e-01,  5.2798e-03],\n",
      "        [ 6.1211e-01, -2.3470e-01],\n",
      "        [ 8.5093e-01, -5.1673e-02],\n",
      "        [ 5.2554e-01,  2.1702e-04],\n",
      "        [ 8.6660e-01,  2.1237e-01],\n",
      "        [ 9.9508e-01,  1.4898e-01],\n",
      "        [ 7.6235e-01,  1.0925e-01],\n",
      "        [ 4.5102e-01,  1.8342e-01],\n",
      "        [ 1.1399e+00,  6.6675e-02],\n",
      "        [ 5.1245e-01,  2.9382e-01],\n",
      "        [ 9.1589e-01,  4.4275e-02],\n",
      "        [ 5.1069e-01,  5.3549e-02],\n",
      "        [ 9.2650e-01,  7.5647e-02],\n",
      "        [ 8.5926e-01,  2.5571e-01],\n",
      "        [ 7.6057e-01, -1.3262e-02],\n",
      "        [ 8.5322e-01, -1.7409e-01],\n",
      "        [ 7.9549e-01,  9.9046e-02],\n",
      "        [ 3.4342e-01,  1.0769e-01],\n",
      "        [ 5.8893e-01,  2.1571e-01],\n",
      "        [ 7.5961e-01,  2.0898e-03],\n",
      "        [ 8.8624e-01,  9.7267e-02],\n",
      "        [ 6.5394e-01,  1.2766e-01],\n",
      "        [ 6.6968e-01,  2.1251e-02],\n",
      "        [ 8.8239e-01,  1.8612e-01]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.8358, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9276,  0.0734],\n",
      "        [ 0.5930,  0.0112],\n",
      "        [ 0.8717,  0.0618],\n",
      "        [ 0.8569, -0.0543],\n",
      "        [ 0.5587,  0.3053],\n",
      "        [ 0.8049,  0.3285],\n",
      "        [ 0.9603,  0.3212],\n",
      "        [ 0.9691,  0.0253],\n",
      "        [ 0.5331,  0.0313],\n",
      "        [ 0.9935,  0.1834],\n",
      "        [ 0.9753, -0.0830],\n",
      "        [ 0.6675,  0.0619],\n",
      "        [ 0.6103, -0.1114],\n",
      "        [ 0.6707,  0.1809],\n",
      "        [ 0.8016, -0.0166],\n",
      "        [ 0.4831,  0.0096],\n",
      "        [ 0.7816,  0.0319],\n",
      "        [ 0.8633,  0.1876],\n",
      "        [ 0.6764,  0.0852],\n",
      "        [ 0.6696,  0.1779],\n",
      "        [ 0.8321,  0.2108],\n",
      "        [ 0.6607,  0.0766],\n",
      "        [ 0.9449,  0.0932],\n",
      "        [ 0.6124,  0.2738],\n",
      "        [ 0.7740,  0.0810],\n",
      "        [ 0.4998,  0.0403],\n",
      "        [ 0.7087,  0.1611],\n",
      "        [ 0.8104,  0.2028],\n",
      "        [ 0.9601,  0.1398],\n",
      "        [ 0.7397, -0.0702],\n",
      "        [ 0.8033,  0.1238],\n",
      "        [ 0.9125, -0.0703]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9346, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4756,  0.1325],\n",
      "        [ 0.7966,  0.1218],\n",
      "        [ 0.8343,  0.1237],\n",
      "        [ 0.9135,  0.1216],\n",
      "        [ 0.7688,  0.0090],\n",
      "        [ 0.6115,  0.2202],\n",
      "        [ 0.6474,  0.0103],\n",
      "        [ 0.5282, -0.0588],\n",
      "        [ 0.7963,  0.1376],\n",
      "        [ 0.5734,  0.0342],\n",
      "        [ 0.5926, -0.0932],\n",
      "        [ 0.8037,  0.2997],\n",
      "        [ 0.6847, -0.0068],\n",
      "        [ 0.9614,  0.1482],\n",
      "        [ 0.7987,  0.0686],\n",
      "        [ 0.8230, -0.1126],\n",
      "        [ 0.6073, -0.0308],\n",
      "        [ 1.0695,  0.0122],\n",
      "        [ 1.0367, -0.0748],\n",
      "        [ 0.6818,  0.1155],\n",
      "        [ 0.3859,  0.4041],\n",
      "        [ 0.3180, -0.0827],\n",
      "        [ 0.6775,  0.1176],\n",
      "        [ 0.6921, -0.0680],\n",
      "        [ 0.5649,  0.0613],\n",
      "        [ 0.6385,  0.1077],\n",
      "        [ 0.6163,  0.2997],\n",
      "        [ 0.0247,  0.2586],\n",
      "        [ 0.7918,  0.2915],\n",
      "        [ 0.6121,  0.0399],\n",
      "        [ 0.8392, -0.0513],\n",
      "        [ 0.8044, -0.0273]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9739, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 8.1993e-01,  1.3536e-01],\n",
      "        [ 7.1216e-01,  1.0145e-01],\n",
      "        [ 5.9748e-01, -1.4481e-01],\n",
      "        [ 6.0726e-01,  1.1106e-01],\n",
      "        [ 7.0109e-01, -1.6891e-01],\n",
      "        [ 5.1688e-01, -5.2235e-02],\n",
      "        [ 7.9078e-01,  2.0457e-01],\n",
      "        [ 7.2971e-01,  2.7808e-01],\n",
      "        [ 7.1430e-01, -3.0023e-01],\n",
      "        [ 5.6654e-01, -1.0959e-01],\n",
      "        [ 5.1438e-01,  1.1162e-01],\n",
      "        [ 8.3670e-01, -1.0874e-01],\n",
      "        [ 5.5515e-01, -1.4642e-02],\n",
      "        [ 8.1655e-01,  1.4761e-01],\n",
      "        [ 7.8157e-01,  1.3445e-01],\n",
      "        [ 6.7397e-01,  6.7593e-02],\n",
      "        [ 8.6383e-01, -5.3175e-02],\n",
      "        [ 5.3461e-01, -5.2261e-02],\n",
      "        [ 6.7074e-01,  2.8818e-01],\n",
      "        [ 7.9661e-01,  2.9944e-01],\n",
      "        [ 8.8619e-01,  7.8861e-02],\n",
      "        [ 8.9779e-01,  4.4208e-03],\n",
      "        [ 9.8523e-01, -7.2940e-04],\n",
      "        [ 6.2646e-01,  2.1555e-01],\n",
      "        [ 6.7873e-01,  4.0295e-02],\n",
      "        [ 9.3637e-01,  2.5264e-01],\n",
      "        [ 6.2159e-01,  4.6815e-02],\n",
      "        [ 6.4601e-01, -9.1143e-02],\n",
      "        [ 5.5267e-01, -8.2245e-02],\n",
      "        [ 5.1363e-01, -1.4906e-01],\n",
      "        [ 8.7413e-01,  1.8971e-01],\n",
      "        [ 1.0975e-01,  9.7792e-02]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8078, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7428,  0.0092],\n",
      "        [ 0.9008, -0.0015],\n",
      "        [ 0.8855,  0.2193],\n",
      "        [ 0.9269,  0.1448],\n",
      "        [ 0.5318,  0.1409],\n",
      "        [ 0.6011,  0.0312],\n",
      "        [ 1.0346,  0.1064],\n",
      "        [ 0.9898,  0.1154],\n",
      "        [ 0.4775, -0.2155],\n",
      "        [ 0.5994, -0.0141],\n",
      "        [ 0.6067,  0.0657],\n",
      "        [ 0.7560, -0.1009],\n",
      "        [ 0.8107,  0.1144],\n",
      "        [ 0.8698,  0.0831],\n",
      "        [ 0.4630,  0.0429],\n",
      "        [ 0.6365,  0.2149],\n",
      "        [ 0.9401,  0.1672],\n",
      "        [ 0.6046, -0.0427],\n",
      "        [ 0.6635,  0.1696],\n",
      "        [ 0.8341,  0.1887],\n",
      "        [ 0.8098,  0.1155],\n",
      "        [ 0.6971,  0.1003],\n",
      "        [ 0.8880, -0.0063],\n",
      "        [ 0.5055,  0.1631],\n",
      "        [ 0.7738, -0.0753],\n",
      "        [ 0.7064,  0.1145],\n",
      "        [ 1.0024,  0.1097],\n",
      "        [ 1.1166,  0.2953],\n",
      "        [ 0.7848,  0.1860],\n",
      "        [ 0.5279, -0.0239],\n",
      "        [ 0.6545,  0.0717],\n",
      "        [ 0.9351,  0.0070]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.8285, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 5.8714e-01,  1.3672e-01],\n",
      "        [ 7.9732e-01,  2.8807e-01],\n",
      "        [ 6.3204e-01, -7.8381e-03],\n",
      "        [ 7.5085e-01,  1.1217e-01],\n",
      "        [ 5.7771e-01,  9.2088e-02],\n",
      "        [ 8.9547e-01, -1.2815e-04],\n",
      "        [ 5.7997e-01, -3.8771e-02],\n",
      "        [ 9.7563e-01,  1.8967e-01],\n",
      "        [ 4.6666e-01,  2.3669e-01],\n",
      "        [ 5.7485e-01,  1.6829e-01],\n",
      "        [ 7.0280e-01,  1.2552e-01],\n",
      "        [ 1.1467e+00,  1.2929e-01],\n",
      "        [ 7.4366e-01,  1.3165e-01],\n",
      "        [ 7.1739e-01, -5.4560e-02],\n",
      "        [ 3.6204e-01,  1.3326e-01],\n",
      "        [ 5.0153e-01, -2.0621e-02],\n",
      "        [ 6.0233e-01,  2.5776e-01],\n",
      "        [ 7.7371e-01,  2.9435e-02],\n",
      "        [ 7.8658e-01, -4.2307e-02],\n",
      "        [ 7.2221e-01,  3.8515e-01],\n",
      "        [ 4.2836e-01,  9.7076e-02],\n",
      "        [ 4.9139e-01, -4.0236e-02],\n",
      "        [ 8.5518e-01, -2.8403e-02],\n",
      "        [ 9.7087e-01,  2.8129e-01],\n",
      "        [ 9.4491e-01,  8.5203e-02],\n",
      "        [ 7.7904e-01,  2.4791e-02],\n",
      "        [ 6.2917e-01, -3.7970e-02],\n",
      "        [ 7.9949e-01, -1.9291e-02],\n",
      "        [ 8.2464e-01,  2.6532e-01],\n",
      "        [ 4.6113e-01, -1.8477e-03],\n",
      "        [ 3.8333e-01,  1.2721e-01],\n",
      "        [ 5.8074e-01, -7.0950e-03]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(1.0015, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4330, -0.0940],\n",
      "        [ 0.8088, -0.3388],\n",
      "        [ 0.7212, -0.0096],\n",
      "        [ 1.0534, -0.1871],\n",
      "        [ 0.0202, -0.0484],\n",
      "        [ 0.9559,  0.1252],\n",
      "        [ 0.8894,  0.2489],\n",
      "        [ 0.6680,  0.0956],\n",
      "        [-0.1728, -0.1037],\n",
      "        [ 0.7763,  0.1187],\n",
      "        [ 0.8896,  0.2317],\n",
      "        [ 0.5862,  0.2507],\n",
      "        [ 0.6153,  0.0487],\n",
      "        [ 0.6580, -0.0726],\n",
      "        [ 0.6332,  0.2064],\n",
      "        [ 0.9336, -0.0607],\n",
      "        [ 0.9832,  0.2118],\n",
      "        [ 0.7413, -0.1218],\n",
      "        [ 0.7071,  0.0758],\n",
      "        [ 0.6514, -0.1834],\n",
      "        [ 0.4520,  0.1197],\n",
      "        [ 0.6176,  0.2033],\n",
      "        [ 0.6078, -0.0210],\n",
      "        [ 0.4690, -0.0666],\n",
      "        [ 0.7142,  0.0043],\n",
      "        [ 0.5106, -0.0683],\n",
      "        [ 0.6121, -0.0579],\n",
      "        [ 0.6037,  0.1800],\n",
      "        [ 0.6096,  0.1777],\n",
      "        [ 0.8724,  0.0220],\n",
      "        [ 0.9296,  0.0344],\n",
      "        [ 0.6391,  0.2464]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8633, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0115,  0.1157],\n",
      "        [ 0.6703, -0.0690],\n",
      "        [ 0.2016,  0.1912],\n",
      "        [ 0.7641,  0.0422],\n",
      "        [ 0.4594,  0.0550],\n",
      "        [ 0.6991,  0.2295],\n",
      "        [ 0.9197,  0.0721],\n",
      "        [ 0.9617,  0.2214],\n",
      "        [ 0.7566,  0.0906],\n",
      "        [ 0.8567,  0.0999],\n",
      "        [ 0.7002,  0.1657],\n",
      "        [ 0.5894,  0.4459],\n",
      "        [ 1.0959,  0.1908],\n",
      "        [ 0.5517,  0.0249],\n",
      "        [ 0.8053,  0.1957],\n",
      "        [ 0.7108,  0.0446],\n",
      "        [ 0.8295,  0.0312],\n",
      "        [ 0.1332, -0.0456],\n",
      "        [ 0.7506,  0.0168],\n",
      "        [ 0.6113,  0.0386],\n",
      "        [ 0.6418, -0.0176],\n",
      "        [ 0.8316,  0.1063],\n",
      "        [ 0.4191, -0.1229],\n",
      "        [ 0.3333,  0.0053],\n",
      "        [ 0.6339,  0.1538],\n",
      "        [ 0.5559, -0.1094],\n",
      "        [ 0.8561,  0.0259],\n",
      "        [ 0.7285,  0.0480],\n",
      "        [ 0.7391,  0.0233],\n",
      "        [ 0.7439,  0.2220],\n",
      "        [ 0.8343,  0.0426],\n",
      "        [ 0.9822,  0.1734]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8742, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6563,  0.0269],\n",
      "        [ 0.8127,  0.0398],\n",
      "        [ 0.5646,  0.0550],\n",
      "        [ 0.6055,  0.1465],\n",
      "        [ 0.8885, -0.0640],\n",
      "        [ 0.6119, -0.0152],\n",
      "        [ 0.9403,  0.2744],\n",
      "        [ 0.7537,  0.1896],\n",
      "        [ 0.8418,  0.2064],\n",
      "        [ 0.8535,  0.1839],\n",
      "        [ 0.5272, -0.0242],\n",
      "        [ 0.7771,  0.1301],\n",
      "        [ 0.8889,  0.1504],\n",
      "        [ 0.9512,  0.2475],\n",
      "        [ 0.8639, -0.0959],\n",
      "        [ 0.5433,  0.1508],\n",
      "        [ 0.8940,  0.0418],\n",
      "        [ 0.7981,  0.1606],\n",
      "        [ 0.8306,  0.0597],\n",
      "        [ 0.8638,  0.2181],\n",
      "        [ 0.9527,  0.0237],\n",
      "        [ 0.4755,  0.1643],\n",
      "        [ 0.6472,  0.1141],\n",
      "        [ 0.4606,  0.1535],\n",
      "        [ 0.7589,  0.1117],\n",
      "        [ 0.7985, -0.0578],\n",
      "        [ 0.8043, -0.0375],\n",
      "        [ 0.5705,  0.2623],\n",
      "        [ 0.6005, -0.0759],\n",
      "        [ 0.4157,  0.1644],\n",
      "        [ 0.4431, -0.1838],\n",
      "        [ 0.7112,  0.2455]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.9782, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4493,  0.0452],\n",
      "        [ 0.8260,  0.0238],\n",
      "        [ 0.6693,  0.1374],\n",
      "        [ 0.8815, -0.1291],\n",
      "        [ 0.7081, -0.0178],\n",
      "        [ 0.6364,  0.0626],\n",
      "        [ 0.6603,  0.0585],\n",
      "        [ 0.8071,  0.0770],\n",
      "        [ 0.6758, -0.2535],\n",
      "        [ 0.7328,  0.0121],\n",
      "        [ 0.3819, -0.4155],\n",
      "        [ 1.0310,  0.3217],\n",
      "        [ 0.6921,  0.1872],\n",
      "        [ 0.7284,  0.2329],\n",
      "        [ 0.7511,  0.1905],\n",
      "        [ 0.9340,  0.1078],\n",
      "        [ 0.9235, -0.0429],\n",
      "        [ 0.4589,  0.4254],\n",
      "        [ 0.2088, -0.0480],\n",
      "        [ 0.6849, -0.0828],\n",
      "        [ 0.3793,  0.0307],\n",
      "        [ 0.5922,  0.0546],\n",
      "        [ 0.7187,  0.0354],\n",
      "        [ 0.5683,  0.0789],\n",
      "        [ 0.6814,  0.0332],\n",
      "        [ 0.9459,  0.0783],\n",
      "        [ 0.3897,  0.1492],\n",
      "        [ 0.8007,  0.0423],\n",
      "        [ 0.6977,  0.1190],\n",
      "        [ 0.6286,  0.2897],\n",
      "        [ 0.6920,  0.1393],\n",
      "        [ 0.8020,  0.1530]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8511, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3192,  0.1753],\n",
      "        [ 0.4553,  0.1279],\n",
      "        [ 0.6653,  0.0961],\n",
      "        [ 0.8244,  0.1065],\n",
      "        [ 0.6192,  0.1881],\n",
      "        [ 0.5216, -0.0697],\n",
      "        [ 0.9545,  0.1230],\n",
      "        [ 0.7067,  0.3348],\n",
      "        [ 0.5982,  0.1652],\n",
      "        [ 0.6131,  0.0708],\n",
      "        [ 1.1511,  0.2405],\n",
      "        [ 0.8358,  0.1353],\n",
      "        [ 0.7729,  0.2168],\n",
      "        [ 0.8053, -0.0490],\n",
      "        [ 0.9963,  0.0616],\n",
      "        [ 0.7675,  0.2477],\n",
      "        [ 0.6054,  0.0085],\n",
      "        [ 0.9585, -0.0324],\n",
      "        [ 0.9640,  0.2772],\n",
      "        [ 1.0086,  0.3010],\n",
      "        [ 1.0270,  0.0756],\n",
      "        [ 0.8055,  0.1638],\n",
      "        [ 0.6970,  0.0260],\n",
      "        [ 0.7219,  0.1289],\n",
      "        [ 0.8030, -0.0557],\n",
      "        [ 0.6009,  0.0991],\n",
      "        [ 1.1123,  0.1186],\n",
      "        [ 0.8691,  0.0533],\n",
      "        [ 0.7050,  0.1519],\n",
      "        [ 0.6790, -0.2289],\n",
      "        [ 0.8798, -0.1019],\n",
      "        [ 0.8052,  0.0211]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8525, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7636,  0.3461],\n",
      "        [ 0.7966,  0.0981],\n",
      "        [ 1.0020,  0.3478],\n",
      "        [ 0.7047, -0.1050],\n",
      "        [ 0.7839,  0.1379],\n",
      "        [ 1.1500,  0.0584],\n",
      "        [ 0.8437,  0.0732],\n",
      "        [ 0.5893, -0.0154],\n",
      "        [ 0.6154,  0.0507],\n",
      "        [ 0.8175,  0.2376],\n",
      "        [ 0.6118,  0.1497],\n",
      "        [ 0.6356,  0.1483],\n",
      "        [ 0.7854, -0.0306],\n",
      "        [ 0.9118, -0.0095],\n",
      "        [ 0.8642, -0.0778],\n",
      "        [ 0.7495,  0.1278],\n",
      "        [ 0.7790,  0.0864],\n",
      "        [ 0.6587, -0.1487],\n",
      "        [ 0.5719,  0.2793],\n",
      "        [ 0.8031,  0.1467],\n",
      "        [ 0.4064,  0.1722],\n",
      "        [ 0.7528,  0.1822],\n",
      "        [ 0.6634, -0.0146],\n",
      "        [ 0.7308,  0.0803],\n",
      "        [ 0.6624,  0.2760],\n",
      "        [ 0.9710,  0.2533],\n",
      "        [ 1.0362,  0.2488],\n",
      "        [ 0.9510,  0.2337],\n",
      "        [ 0.7118,  0.0326],\n",
      "        [ 0.5059,  0.0542],\n",
      "        [ 0.5333, -0.4579],\n",
      "        [ 0.4391, -0.1317]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8240, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6377,  0.3499],\n",
      "        [ 0.8506,  0.0480],\n",
      "        [ 0.4038,  0.3533],\n",
      "        [ 0.8174, -0.0740],\n",
      "        [ 0.7302,  0.1391],\n",
      "        [ 0.4377,  0.0739],\n",
      "        [ 0.5898, -0.0908],\n",
      "        [ 0.7899, -0.0548],\n",
      "        [ 0.8198, -0.2011],\n",
      "        [ 0.6483, -0.0073],\n",
      "        [ 0.8452,  0.0152],\n",
      "        [ 0.5084,  0.0543],\n",
      "        [ 0.6344,  0.1330],\n",
      "        [ 0.7469, -0.0578],\n",
      "        [ 0.7552,  0.2791],\n",
      "        [ 0.8326,  0.3038],\n",
      "        [ 0.6317, -0.1191],\n",
      "        [ 0.6017,  0.1050],\n",
      "        [ 0.7483,  0.2135],\n",
      "        [ 0.4833,  0.1323],\n",
      "        [ 0.6573,  0.1926],\n",
      "        [ 0.4244, -0.0485],\n",
      "        [ 0.5844,  0.2172],\n",
      "        [ 0.6067,  0.0222],\n",
      "        [ 0.5349,  0.1071],\n",
      "        [ 0.8350,  0.0262],\n",
      "        [ 0.7247,  0.0591],\n",
      "        [ 0.6645,  0.2046],\n",
      "        [ 0.8146,  0.2152],\n",
      "        [ 0.9521, -0.0777],\n",
      "        [ 0.4741,  0.1045],\n",
      "        [ 0.8624,  0.2369]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.7610, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8714,  0.2707],\n",
      "        [ 0.7637,  0.1567],\n",
      "        [ 0.3728,  0.2676],\n",
      "        [ 0.4561, -0.0440],\n",
      "        [ 0.8525,  0.2863],\n",
      "        [ 0.4709,  0.1581],\n",
      "        [ 0.6846,  0.3699],\n",
      "        [ 0.4172, -0.0147],\n",
      "        [ 0.7555, -0.1362],\n",
      "        [ 0.7110,  0.1257],\n",
      "        [ 0.5132, -0.1027],\n",
      "        [ 0.5562, -0.1103],\n",
      "        [ 0.6498,  0.1172],\n",
      "        [ 1.0371,  0.2188],\n",
      "        [ 0.6305,  0.0230],\n",
      "        [ 0.7387,  0.2313],\n",
      "        [ 0.5933,  0.1398],\n",
      "        [ 0.5154,  0.0052],\n",
      "        [ 1.0236,  0.1743],\n",
      "        [ 0.5009,  0.1924],\n",
      "        [ 0.6047,  0.0372],\n",
      "        [ 0.4682,  0.0582],\n",
      "        [ 0.7666,  0.0667],\n",
      "        [ 0.9684, -0.0952],\n",
      "        [ 1.0265,  0.1609],\n",
      "        [ 1.0605,  0.1941],\n",
      "        [ 0.5952,  0.1011],\n",
      "        [ 0.8453,  0.3963],\n",
      "        [ 1.0640,  0.1638],\n",
      "        [ 0.8405,  0.0889],\n",
      "        [ 1.0257, -0.1011],\n",
      "        [ 0.8482,  0.1230]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9314, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3695,  0.1854],\n",
      "        [ 0.6332, -0.1288],\n",
      "        [ 0.7304, -0.0495],\n",
      "        [ 0.7657,  0.1590],\n",
      "        [ 0.8100,  0.1048],\n",
      "        [ 0.8113,  0.2984],\n",
      "        [ 0.5554,  0.0774],\n",
      "        [ 0.9997,  0.2451],\n",
      "        [ 0.7661,  0.0959],\n",
      "        [ 0.6008,  0.1025],\n",
      "        [ 0.6905, -0.1901],\n",
      "        [ 0.7165, -0.2865],\n",
      "        [ 0.7214,  0.2194],\n",
      "        [ 0.8002,  0.2710],\n",
      "        [ 0.6671, -0.0985],\n",
      "        [ 0.6551,  0.3025],\n",
      "        [ 0.5563, -0.0897],\n",
      "        [ 0.4122, -0.0039],\n",
      "        [ 1.2390,  0.0296],\n",
      "        [ 1.0386, -0.0120],\n",
      "        [ 0.7155,  0.0940],\n",
      "        [ 0.7154,  0.2042],\n",
      "        [ 0.6050, -0.3090],\n",
      "        [ 0.7520,  0.0793],\n",
      "        [ 0.8919, -0.1770],\n",
      "        [ 0.7329, -0.1876],\n",
      "        [ 0.8033,  0.1651],\n",
      "        [ 0.7759,  0.2605],\n",
      "        [ 0.6217,  0.1949],\n",
      "        [ 0.8776,  0.0387],\n",
      "        [ 0.6994,  0.1818],\n",
      "        [ 0.5992, -0.0302]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.7108, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6906,  0.2395],\n",
      "        [ 0.3897,  0.0167],\n",
      "        [ 0.7270,  0.0679],\n",
      "        [ 1.0295,  0.1049],\n",
      "        [ 0.9185, -0.0335],\n",
      "        [ 0.6648,  0.1267],\n",
      "        [ 0.5714, -0.1587],\n",
      "        [ 0.7237,  0.0624],\n",
      "        [ 0.7704,  0.1301],\n",
      "        [ 0.6870,  0.1620],\n",
      "        [ 0.7951,  0.1441],\n",
      "        [ 0.5479, -0.1020],\n",
      "        [ 0.5274,  0.2836],\n",
      "        [ 0.9071,  0.0701],\n",
      "        [ 0.5625, -0.0782],\n",
      "        [ 0.7309, -0.0743],\n",
      "        [ 0.3234, -0.0198],\n",
      "        [ 0.9084,  0.0429],\n",
      "        [ 0.5794,  0.2165],\n",
      "        [ 0.7918,  0.1984],\n",
      "        [ 0.5615, -0.0708],\n",
      "        [ 0.7305, -0.0829],\n",
      "        [ 0.7427,  0.0844],\n",
      "        [ 0.9335,  0.1161],\n",
      "        [ 0.7208, -0.0981],\n",
      "        [-0.0371,  0.0753],\n",
      "        [ 0.6908, -0.0427],\n",
      "        [ 0.5969,  0.1033],\n",
      "        [ 0.9541,  0.2102],\n",
      "        [ 0.7291,  0.1047],\n",
      "        [ 0.3017,  0.0391],\n",
      "        [ 0.8231,  0.1808]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8550, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9120,  0.0939],\n",
      "        [ 0.2552, -0.0127],\n",
      "        [ 0.8177,  0.1753],\n",
      "        [ 0.7412,  0.0337],\n",
      "        [ 0.9492,  0.1516],\n",
      "        [ 1.0293,  0.2107],\n",
      "        [ 0.7323, -0.0745],\n",
      "        [ 1.0079,  0.1731],\n",
      "        [ 0.6979,  0.0783],\n",
      "        [ 0.8115,  0.1424],\n",
      "        [ 0.7474,  0.3481],\n",
      "        [ 0.3432, -0.0549],\n",
      "        [ 0.8366, -0.0210],\n",
      "        [ 0.6748,  0.0188],\n",
      "        [ 0.8130,  0.1266],\n",
      "        [ 0.6485,  0.1815],\n",
      "        [ 0.9404,  0.0180],\n",
      "        [ 0.4560,  0.0037],\n",
      "        [ 0.8369,  0.1493],\n",
      "        [ 0.5409,  0.1178],\n",
      "        [ 0.6371, -0.0818],\n",
      "        [ 0.7827, -0.0236],\n",
      "        [ 0.6607,  0.1281],\n",
      "        [ 1.1080,  0.2851],\n",
      "        [ 0.7933, -0.0847],\n",
      "        [ 0.3620,  0.0735],\n",
      "        [ 1.0217,  0.2376],\n",
      "        [ 0.7593, -0.0499],\n",
      "        [ 0.5310,  0.2061],\n",
      "        [ 0.7277, -0.2498],\n",
      "        [ 1.0132,  0.0943],\n",
      "        [ 0.7239, -0.2950]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.8818, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7469,  0.0612],\n",
      "        [ 0.7604,  0.2498],\n",
      "        [ 0.7003,  0.0413],\n",
      "        [ 0.8678, -0.1376],\n",
      "        [ 0.8444,  0.1399],\n",
      "        [ 0.5493,  0.1400],\n",
      "        [ 0.8159,  0.0950],\n",
      "        [ 0.9270,  0.0521],\n",
      "        [ 1.0157,  0.2474],\n",
      "        [ 0.7967,  0.0570],\n",
      "        [ 0.7520,  0.0931],\n",
      "        [ 0.6334,  0.2906],\n",
      "        [ 0.8777, -0.0664],\n",
      "        [ 0.4995,  0.2157],\n",
      "        [ 0.6800,  0.0857],\n",
      "        [ 1.0914,  0.0325],\n",
      "        [ 0.5170,  0.0686],\n",
      "        [ 0.8396,  0.2542],\n",
      "        [ 0.6920,  0.2367],\n",
      "        [ 0.6358,  0.1209],\n",
      "        [ 0.6044,  0.1148],\n",
      "        [ 0.5727,  0.1883],\n",
      "        [ 0.9428,  0.0948],\n",
      "        [ 0.6886,  0.2998],\n",
      "        [ 0.6932,  0.0492],\n",
      "        [ 0.6676, -0.0486],\n",
      "        [ 0.6385,  0.2320],\n",
      "        [ 0.8999,  0.1951],\n",
      "        [ 0.9401,  0.0863],\n",
      "        [ 1.0669,  0.1690],\n",
      "        [ 0.8923, -0.0728],\n",
      "        [ 0.2579, -0.1166]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9144, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9155,  0.2527],\n",
      "        [ 0.8143,  0.2162],\n",
      "        [ 0.8069, -0.0313],\n",
      "        [ 0.5579,  0.2076],\n",
      "        [ 1.1362, -0.0404],\n",
      "        [ 0.8484, -0.1176],\n",
      "        [ 0.3924, -0.0597],\n",
      "        [ 0.8247, -0.0495],\n",
      "        [ 0.9548, -0.2436],\n",
      "        [ 0.8403, -0.0741],\n",
      "        [ 0.7481,  0.1046],\n",
      "        [ 0.6117,  0.0234],\n",
      "        [ 0.9446, -0.0662],\n",
      "        [ 0.5399,  0.1856],\n",
      "        [ 0.9371,  0.1786],\n",
      "        [ 0.7485,  0.0800],\n",
      "        [ 0.7739, -0.0264],\n",
      "        [ 1.0096, -0.1068],\n",
      "        [ 0.7695,  0.1306],\n",
      "        [ 0.6250,  0.0559],\n",
      "        [ 0.8678,  0.1434],\n",
      "        [ 0.6415,  0.1770],\n",
      "        [ 0.8343,  0.2411],\n",
      "        [ 0.8119,  0.1606],\n",
      "        [ 0.8514,  0.1270],\n",
      "        [ 0.9738,  0.3533],\n",
      "        [ 0.7428,  0.1250],\n",
      "        [ 0.8273,  0.4575],\n",
      "        [ 0.6616,  0.2051],\n",
      "        [ 0.7829,  0.1490],\n",
      "        [ 0.2691, -0.0254],\n",
      "        [ 0.5751,  0.0897]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8466, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7431, -0.0379],\n",
      "        [ 1.1084,  0.1645],\n",
      "        [ 0.6596, -0.0489],\n",
      "        [ 0.9080,  0.4704],\n",
      "        [ 0.9530,  0.0743],\n",
      "        [ 0.8716,  0.0447],\n",
      "        [ 1.0816,  0.2188],\n",
      "        [ 0.6825,  0.0113],\n",
      "        [ 1.0942,  0.2698],\n",
      "        [ 1.0267, -0.3324],\n",
      "        [ 0.6882,  0.1603],\n",
      "        [ 0.6813,  0.1804],\n",
      "        [ 0.8966,  0.1136],\n",
      "        [ 0.5943,  0.1179],\n",
      "        [ 0.7491,  0.0064],\n",
      "        [ 0.6656,  0.2254],\n",
      "        [ 0.9096,  0.2277],\n",
      "        [ 0.8072, -0.1660],\n",
      "        [ 0.7368,  0.0468],\n",
      "        [ 0.6611,  0.2496],\n",
      "        [ 0.8266,  0.0401],\n",
      "        [ 0.3524,  0.0012],\n",
      "        [ 1.0942,  0.2199],\n",
      "        [ 0.8632,  0.3019],\n",
      "        [ 0.3775, -0.1235],\n",
      "        [ 0.8212,  0.0779],\n",
      "        [ 0.9435, -0.0601],\n",
      "        [ 0.4874,  0.0098],\n",
      "        [ 0.5723, -0.0898],\n",
      "        [ 0.7996,  0.1284],\n",
      "        [ 0.6975, -0.0261],\n",
      "        [ 0.7716, -0.0349]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8484, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7741,  0.0438],\n",
      "        [ 0.9309,  0.1901],\n",
      "        [ 0.5395,  0.2568],\n",
      "        [ 0.7532,  0.1194],\n",
      "        [ 0.6563,  0.1548],\n",
      "        [ 0.6838, -0.0763],\n",
      "        [ 0.7304,  0.1675],\n",
      "        [ 0.7002,  0.1779],\n",
      "        [ 0.8268,  0.2363],\n",
      "        [ 0.6786,  0.1946],\n",
      "        [ 1.0513,  0.2981],\n",
      "        [ 0.4897,  0.0188],\n",
      "        [ 0.6108, -0.0225],\n",
      "        [ 0.8410,  0.1268],\n",
      "        [ 0.4009, -0.1046],\n",
      "        [ 0.7793, -0.1107],\n",
      "        [ 0.6150, -0.0633],\n",
      "        [ 0.1802,  0.2550],\n",
      "        [ 0.3196, -0.1411],\n",
      "        [ 0.6458, -0.0036],\n",
      "        [ 0.7483,  0.0057],\n",
      "        [ 1.0112,  0.2566],\n",
      "        [ 0.6044,  0.0922],\n",
      "        [ 0.8291,  0.1401],\n",
      "        [ 1.0450,  0.0097],\n",
      "        [ 0.2395,  0.1213],\n",
      "        [ 0.7600,  0.1698],\n",
      "        [ 1.0629,  0.2067],\n",
      "        [ 0.3957, -0.0108],\n",
      "        [ 0.8444,  0.1118],\n",
      "        [ 0.9059, -0.0253],\n",
      "        [ 0.6373, -0.0262]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.8266, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6462,  0.1514],\n",
      "        [ 0.7269,  0.0731],\n",
      "        [ 0.8368,  0.1534],\n",
      "        [ 0.4711, -0.0602],\n",
      "        [ 0.6010,  0.0434],\n",
      "        [ 0.6023,  0.1061],\n",
      "        [ 0.6772,  0.1113],\n",
      "        [ 0.9054,  0.0978],\n",
      "        [ 0.8418,  0.0200],\n",
      "        [ 0.6782, -0.0078],\n",
      "        [ 0.3983,  0.2208],\n",
      "        [ 0.8635,  0.0813],\n",
      "        [ 0.5781,  0.2091],\n",
      "        [ 0.6332,  0.1677],\n",
      "        [ 1.2822,  0.2968],\n",
      "        [ 0.8330, -0.1373],\n",
      "        [ 1.0456,  0.0908],\n",
      "        [ 0.9373,  0.0130],\n",
      "        [ 1.0765,  0.2114],\n",
      "        [ 0.9060,  0.0662],\n",
      "        [ 0.3719,  0.0316],\n",
      "        [ 0.7888, -0.0756],\n",
      "        [ 0.7000,  0.0553],\n",
      "        [ 0.9064,  0.1956],\n",
      "        [ 0.9625,  0.4064],\n",
      "        [ 0.7839,  0.1461],\n",
      "        [ 0.7917,  0.1933],\n",
      "        [ 0.3898,  0.2746],\n",
      "        [ 0.7713,  0.1874],\n",
      "        [ 0.6025, -0.0648],\n",
      "        [ 0.5764,  0.1186],\n",
      "        [ 0.8561,  0.2167]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8875, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8474,  0.2261],\n",
      "        [ 0.7610,  0.1774],\n",
      "        [ 0.7904,  0.0475],\n",
      "        [ 0.8855,  0.2735],\n",
      "        [ 0.7903,  0.1103],\n",
      "        [ 0.7452,  0.2138],\n",
      "        [ 0.5050,  0.1040],\n",
      "        [ 0.8645,  0.0610],\n",
      "        [ 0.5201,  0.1202],\n",
      "        [ 0.6833,  0.0098],\n",
      "        [ 0.7413,  0.0521],\n",
      "        [-0.0935,  0.0251],\n",
      "        [ 0.9682, -0.0130],\n",
      "        [ 0.6155,  0.1115],\n",
      "        [ 0.5276,  0.0647],\n",
      "        [ 0.6959,  0.0739],\n",
      "        [ 0.5019,  0.1017],\n",
      "        [ 0.8190, -0.0266],\n",
      "        [ 0.9205,  0.0433],\n",
      "        [ 0.8198,  0.1730],\n",
      "        [ 0.5441,  0.1483],\n",
      "        [ 0.9240,  0.1935],\n",
      "        [ 0.9667,  0.1827],\n",
      "        [ 0.5186,  0.1810],\n",
      "        [ 0.9506, -0.0214],\n",
      "        [ 0.7301, -0.1509],\n",
      "        [ 0.9887,  0.1447],\n",
      "        [-0.0700,  0.1011],\n",
      "        [ 0.6465,  0.1825],\n",
      "        [ 0.9119, -0.0696],\n",
      "        [ 0.5203,  0.2209],\n",
      "        [ 0.8300,  0.1644]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9629, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8042,  0.0739],\n",
      "        [ 1.0439,  0.1837],\n",
      "        [ 0.7551, -0.1182],\n",
      "        [ 0.6211,  0.2020],\n",
      "        [ 0.5111, -0.2298],\n",
      "        [ 0.9335,  0.0443],\n",
      "        [ 0.7078,  0.2126],\n",
      "        [ 0.6634,  0.0179],\n",
      "        [ 0.5634,  0.0173],\n",
      "        [ 0.7725, -0.1029],\n",
      "        [ 0.4800, -0.2258],\n",
      "        [ 0.8381,  0.0466],\n",
      "        [ 0.3833,  0.0396],\n",
      "        [ 0.6593,  0.1947],\n",
      "        [ 0.6788,  0.0742],\n",
      "        [ 0.7022, -0.0407],\n",
      "        [ 0.5591, -0.2221],\n",
      "        [ 0.7770,  0.0284],\n",
      "        [ 0.4409, -0.0979],\n",
      "        [ 0.7195,  0.0912],\n",
      "        [ 0.4387,  0.0654],\n",
      "        [ 0.6748,  0.0665],\n",
      "        [ 0.8169,  0.2749],\n",
      "        [ 0.8448, -0.0607],\n",
      "        [ 0.8269,  0.0558],\n",
      "        [ 0.5903,  0.1401],\n",
      "        [ 0.4500,  0.0070],\n",
      "        [ 0.6344,  0.3429],\n",
      "        [ 0.7584, -0.0902],\n",
      "        [ 0.4869,  0.1030],\n",
      "        [ 0.8628,  0.1233],\n",
      "        [ 1.0671,  0.0942]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8486, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3379,  0.0627],\n",
      "        [ 0.8403, -0.2163],\n",
      "        [ 0.8430,  0.1028],\n",
      "        [ 0.9405, -0.0817],\n",
      "        [ 0.8928,  0.1977],\n",
      "        [ 1.0658,  0.0867],\n",
      "        [ 0.5391,  0.2285],\n",
      "        [ 0.5977,  0.2042],\n",
      "        [ 0.9071, -0.0185],\n",
      "        [ 1.0690, -0.3617],\n",
      "        [ 0.5846,  0.0873],\n",
      "        [ 0.8429,  0.2344],\n",
      "        [ 0.7333,  0.2894],\n",
      "        [ 0.7792,  0.0935],\n",
      "        [ 0.8769, -0.0573],\n",
      "        [ 0.7746,  0.0118],\n",
      "        [ 0.7608,  0.1660],\n",
      "        [ 0.8795,  0.0404],\n",
      "        [ 0.6360, -0.0187],\n",
      "        [ 0.6478,  0.2962],\n",
      "        [ 0.7163,  0.1433],\n",
      "        [ 0.7959,  0.1293],\n",
      "        [ 0.7911,  0.1099],\n",
      "        [ 0.6374,  0.1873],\n",
      "        [ 0.5380,  0.1257],\n",
      "        [ 0.8884,  0.1823],\n",
      "        [ 0.8047,  0.0177],\n",
      "        [ 0.6485, -0.1225],\n",
      "        [ 0.7163,  0.0443],\n",
      "        [ 0.8166,  0.0963],\n",
      "        [ 0.6808,  0.1214],\n",
      "        [ 0.4149,  0.1612]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.8533, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 8.6878e-01,  1.8284e-01],\n",
      "        [ 8.7948e-01, -5.4660e-04],\n",
      "        [ 6.2813e-01,  7.4799e-02],\n",
      "        [ 8.1248e-01,  4.8459e-02],\n",
      "        [ 6.1475e-01,  3.2974e-01],\n",
      "        [ 8.2470e-01,  2.4418e-01],\n",
      "        [ 7.9102e-01,  3.0857e-01],\n",
      "        [ 4.9065e-01,  1.0327e-02],\n",
      "        [ 5.4914e-01,  7.9578e-02],\n",
      "        [ 5.8098e-01,  1.4102e-01],\n",
      "        [ 6.8324e-01,  2.1191e-02],\n",
      "        [ 7.2714e-01, -8.9353e-02],\n",
      "        [ 6.6713e-01, -1.1375e-01],\n",
      "        [ 4.7392e-01,  2.8409e-01],\n",
      "        [ 7.2435e-01,  1.7602e-02],\n",
      "        [ 5.7597e-01,  9.1109e-02],\n",
      "        [ 8.5809e-01, -3.1504e-02],\n",
      "        [ 7.4281e-01,  1.9758e-01],\n",
      "        [ 6.4917e-01, -6.9639e-02],\n",
      "        [ 3.9751e-01,  1.0574e-01],\n",
      "        [ 9.2647e-01,  1.3234e-01],\n",
      "        [ 5.4507e-01,  1.0048e-01],\n",
      "        [ 4.1788e-01, -1.1529e-01],\n",
      "        [ 8.2681e-01, -1.4495e-01],\n",
      "        [ 6.2975e-01,  6.9409e-02],\n",
      "        [ 8.5919e-01,  8.2159e-02],\n",
      "        [ 7.9353e-01, -7.4563e-02],\n",
      "        [ 4.8444e-01,  5.9531e-02],\n",
      "        [ 5.5665e-01, -1.0688e-01],\n",
      "        [ 6.4889e-01, -4.7430e-02],\n",
      "        [ 7.5278e-01,  1.1898e-02],\n",
      "        [ 4.3917e-01,  4.3594e-02]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9034, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8063,  0.0566],\n",
      "        [ 0.9599,  0.0659],\n",
      "        [ 0.9353,  0.1181],\n",
      "        [ 0.7498,  0.3108],\n",
      "        [ 0.7998,  0.1222],\n",
      "        [ 1.0516, -0.0972],\n",
      "        [ 0.7000, -0.1313],\n",
      "        [ 0.8571,  0.1926],\n",
      "        [ 0.8347,  0.3189],\n",
      "        [ 0.5003,  0.0662],\n",
      "        [ 0.7958, -0.0312],\n",
      "        [ 0.6844,  0.2407],\n",
      "        [ 0.8251,  0.1350],\n",
      "        [ 0.4107,  0.0337],\n",
      "        [ 0.6725,  0.1564],\n",
      "        [ 0.4883, -0.0196],\n",
      "        [ 0.8528,  0.2809],\n",
      "        [ 0.2693,  0.0446],\n",
      "        [ 0.5769,  0.0899],\n",
      "        [ 0.6179, -0.0622],\n",
      "        [ 0.7701,  0.2148],\n",
      "        [ 0.8985,  0.1752],\n",
      "        [ 0.5285, -0.0542],\n",
      "        [ 0.7720,  0.0416],\n",
      "        [ 0.7535,  0.1404],\n",
      "        [ 1.1560, -0.2009],\n",
      "        [ 0.7137, -0.1231],\n",
      "        [ 0.9353,  0.1417],\n",
      "        [ 0.7543,  0.1088],\n",
      "        [ 0.7428, -0.0980],\n",
      "        [ 0.8945, -0.0829],\n",
      "        [ 0.8438,  0.2823]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8751, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 5.6476e-01, -2.4318e-02],\n",
      "        [ 1.0756e+00,  1.1306e-01],\n",
      "        [ 7.9680e-01,  1.9872e-01],\n",
      "        [ 9.4273e-01,  9.5881e-02],\n",
      "        [ 5.7303e-01,  2.2058e-01],\n",
      "        [ 5.0095e-01,  1.7458e-01],\n",
      "        [ 7.4421e-01,  1.1831e-01],\n",
      "        [ 3.6228e-01, -1.7191e-02],\n",
      "        [ 5.2637e-01,  1.2242e-01],\n",
      "        [ 4.8581e-01, -2.9019e-04],\n",
      "        [ 9.9591e-01, -1.9390e-01],\n",
      "        [ 4.8249e-01,  2.6438e-02],\n",
      "        [ 9.1829e-01,  6.5891e-02],\n",
      "        [ 7.9763e-01,  2.7021e-01],\n",
      "        [ 7.3681e-01, -8.5034e-02],\n",
      "        [ 7.3183e-01,  2.8780e-01],\n",
      "        [ 4.1518e-01,  1.2986e-01],\n",
      "        [ 8.7258e-01,  8.9220e-02],\n",
      "        [ 7.3533e-01,  2.1186e-01],\n",
      "        [ 7.3933e-01, -1.1732e-01],\n",
      "        [ 7.7234e-01,  8.9558e-02],\n",
      "        [ 6.0662e-01,  7.4539e-02],\n",
      "        [ 3.6452e-01,  4.3496e-02],\n",
      "        [ 7.0708e-01,  1.6845e-01],\n",
      "        [ 8.6593e-01, -3.3854e-02],\n",
      "        [ 9.9502e-01,  4.6597e-02],\n",
      "        [ 3.5886e-01,  8.4159e-03],\n",
      "        [ 8.0898e-01,  1.2058e-01],\n",
      "        [ 8.0508e-01,  6.6822e-02],\n",
      "        [ 2.6550e-01,  1.7964e-01],\n",
      "        [ 6.7154e-01, -2.0871e-02],\n",
      "        [ 1.0003e+00,  1.0877e-01]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8235, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8934, -0.1118],\n",
      "        [ 0.7376, -0.0294],\n",
      "        [ 0.7251,  0.1341],\n",
      "        [ 0.5252,  0.0204],\n",
      "        [ 1.0223, -0.0505],\n",
      "        [ 1.1414,  0.1593],\n",
      "        [ 0.5742,  0.1382],\n",
      "        [ 0.6813,  0.1217],\n",
      "        [ 0.2273,  0.0840],\n",
      "        [ 0.5356,  0.0585],\n",
      "        [ 0.7014,  0.0140],\n",
      "        [ 0.4386,  0.1406],\n",
      "        [ 0.5876,  0.2473],\n",
      "        [ 0.7769, -0.0842],\n",
      "        [ 0.7501, -0.0217],\n",
      "        [ 0.8294,  0.0430],\n",
      "        [ 0.5133, -0.0703],\n",
      "        [ 0.4894, -0.2363],\n",
      "        [ 0.7726,  0.2416],\n",
      "        [ 0.6359,  0.0965],\n",
      "        [ 0.5946,  0.1729],\n",
      "        [ 0.8537,  0.2339],\n",
      "        [ 0.9365,  0.2565],\n",
      "        [ 0.6793, -0.2610],\n",
      "        [ 0.9076,  0.1184],\n",
      "        [ 0.1729,  0.0149],\n",
      "        [ 0.7689, -0.0641],\n",
      "        [ 0.6619, -0.0715],\n",
      "        [ 0.9270,  0.1470],\n",
      "        [ 0.9612,  0.1159],\n",
      "        [ 0.9551,  0.1452],\n",
      "        [ 0.8214,  0.2412]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.8731, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7978, -0.0012],\n",
      "        [ 0.5947,  0.0984],\n",
      "        [ 0.2086,  0.0857],\n",
      "        [ 0.8287,  0.0438],\n",
      "        [ 0.4216,  0.0740],\n",
      "        [ 0.5359,  0.1255],\n",
      "        [ 0.6479, -0.0066],\n",
      "        [ 1.0823,  0.2824],\n",
      "        [ 0.7090,  0.2230],\n",
      "        [ 0.6530,  0.2493],\n",
      "        [ 0.6953,  0.0257],\n",
      "        [ 0.6354,  0.0494],\n",
      "        [ 0.4718,  0.0807],\n",
      "        [ 0.6924,  0.1836],\n",
      "        [ 0.9652, -0.0989],\n",
      "        [ 0.7664,  0.1918],\n",
      "        [ 0.2298,  0.0609],\n",
      "        [ 0.7243, -0.0141],\n",
      "        [ 0.8164,  0.1752],\n",
      "        [ 0.7796,  0.1027],\n",
      "        [ 0.7639,  0.1246],\n",
      "        [ 0.4446, -0.2065],\n",
      "        [ 0.8586,  0.0579],\n",
      "        [ 0.7297,  0.1381],\n",
      "        [ 0.7561,  0.0210],\n",
      "        [ 0.8398,  0.2459],\n",
      "        [ 0.7968,  0.1436],\n",
      "        [ 0.3776,  0.0720],\n",
      "        [ 0.6035, -0.1577],\n",
      "        [ 0.8835, -0.0306],\n",
      "        [ 0.4682, -0.0150],\n",
      "        [ 0.4215, -0.0020]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8497, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8024, -0.0248],\n",
      "        [ 0.9710,  0.1918],\n",
      "        [ 0.9360,  0.0096],\n",
      "        [ 0.7546, -0.0095],\n",
      "        [ 0.9355,  0.1581],\n",
      "        [ 0.8213, -0.3183],\n",
      "        [ 0.9321,  0.1249],\n",
      "        [ 0.7768,  0.1208],\n",
      "        [ 1.0869,  0.1056],\n",
      "        [ 0.4145, -0.1169],\n",
      "        [ 0.7814,  0.0257],\n",
      "        [ 0.9583,  0.1047],\n",
      "        [ 0.8860,  0.1072],\n",
      "        [ 0.7462,  0.0475],\n",
      "        [ 0.7108, -0.0093],\n",
      "        [ 0.3646,  0.0394],\n",
      "        [ 0.9151,  0.2668],\n",
      "        [ 0.7658,  0.1891],\n",
      "        [ 0.8439, -0.2787],\n",
      "        [ 0.8625,  0.1775],\n",
      "        [ 0.7941,  0.1613],\n",
      "        [ 0.3167,  0.1361],\n",
      "        [ 0.6412,  0.0397],\n",
      "        [ 0.4549,  0.0354],\n",
      "        [ 0.8151,  0.1298],\n",
      "        [ 0.7109, -0.0226],\n",
      "        [ 0.7984,  0.1011],\n",
      "        [ 0.7725,  0.1517],\n",
      "        [ 1.0015,  0.0879],\n",
      "        [ 0.8438, -0.0825],\n",
      "        [ 1.1679,  0.0870],\n",
      "        [ 0.6374, -0.0534]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8830, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8457, -0.0854],\n",
      "        [ 0.8243,  0.3191],\n",
      "        [ 0.3541, -0.0138],\n",
      "        [ 0.3033, -0.0292],\n",
      "        [ 0.7496,  0.1211],\n",
      "        [ 0.3186, -0.0243],\n",
      "        [ 0.3037,  0.0992],\n",
      "        [ 0.8949,  0.1255],\n",
      "        [ 0.6898,  0.3829],\n",
      "        [ 0.5533, -0.0381],\n",
      "        [ 0.7789,  0.0846],\n",
      "        [ 1.0016,  0.0116],\n",
      "        [ 1.0332,  0.2037],\n",
      "        [ 0.6625, -0.0317],\n",
      "        [ 0.8474,  0.1098],\n",
      "        [ 0.6999,  0.1824],\n",
      "        [ 0.6437,  0.1694],\n",
      "        [ 0.8299,  0.2907],\n",
      "        [ 1.0036, -0.1125],\n",
      "        [ 0.7153,  0.1131],\n",
      "        [ 0.8512, -0.2682],\n",
      "        [ 0.4310,  0.3887],\n",
      "        [ 0.7826, -0.0639],\n",
      "        [ 0.9423,  0.1779],\n",
      "        [ 0.3826, -0.0251],\n",
      "        [ 0.6982,  0.2206],\n",
      "        [ 0.7376,  0.1060],\n",
      "        [ 0.6264, -0.1158],\n",
      "        [ 1.0251, -0.0049],\n",
      "        [ 0.8502,  0.0715],\n",
      "        [ 0.5923,  0.1705],\n",
      "        [ 0.9696,  0.1079]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.7733, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5219,  0.1035],\n",
      "        [ 0.8267, -0.0501],\n",
      "        [ 0.6574,  0.0842],\n",
      "        [ 0.6404, -0.0792],\n",
      "        [ 1.0571,  0.1402],\n",
      "        [ 0.3283, -0.0747],\n",
      "        [ 0.6653, -0.0427],\n",
      "        [ 0.7873,  0.1603],\n",
      "        [ 0.5544,  0.1565],\n",
      "        [ 0.8098,  0.1483],\n",
      "        [ 0.8571,  0.1210],\n",
      "        [ 1.0743,  0.0245],\n",
      "        [ 0.6324,  0.0097],\n",
      "        [ 0.5416,  0.2934],\n",
      "        [ 0.8816,  0.0314],\n",
      "        [ 1.0709, -0.0590],\n",
      "        [ 0.6792,  0.0729],\n",
      "        [ 0.9926,  0.2016],\n",
      "        [ 0.9308,  0.0701],\n",
      "        [ 0.4489,  0.0051],\n",
      "        [ 0.9573,  0.1828],\n",
      "        [ 0.4848,  0.1718],\n",
      "        [ 0.8587,  0.1520],\n",
      "        [ 0.7202, -0.2462],\n",
      "        [ 0.4516,  0.2064],\n",
      "        [ 0.7415,  0.0560],\n",
      "        [ 0.7625,  0.0826],\n",
      "        [ 0.6827, -0.0168],\n",
      "        [ 0.6132,  0.1151],\n",
      "        [ 0.6901,  0.0730],\n",
      "        [ 0.5033,  0.0196],\n",
      "        [ 0.9738,  0.1906]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.8579, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4558,  0.0502],\n",
      "        [ 0.8184,  0.1159],\n",
      "        [ 0.6485,  0.1553],\n",
      "        [ 0.6512,  0.0549],\n",
      "        [ 0.2913,  0.0157],\n",
      "        [ 0.7461, -0.2071],\n",
      "        [ 0.9383,  0.1147],\n",
      "        [ 0.6888,  0.0870],\n",
      "        [ 0.6326, -0.0326],\n",
      "        [ 0.7781, -0.0712],\n",
      "        [ 0.5838,  0.1022],\n",
      "        [ 0.9876,  0.2664],\n",
      "        [ 0.6917,  0.0082],\n",
      "        [ 0.7978,  0.2349],\n",
      "        [ 1.1667,  0.1694],\n",
      "        [ 0.7995, -0.0168],\n",
      "        [ 0.7458, -0.0961],\n",
      "        [ 0.6681,  0.0691],\n",
      "        [ 0.9334,  0.2713],\n",
      "        [ 0.5873, -0.1187],\n",
      "        [ 0.4332,  0.1461],\n",
      "        [ 0.7878,  0.1507],\n",
      "        [ 0.9474,  0.1841],\n",
      "        [ 0.5043,  0.0127],\n",
      "        [ 0.5837,  0.0604],\n",
      "        [ 0.9820,  0.0943],\n",
      "        [ 0.9139,  0.0092],\n",
      "        [ 0.7275, -0.1303],\n",
      "        [ 0.5012,  0.0488],\n",
      "        [-0.5179,  0.1847],\n",
      "        [ 0.7767, -0.0419],\n",
      "        [ 0.1781, -0.0239]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8157, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7830, -0.0501],\n",
      "        [ 0.3488, -0.0276],\n",
      "        [ 0.6353, -0.0253],\n",
      "        [ 0.8925,  0.0178],\n",
      "        [ 0.7982,  0.2254],\n",
      "        [ 0.8480, -0.1662],\n",
      "        [ 0.9539,  0.0784],\n",
      "        [ 0.8778,  0.3069],\n",
      "        [ 0.5028, -0.0260],\n",
      "        [ 0.4932,  0.0169],\n",
      "        [ 0.7593, -0.1951],\n",
      "        [ 0.7480,  0.0798],\n",
      "        [ 1.0369, -0.0203],\n",
      "        [ 0.6169,  0.0315],\n",
      "        [ 0.8408, -0.1288],\n",
      "        [ 0.8657, -0.1386],\n",
      "        [ 1.0968,  0.2191],\n",
      "        [ 0.7133,  0.1082],\n",
      "        [ 0.8794,  0.0338],\n",
      "        [ 0.6789,  0.0640],\n",
      "        [ 0.8425,  0.1176],\n",
      "        [ 0.1416,  0.1986],\n",
      "        [ 0.5994,  0.1230],\n",
      "        [ 0.9022,  0.0266],\n",
      "        [ 0.8981, -0.0260],\n",
      "        [ 0.6215,  0.3260],\n",
      "        [ 0.6868,  0.0451],\n",
      "        [ 0.8596, -0.0629],\n",
      "        [ 0.6339,  0.1400],\n",
      "        [ 0.4813,  0.1127],\n",
      "        [ 0.2295,  0.0776],\n",
      "        [ 0.7935,  0.2157]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8913, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7278, -0.0451],\n",
      "        [ 0.5320,  0.2468],\n",
      "        [ 1.1298, -0.3173],\n",
      "        [ 0.6612,  0.2224],\n",
      "        [ 0.8675, -0.0090],\n",
      "        [ 0.5171,  0.2141],\n",
      "        [ 0.6169,  0.2650],\n",
      "        [ 0.6561, -0.0207],\n",
      "        [ 0.4904,  0.0949],\n",
      "        [ 1.1226,  0.1665],\n",
      "        [ 0.7974,  0.0647],\n",
      "        [ 0.9967,  0.2288],\n",
      "        [ 0.2976,  0.0488],\n",
      "        [ 0.5450, -0.1631],\n",
      "        [ 0.9012,  0.0512],\n",
      "        [ 0.7128,  0.1299],\n",
      "        [ 0.8145, -0.0960],\n",
      "        [ 0.6432, -0.0167],\n",
      "        [ 0.5187, -0.0411],\n",
      "        [ 0.4905,  0.2756],\n",
      "        [ 0.5660,  0.0706],\n",
      "        [ 0.6049,  0.1402],\n",
      "        [ 0.7834,  0.1645],\n",
      "        [ 1.0898,  0.0483],\n",
      "        [ 0.7130,  0.2277],\n",
      "        [ 0.9027,  0.0742],\n",
      "        [ 1.0031,  0.1712],\n",
      "        [ 0.7333,  0.1367],\n",
      "        [ 0.5839, -0.1151],\n",
      "        [ 0.1343,  0.0598],\n",
      "        [ 0.4930,  0.0922],\n",
      "        [ 1.1971,  0.0547]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8780, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7842,  0.0022],\n",
      "        [ 0.6898,  0.2143],\n",
      "        [ 0.7091,  0.1257],\n",
      "        [ 0.7621,  0.2146],\n",
      "        [ 0.7006,  0.2042],\n",
      "        [ 1.0072,  0.1224],\n",
      "        [ 0.8196,  0.1021],\n",
      "        [ 0.6200,  0.0517],\n",
      "        [ 0.4907,  0.1902],\n",
      "        [ 0.6467,  0.2460],\n",
      "        [ 0.9659,  0.2632],\n",
      "        [ 0.8140, -0.2839],\n",
      "        [ 0.6862,  0.2416],\n",
      "        [ 1.0587,  0.1575],\n",
      "        [ 0.8931,  0.1032],\n",
      "        [ 0.4117,  0.1228],\n",
      "        [ 0.8610,  0.0825],\n",
      "        [ 0.9753,  0.0159],\n",
      "        [ 0.7325,  0.0125],\n",
      "        [ 1.0269, -0.0068],\n",
      "        [ 0.7080,  0.0601],\n",
      "        [ 0.6914,  0.3830],\n",
      "        [ 0.8232,  0.1401],\n",
      "        [ 0.7097,  0.1558],\n",
      "        [ 0.8352,  0.2389],\n",
      "        [ 0.8746,  0.0393],\n",
      "        [ 0.5931,  0.1251],\n",
      "        [ 0.7856,  0.1854],\n",
      "        [ 0.8828,  0.0500],\n",
      "        [ 0.9475,  0.1832],\n",
      "        [ 0.8452,  0.1451],\n",
      "        [ 0.5689, -0.0390]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.9167, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7176,  0.2092],\n",
      "        [ 0.9914,  0.2417],\n",
      "        [ 1.0611,  0.1065],\n",
      "        [ 0.5132, -0.0386],\n",
      "        [ 0.7876, -0.1662],\n",
      "        [ 0.9363, -0.0591],\n",
      "        [ 1.0133, -0.0445],\n",
      "        [ 0.9083,  0.1443],\n",
      "        [ 0.7845, -0.0399],\n",
      "        [ 0.7995,  0.1858],\n",
      "        [ 0.6444, -0.0831],\n",
      "        [ 0.5307,  0.3089],\n",
      "        [ 0.7819,  0.2542],\n",
      "        [ 0.7127,  0.0480],\n",
      "        [ 0.7784,  0.1312],\n",
      "        [ 0.9024,  0.0125],\n",
      "        [ 1.1016,  0.1437],\n",
      "        [ 0.5952,  0.0934],\n",
      "        [ 0.9124,  0.1869],\n",
      "        [ 0.5252,  0.1569],\n",
      "        [ 1.0963,  0.1531],\n",
      "        [ 0.8542,  0.1373],\n",
      "        [ 0.6803,  0.3172],\n",
      "        [ 0.8924,  0.0814],\n",
      "        [ 0.9623,  0.0033],\n",
      "        [ 0.8451,  0.0758],\n",
      "        [ 0.8422,  0.1889],\n",
      "        [ 1.0054, -0.0381],\n",
      "        [ 0.5868,  0.1914],\n",
      "        [ 0.4275, -0.0329],\n",
      "        [ 0.7498, -0.1479],\n",
      "        [ 0.5659, -0.0142]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8879, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6311, -0.0486],\n",
      "        [ 0.6670,  0.1645],\n",
      "        [ 0.4669,  0.2804],\n",
      "        [ 0.9657, -0.0679],\n",
      "        [ 0.7995,  0.0895],\n",
      "        [ 0.6903,  0.0535],\n",
      "        [ 0.8634,  0.2419],\n",
      "        [ 0.5440,  0.0758],\n",
      "        [ 0.7861, -0.0756],\n",
      "        [ 0.7582,  0.1586],\n",
      "        [ 0.6753, -0.0289],\n",
      "        [ 0.7888,  0.0876],\n",
      "        [ 0.3593,  0.0632],\n",
      "        [ 0.7697,  0.0552],\n",
      "        [ 0.6377, -0.0688],\n",
      "        [ 0.5461, -0.1327],\n",
      "        [ 0.5651,  0.0426],\n",
      "        [ 0.5996,  0.3201],\n",
      "        [ 0.9299,  0.3026],\n",
      "        [ 0.4826,  0.0212],\n",
      "        [ 0.5676,  0.1054],\n",
      "        [ 0.7009,  0.0105],\n",
      "        [ 0.6734,  0.2470],\n",
      "        [ 0.6511, -0.2138],\n",
      "        [ 0.5495,  0.0824],\n",
      "        [ 0.3515,  0.0918],\n",
      "        [ 0.6247, -0.1199],\n",
      "        [ 0.3862,  0.3221],\n",
      "        [ 0.8152,  0.1014],\n",
      "        [ 0.7683,  0.2131],\n",
      "        [ 0.7219,  0.0377],\n",
      "        [ 0.6101,  0.2232]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8308, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7756, -0.1326],\n",
      "        [ 0.4711,  0.0671],\n",
      "        [ 0.5651,  0.1219],\n",
      "        [ 0.9140,  0.1223],\n",
      "        [ 0.6868,  0.0885],\n",
      "        [ 0.9763, -0.1476],\n",
      "        [ 0.6992, -0.0386],\n",
      "        [ 0.8057,  0.2004],\n",
      "        [ 0.7799,  0.2926],\n",
      "        [ 0.8175,  0.1267],\n",
      "        [ 0.6308, -0.3305],\n",
      "        [ 0.6303,  0.1884],\n",
      "        [ 0.4683,  0.1276],\n",
      "        [ 1.0080,  0.0201],\n",
      "        [ 0.6084, -0.0467],\n",
      "        [ 0.7463, -0.0469],\n",
      "        [ 0.9919,  0.1223],\n",
      "        [ 0.9769, -0.0707],\n",
      "        [ 0.8832,  0.1122],\n",
      "        [ 0.6171,  0.1469],\n",
      "        [ 0.8010,  0.0163],\n",
      "        [ 0.9195,  0.1650],\n",
      "        [ 0.8154, -0.0763],\n",
      "        [ 0.8959,  0.0750],\n",
      "        [ 0.7004, -0.0785],\n",
      "        [ 0.8374,  0.0803],\n",
      "        [ 0.6752, -0.1079],\n",
      "        [ 0.9306,  0.2508],\n",
      "        [ 0.7824,  0.2181],\n",
      "        [ 0.4584,  0.0466],\n",
      "        [ 0.6698,  0.0205],\n",
      "        [ 0.9653,  0.1879]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(1.0519, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8797,  0.1894],\n",
      "        [ 0.5645,  0.0497],\n",
      "        [ 0.6338,  0.1404],\n",
      "        [ 0.7111, -0.1011],\n",
      "        [ 0.7334,  0.0016],\n",
      "        [ 0.8043, -0.1231],\n",
      "        [ 0.9691,  0.1047],\n",
      "        [ 0.7347,  0.3822],\n",
      "        [ 0.8609,  0.0420],\n",
      "        [ 0.5467, -0.1520],\n",
      "        [ 1.0580,  0.1058],\n",
      "        [ 0.6030,  0.0414],\n",
      "        [ 0.6597,  0.0736],\n",
      "        [ 0.6893,  0.1487],\n",
      "        [ 0.8685, -0.1204],\n",
      "        [ 0.8675, -0.0021],\n",
      "        [ 0.5371,  0.0711],\n",
      "        [ 0.9748, -0.3611],\n",
      "        [ 0.9044, -0.0376],\n",
      "        [ 0.9628,  0.1484],\n",
      "        [ 0.6192,  0.3324],\n",
      "        [ 0.9682,  0.1069],\n",
      "        [ 0.8150,  0.1258],\n",
      "        [ 0.6662,  0.0277],\n",
      "        [ 0.7641, -0.2814],\n",
      "        [ 0.6201, -0.1615],\n",
      "        [ 0.8945, -0.0146],\n",
      "        [ 0.7346,  0.0591],\n",
      "        [ 1.3032,  0.1942],\n",
      "        [ 0.7379, -0.1842],\n",
      "        [ 0.9318,  0.2764],\n",
      "        [ 0.7081,  0.1567]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.9548, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7040,  0.0336],\n",
      "        [ 0.8670, -0.1706],\n",
      "        [ 0.4601,  0.1923],\n",
      "        [ 0.9517,  0.2973],\n",
      "        [ 0.8586, -0.1884],\n",
      "        [ 0.9116,  0.0444],\n",
      "        [ 0.9421,  0.2547],\n",
      "        [ 0.9954,  0.0152],\n",
      "        [ 0.7622,  0.0678],\n",
      "        [ 0.8304,  0.1897],\n",
      "        [ 0.9711, -0.1294],\n",
      "        [ 0.6684, -0.0610],\n",
      "        [ 0.5358, -0.0295],\n",
      "        [ 0.6551,  0.1442],\n",
      "        [ 1.0086,  0.3309],\n",
      "        [ 0.7942,  0.0231],\n",
      "        [ 0.7933, -0.1053],\n",
      "        [ 0.5217,  0.3000],\n",
      "        [ 0.7398, -0.0528],\n",
      "        [ 0.8569,  0.2555],\n",
      "        [ 0.8200,  0.1331],\n",
      "        [ 1.0187,  0.0526],\n",
      "        [ 0.6797, -0.0539],\n",
      "        [ 0.7739,  0.1955],\n",
      "        [ 0.8915,  0.2490],\n",
      "        [ 0.7119,  0.1741],\n",
      "        [ 0.5347,  0.0226],\n",
      "        [ 1.1341,  0.1156],\n",
      "        [ 0.8361,  0.0691],\n",
      "        [ 0.5460,  0.0083],\n",
      "        [ 0.7144,  0.1462],\n",
      "        [ 0.8354, -0.0941]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(1.0286, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8368,  0.1834],\n",
      "        [ 1.1901,  0.1694],\n",
      "        [ 0.8948,  0.2466],\n",
      "        [ 0.7850, -0.2347],\n",
      "        [ 0.8086,  0.1488],\n",
      "        [ 0.7481, -0.0523],\n",
      "        [ 0.6957,  0.2579],\n",
      "        [ 0.8261,  0.0104],\n",
      "        [ 0.4930,  0.2542],\n",
      "        [ 0.8943,  0.2718],\n",
      "        [ 0.8744,  0.1687],\n",
      "        [ 0.8096,  0.1370],\n",
      "        [ 0.4891,  0.1411],\n",
      "        [ 0.6993,  0.2145],\n",
      "        [ 0.9948,  0.0798],\n",
      "        [ 0.5438,  0.1222],\n",
      "        [ 0.8002,  0.1421],\n",
      "        [ 0.9435, -0.0488],\n",
      "        [ 0.9184,  0.1519],\n",
      "        [ 0.6591,  0.0946],\n",
      "        [ 0.4614,  0.0124],\n",
      "        [ 0.8714,  0.1216],\n",
      "        [ 0.7308,  0.0543],\n",
      "        [ 0.7431, -0.0206],\n",
      "        [ 0.4888, -0.3154],\n",
      "        [ 1.0266, -0.0501],\n",
      "        [ 0.8218, -0.1484],\n",
      "        [ 0.7812, -0.0464],\n",
      "        [ 0.7900,  0.2170],\n",
      "        [ 0.9413, -0.0659],\n",
      "        [ 0.5976,  0.1229],\n",
      "        [ 0.6851,  0.1743]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8867, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5644,  0.2173],\n",
      "        [ 0.7619,  0.0105],\n",
      "        [ 0.7743,  0.0714],\n",
      "        [ 0.4841,  0.1368],\n",
      "        [ 0.9870, -0.1046],\n",
      "        [ 0.7059,  0.1383],\n",
      "        [ 0.8084,  0.0034],\n",
      "        [ 0.9420,  0.1325],\n",
      "        [ 0.9881,  0.0444],\n",
      "        [ 0.8033,  0.3743],\n",
      "        [ 1.1674, -0.1170],\n",
      "        [ 0.5509,  0.1784],\n",
      "        [ 0.0933, -0.1943],\n",
      "        [ 0.6111,  0.1942],\n",
      "        [ 0.7891,  0.1548],\n",
      "        [ 0.6819,  0.1559],\n",
      "        [ 0.6728, -0.1598],\n",
      "        [ 0.4058, -0.0118],\n",
      "        [ 0.6495,  0.0180],\n",
      "        [ 1.0260,  0.0892],\n",
      "        [ 0.9408, -0.0971],\n",
      "        [ 0.6282, -0.0924],\n",
      "        [ 0.4877,  0.0200],\n",
      "        [ 0.6759,  0.1718],\n",
      "        [ 0.5543, -0.0737],\n",
      "        [ 0.4806,  0.0982],\n",
      "        [ 0.8164,  0.2229],\n",
      "        [ 0.7073,  0.1270],\n",
      "        [ 0.3214,  0.1213],\n",
      "        [ 0.6526, -0.2785],\n",
      "        [ 0.9060,  0.1822],\n",
      "        [ 0.4552,  0.1188]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8442, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 1.0234,  0.3902],\n",
      "        [ 1.0050,  0.2300],\n",
      "        [ 0.7275,  0.1604],\n",
      "        [ 0.8904, -0.0377],\n",
      "        [ 0.5530, -0.0208],\n",
      "        [ 0.9066,  0.2038],\n",
      "        [ 0.7215, -0.1803],\n",
      "        [ 1.0020,  0.3319],\n",
      "        [ 0.8886,  0.1874],\n",
      "        [ 0.6444,  0.1908],\n",
      "        [ 0.8676,  0.1592],\n",
      "        [ 0.4759,  0.1968],\n",
      "        [ 0.9697,  0.1455],\n",
      "        [ 0.8172,  0.0402],\n",
      "        [ 0.7597, -0.1395],\n",
      "        [ 0.7361, -0.0155],\n",
      "        [ 0.9383,  0.3487],\n",
      "        [ 0.7619,  0.2373],\n",
      "        [ 0.7403,  0.2226],\n",
      "        [ 0.6635, -0.0327],\n",
      "        [ 0.7918, -0.0041],\n",
      "        [ 1.0329, -0.0695],\n",
      "        [ 0.7928, -0.0243],\n",
      "        [ 0.4328,  0.0935],\n",
      "        [ 0.7825,  0.1087],\n",
      "        [ 0.6847,  0.0175],\n",
      "        [ 1.0241,  0.0882],\n",
      "        [ 0.6826, -0.0319],\n",
      "        [ 0.5583, -0.0048],\n",
      "        [ 0.7549, -0.0647],\n",
      "        [ 0.5798,  0.2476],\n",
      "        [ 0.7455, -0.0319]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.8466, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5895,  0.0900],\n",
      "        [ 0.9764,  0.1605],\n",
      "        [ 0.6198,  0.0611],\n",
      "        [ 1.0655,  0.3266],\n",
      "        [ 0.9051,  0.2431],\n",
      "        [ 0.9756, -0.2429],\n",
      "        [ 0.9812,  0.0681],\n",
      "        [ 0.3210,  0.1312],\n",
      "        [ 0.8697,  0.0420],\n",
      "        [ 0.6170,  0.0629],\n",
      "        [ 0.8585, -0.2279],\n",
      "        [ 0.7082,  0.0401],\n",
      "        [ 0.8797,  0.0546],\n",
      "        [ 0.1768,  0.1208],\n",
      "        [ 1.1794,  0.3713],\n",
      "        [ 0.9682,  0.3321],\n",
      "        [ 0.2758,  0.0089],\n",
      "        [ 0.7756, -0.0040],\n",
      "        [ 0.9194,  0.0074],\n",
      "        [ 0.4579, -0.0700],\n",
      "        [ 0.9735,  0.4202],\n",
      "        [ 0.5973,  0.3554],\n",
      "        [ 0.8897,  0.1110],\n",
      "        [ 0.7978,  0.0073],\n",
      "        [ 0.8311,  0.0445],\n",
      "        [ 0.9321,  0.2389],\n",
      "        [ 0.7766,  0.1285],\n",
      "        [ 0.5401,  0.2085],\n",
      "        [ 0.7489, -0.0706],\n",
      "        [ 0.9260,  0.2697],\n",
      "        [ 0.7043,  0.2206],\n",
      "        [ 0.8423, -0.0040]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8728, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8010,  0.0454],\n",
      "        [ 0.6746,  0.0925],\n",
      "        [ 0.7750,  0.0508],\n",
      "        [ 0.6503,  0.1098],\n",
      "        [ 0.6407, -0.0990],\n",
      "        [ 0.8989, -0.0424],\n",
      "        [ 0.3025, -0.0196],\n",
      "        [ 0.6950,  0.1711],\n",
      "        [ 0.6809, -0.0821],\n",
      "        [ 0.8586,  0.4355],\n",
      "        [ 0.6765,  0.1973],\n",
      "        [ 0.8071,  0.1601],\n",
      "        [ 0.7018, -0.2012],\n",
      "        [ 0.8995,  0.0338],\n",
      "        [ 0.9283,  0.1784],\n",
      "        [ 0.7060, -0.0067],\n",
      "        [ 0.7664,  0.0156],\n",
      "        [ 0.5073,  0.0011],\n",
      "        [ 0.7196,  0.1665],\n",
      "        [ 0.8041, -0.0066],\n",
      "        [ 0.7992,  0.1346],\n",
      "        [ 0.7575, -0.1203],\n",
      "        [ 0.8050,  0.2925],\n",
      "        [ 0.5129,  0.2444],\n",
      "        [ 0.9639,  0.2135],\n",
      "        [ 0.8781, -0.0179],\n",
      "        [ 0.7831, -0.0021],\n",
      "        [ 0.5090, -0.0525],\n",
      "        [ 0.6948,  0.2571],\n",
      "        [ 1.0453,  0.0261],\n",
      "        [ 0.4645,  0.2975],\n",
      "        [ 0.6761, -0.0624]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8470, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5575, -0.0822],\n",
      "        [ 1.1747,  0.1655],\n",
      "        [ 0.8285,  0.2260],\n",
      "        [ 0.7928,  0.1032],\n",
      "        [ 0.7895,  0.0418],\n",
      "        [ 0.4454,  0.2058],\n",
      "        [ 0.9013,  0.0836],\n",
      "        [ 0.5049,  0.1802],\n",
      "        [ 0.7765,  0.0888],\n",
      "        [ 0.8717,  0.1888],\n",
      "        [ 0.7845, -0.0638],\n",
      "        [ 0.6752, -0.2260],\n",
      "        [ 0.5772,  0.0460],\n",
      "        [ 0.9611,  0.0279],\n",
      "        [ 0.9720,  0.0484],\n",
      "        [ 0.8325, -0.0865],\n",
      "        [ 0.8537,  0.1163],\n",
      "        [ 0.8306,  0.0810],\n",
      "        [ 0.8079,  0.2259],\n",
      "        [ 0.7704,  0.0094],\n",
      "        [ 0.4497, -0.0214],\n",
      "        [ 0.9380, -0.0431],\n",
      "        [ 1.0981,  0.2461],\n",
      "        [ 0.6681,  0.3071],\n",
      "        [ 0.8887,  0.0373],\n",
      "        [ 1.0299,  0.0647],\n",
      "        [ 1.1757,  0.2771],\n",
      "        [ 0.8003,  0.0086],\n",
      "        [ 0.8357,  0.0560],\n",
      "        [ 0.4971,  0.1420],\n",
      "        [ 0.6248, -0.0837],\n",
      "        [ 0.6481, -0.0794]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8871, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7722,  0.2899],\n",
      "        [ 0.4269,  0.2537],\n",
      "        [ 0.8988, -0.0722],\n",
      "        [ 0.9314,  0.2283],\n",
      "        [ 0.3936,  0.0746],\n",
      "        [ 0.4912, -0.0707],\n",
      "        [ 0.8382,  0.2131],\n",
      "        [ 0.8606,  0.2360],\n",
      "        [ 0.5821,  0.1844],\n",
      "        [ 0.8496, -0.1143],\n",
      "        [ 0.4044,  0.2305],\n",
      "        [ 0.9782,  0.1481],\n",
      "        [ 0.7462,  0.2067],\n",
      "        [ 0.7144,  0.3293],\n",
      "        [ 0.4625, -0.0494],\n",
      "        [ 0.6557, -0.0097],\n",
      "        [ 0.8844,  0.0357],\n",
      "        [ 0.3856,  0.3586],\n",
      "        [ 0.7762,  0.1237],\n",
      "        [ 0.7888,  0.2104],\n",
      "        [ 0.3072, -0.0036],\n",
      "        [ 0.6305,  0.0597],\n",
      "        [ 0.9042, -0.0452],\n",
      "        [ 0.6734, -0.0880],\n",
      "        [ 0.5919,  0.0271],\n",
      "        [ 0.8960,  0.1377],\n",
      "        [ 0.9540,  0.2964],\n",
      "        [ 0.6946,  0.0826],\n",
      "        [ 0.5626, -0.0315],\n",
      "        [ 0.7361,  0.0277],\n",
      "        [ 0.4427,  0.0190],\n",
      "        [ 0.2392,  0.2016]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.8942, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6430,  0.2637],\n",
      "        [ 0.9523,  0.0452],\n",
      "        [ 0.5560,  0.2626],\n",
      "        [ 0.4881,  0.0455],\n",
      "        [ 0.4973, -0.0051],\n",
      "        [ 0.6854,  0.0158],\n",
      "        [ 0.5596, -0.0574],\n",
      "        [ 0.5205,  0.2639],\n",
      "        [ 0.9921, -0.1117],\n",
      "        [ 0.7015,  0.0555],\n",
      "        [ 0.9230,  0.1531],\n",
      "        [ 0.9189,  0.0960],\n",
      "        [ 0.8933, -0.0733],\n",
      "        [ 0.7016, -0.1699],\n",
      "        [ 0.6890,  0.0636],\n",
      "        [ 1.1727, -0.0660],\n",
      "        [ 0.5646, -0.0628],\n",
      "        [ 0.9719,  0.1991],\n",
      "        [ 0.7890,  0.2961],\n",
      "        [ 0.6531, -0.0234],\n",
      "        [ 0.5522,  0.3504],\n",
      "        [ 0.7418, -0.2650],\n",
      "        [ 0.6460,  0.2640],\n",
      "        [ 0.5989,  0.0470],\n",
      "        [ 0.7311, -0.1680],\n",
      "        [ 0.0897, -0.0195],\n",
      "        [ 0.8414,  0.1787],\n",
      "        [ 0.8599,  0.0387],\n",
      "        [ 0.7658, -0.1587],\n",
      "        [ 0.5305,  0.3270],\n",
      "        [ 0.8118,  0.1652],\n",
      "        [ 0.6828, -0.0142]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8805, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7530,  0.0278],\n",
      "        [ 0.5702, -0.0563],\n",
      "        [ 0.7147, -0.0077],\n",
      "        [ 0.9492,  0.1843],\n",
      "        [ 0.5346,  0.0262],\n",
      "        [ 0.7603,  0.0962],\n",
      "        [ 0.8046,  0.2219],\n",
      "        [ 0.4461,  0.3051],\n",
      "        [ 0.7347, -0.0878],\n",
      "        [ 1.1170,  0.2834],\n",
      "        [ 0.7574, -0.0027],\n",
      "        [ 0.3866,  0.2625],\n",
      "        [ 1.0335, -0.0929],\n",
      "        [ 0.6610, -0.0790],\n",
      "        [ 0.6672,  0.2108],\n",
      "        [ 0.8368,  0.3344],\n",
      "        [ 0.5504,  0.2803],\n",
      "        [ 0.5765,  0.1111],\n",
      "        [ 0.5710,  0.1128],\n",
      "        [ 0.6670,  0.1482],\n",
      "        [ 0.5913, -0.0989],\n",
      "        [ 0.5970, -0.1199],\n",
      "        [ 0.9755, -0.0165],\n",
      "        [ 0.8681,  0.1520],\n",
      "        [ 0.5948,  0.0948],\n",
      "        [ 0.4961, -0.1642],\n",
      "        [ 0.7709,  0.1314],\n",
      "        [ 0.6560,  0.1685],\n",
      "        [ 0.4151, -0.1271],\n",
      "        [ 0.6630,  0.1032],\n",
      "        [ 0.4927,  0.2935],\n",
      "        [ 0.5954,  0.0448]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9444, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5392,  0.1911],\n",
      "        [ 0.5640,  0.1817],\n",
      "        [ 0.8833,  0.2116],\n",
      "        [ 0.2804,  0.1042],\n",
      "        [ 0.8162,  0.1045],\n",
      "        [ 0.6255,  0.3027],\n",
      "        [ 0.8546, -0.0140],\n",
      "        [ 0.3712, -0.0345],\n",
      "        [ 0.6803, -0.0108],\n",
      "        [ 0.6832, -0.0502],\n",
      "        [ 0.6227,  0.0788],\n",
      "        [ 1.1920, -0.0243],\n",
      "        [ 0.7837,  0.0305],\n",
      "        [ 0.8061,  0.0154],\n",
      "        [ 0.4793, -0.0887],\n",
      "        [ 0.8121,  0.2344],\n",
      "        [ 0.9263,  0.2319],\n",
      "        [ 0.6418,  0.0935],\n",
      "        [ 0.7056, -0.0861],\n",
      "        [ 0.9784,  0.2162],\n",
      "        [ 0.7090,  0.0180],\n",
      "        [ 0.6903,  0.3379],\n",
      "        [ 0.9849,  0.0799],\n",
      "        [ 0.6703,  0.2132],\n",
      "        [ 0.6729,  0.0380],\n",
      "        [ 0.9490,  0.2348],\n",
      "        [ 0.9299,  0.3070],\n",
      "        [ 0.8743, -0.0434],\n",
      "        [ 0.5408,  0.1635],\n",
      "        [ 0.5540,  0.0326],\n",
      "        [ 0.7211, -0.1419],\n",
      "        [ 0.8907,  0.1475]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9104, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6164,  0.0617],\n",
      "        [ 0.7145,  0.3666],\n",
      "        [ 0.8274,  0.0599],\n",
      "        [ 0.3770, -0.2186],\n",
      "        [ 0.8107,  0.0980],\n",
      "        [ 0.5462, -0.0539],\n",
      "        [ 0.8644,  0.2608],\n",
      "        [ 0.7941,  0.0479],\n",
      "        [ 0.3499, -0.0710],\n",
      "        [ 0.6584, -0.0622],\n",
      "        [ 0.8028, -0.1981],\n",
      "        [ 0.7195, -0.2073],\n",
      "        [ 0.8336, -0.0322],\n",
      "        [ 0.6606,  0.0851],\n",
      "        [ 0.2778,  0.0112],\n",
      "        [ 0.7970,  0.1640],\n",
      "        [ 0.2769,  0.0552],\n",
      "        [ 0.8344,  0.0094],\n",
      "        [ 0.4393,  0.0196],\n",
      "        [ 0.7663, -0.0100],\n",
      "        [ 0.5185,  0.0726],\n",
      "        [ 1.0027,  0.0784],\n",
      "        [ 0.9712,  0.1054],\n",
      "        [ 0.4411,  0.1714],\n",
      "        [ 0.7824,  0.1381],\n",
      "        [ 0.9083,  0.1795],\n",
      "        [ 0.4383, -0.0336],\n",
      "        [ 0.7738, -0.1196],\n",
      "        [ 0.7753, -0.2882],\n",
      "        [ 0.4806,  0.2195],\n",
      "        [ 0.8552,  0.1523],\n",
      "        [ 0.8256, -0.2214]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.9133, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9367,  0.0968],\n",
      "        [ 1.0969,  0.2840],\n",
      "        [ 0.7759, -0.0279],\n",
      "        [ 0.6926,  0.1652],\n",
      "        [ 0.8799,  0.1868],\n",
      "        [ 0.9198,  0.2344],\n",
      "        [ 0.4248,  0.2247],\n",
      "        [ 0.7822,  0.1992],\n",
      "        [ 0.6137,  0.0609],\n",
      "        [ 0.6372, -0.0592],\n",
      "        [ 0.7962, -0.0074],\n",
      "        [ 0.8560, -0.0174],\n",
      "        [ 0.3714,  0.0476],\n",
      "        [ 0.3727,  0.1440],\n",
      "        [ 0.5567,  0.2315],\n",
      "        [ 0.7532,  0.2074],\n",
      "        [ 0.8844, -0.0648],\n",
      "        [ 1.0157,  0.3021],\n",
      "        [ 1.0665,  0.1513],\n",
      "        [ 0.7946,  0.0021],\n",
      "        [ 1.1729,  0.2602],\n",
      "        [ 0.6457,  0.2065],\n",
      "        [ 0.6014,  0.1396],\n",
      "        [ 0.7187,  0.1189],\n",
      "        [ 0.8501,  0.0091],\n",
      "        [ 0.6332,  0.1199],\n",
      "        [ 0.5243, -0.0077],\n",
      "        [ 0.7249,  0.1697],\n",
      "        [ 0.6885,  0.0745],\n",
      "        [ 1.1519,  0.0691],\n",
      "        [ 0.4650, -0.0716],\n",
      "        [ 0.6884,  0.1071]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8192, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7969,  0.1160],\n",
      "        [ 0.9782,  0.3440],\n",
      "        [ 0.7869,  0.1128],\n",
      "        [ 0.8864,  0.1771],\n",
      "        [ 0.8502,  0.2396],\n",
      "        [ 0.8094, -0.0111],\n",
      "        [ 0.7915,  0.2164],\n",
      "        [ 0.7057,  0.2347],\n",
      "        [ 0.7640,  0.1036],\n",
      "        [ 0.7684, -0.0331],\n",
      "        [ 0.5214, -0.0730],\n",
      "        [ 0.5794, -0.2313],\n",
      "        [ 1.0114,  0.1154],\n",
      "        [ 1.0153,  0.0556],\n",
      "        [ 0.6922,  0.1861],\n",
      "        [ 0.6667,  0.0484],\n",
      "        [ 1.1500, -0.0745],\n",
      "        [ 0.8022, -0.0340],\n",
      "        [ 1.0056,  0.0912],\n",
      "        [ 1.0500,  0.4206],\n",
      "        [ 0.6372,  0.1656],\n",
      "        [ 0.7790, -0.1959],\n",
      "        [ 0.7414,  0.0836],\n",
      "        [ 0.8094,  0.2429],\n",
      "        [ 0.0561,  0.0922],\n",
      "        [ 0.6905,  0.0668],\n",
      "        [ 0.6096,  0.1046],\n",
      "        [ 0.0372,  0.1523],\n",
      "        [ 0.5973, -0.0376],\n",
      "        [ 1.1598,  0.0429],\n",
      "        [ 0.6641,  0.0763],\n",
      "        [ 1.0206,  0.1196]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9048, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7337,  0.1295],\n",
      "        [ 0.9892, -0.0746],\n",
      "        [ 0.6099,  0.2245],\n",
      "        [ 0.7750,  0.1123],\n",
      "        [ 0.8727,  0.1240],\n",
      "        [ 1.0836,  0.0415],\n",
      "        [ 0.8653,  0.0238],\n",
      "        [ 0.6042,  0.2079],\n",
      "        [ 0.4497,  0.0383],\n",
      "        [ 1.1936,  0.2321],\n",
      "        [ 0.6183, -0.2554],\n",
      "        [ 0.7427,  0.1499],\n",
      "        [ 0.8473,  0.0729],\n",
      "        [ 0.4471,  0.2860],\n",
      "        [ 0.6730,  0.4093],\n",
      "        [ 0.7855,  0.1699],\n",
      "        [ 0.9728,  0.1327],\n",
      "        [ 1.0252, -0.1738],\n",
      "        [ 0.8483,  0.1191],\n",
      "        [ 0.6345,  0.0497],\n",
      "        [ 0.9819,  0.0851],\n",
      "        [ 0.7125,  0.0056],\n",
      "        [ 0.6623, -0.1619],\n",
      "        [ 0.7783,  0.0430],\n",
      "        [ 0.2665,  0.0520],\n",
      "        [ 0.6409, -0.0861],\n",
      "        [ 0.8906,  0.1860],\n",
      "        [ 0.7465,  0.0028],\n",
      "        [ 0.8966,  0.0896],\n",
      "        [ 0.9340,  0.0609],\n",
      "        [ 0.5827,  0.0718],\n",
      "        [ 0.7871,  0.0978]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8333, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7804,  0.1481],\n",
      "        [ 0.7909,  0.1968],\n",
      "        [ 0.6726, -0.2108],\n",
      "        [ 0.6195, -0.0555],\n",
      "        [ 0.5277,  0.0399],\n",
      "        [ 1.0039,  0.1056],\n",
      "        [ 1.1731,  0.0695],\n",
      "        [ 0.8145, -0.3844],\n",
      "        [ 0.6268,  0.1287],\n",
      "        [ 0.9151,  0.2192],\n",
      "        [ 0.8875,  0.1959],\n",
      "        [ 0.7382,  0.3360],\n",
      "        [ 0.3913,  0.1525],\n",
      "        [ 0.6676,  0.1650],\n",
      "        [ 0.6122,  0.0130],\n",
      "        [ 0.8986,  0.0275],\n",
      "        [ 0.8386,  0.1671],\n",
      "        [ 0.7292,  0.2247],\n",
      "        [ 0.7629,  0.0290],\n",
      "        [ 0.6718,  0.2860],\n",
      "        [ 0.8447,  0.1638],\n",
      "        [ 0.8263,  0.0803],\n",
      "        [ 0.8121,  0.1585],\n",
      "        [ 0.4954,  0.1193],\n",
      "        [ 0.5924, -0.0293],\n",
      "        [ 0.8821,  0.0384],\n",
      "        [ 0.6210, -0.0406],\n",
      "        [ 0.6365,  0.2696],\n",
      "        [ 0.7372, -0.0765],\n",
      "        [ 0.5072,  0.0315],\n",
      "        [ 0.6308,  0.1707],\n",
      "        [ 0.9723,  0.0874]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.7877, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4632, -0.0033],\n",
      "        [ 0.6045,  0.0804],\n",
      "        [ 0.6477,  0.1731],\n",
      "        [ 0.9166,  0.1767],\n",
      "        [ 0.9454,  0.2087],\n",
      "        [ 0.5677, -0.0127],\n",
      "        [ 0.7512, -0.0140],\n",
      "        [ 0.9155,  0.1893],\n",
      "        [ 0.7691, -0.2163],\n",
      "        [ 0.8533, -0.0255],\n",
      "        [ 1.0010,  0.3049],\n",
      "        [ 0.8302,  0.1177],\n",
      "        [ 0.7351, -0.0243],\n",
      "        [ 0.4403,  0.0529],\n",
      "        [ 0.6577,  0.0972],\n",
      "        [ 1.1216,  0.0492],\n",
      "        [ 0.8687,  0.0030],\n",
      "        [ 0.9356,  0.1439],\n",
      "        [ 0.5579, -0.1050],\n",
      "        [ 0.8072, -0.0245],\n",
      "        [ 0.8881, -0.1439],\n",
      "        [ 0.7972,  0.0303],\n",
      "        [ 0.8009, -0.1050],\n",
      "        [ 0.3870,  0.0952],\n",
      "        [ 0.8508,  0.1812],\n",
      "        [ 0.8519, -0.0397],\n",
      "        [ 0.4057,  0.2143],\n",
      "        [ 1.1530, -0.0144],\n",
      "        [ 0.8464,  0.3563],\n",
      "        [ 0.7441,  0.0047],\n",
      "        [ 0.5268, -0.0985],\n",
      "        [ 0.3959,  0.2937]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9113, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 6.7895e-01,  1.2093e-02],\n",
      "        [ 4.8723e-01, -1.1848e-02],\n",
      "        [ 7.1023e-01, -4.2527e-02],\n",
      "        [ 6.7891e-01,  3.0846e-01],\n",
      "        [ 9.1754e-01,  2.3569e-01],\n",
      "        [ 3.9089e-01, -1.6368e-01],\n",
      "        [ 7.2831e-01,  1.0445e-01],\n",
      "        [ 6.8717e-01,  2.4974e-01],\n",
      "        [ 7.7453e-01,  2.5067e-02],\n",
      "        [ 6.1446e-01, -7.2338e-02],\n",
      "        [ 8.1148e-01,  1.3234e-01],\n",
      "        [ 9.5430e-01,  1.6192e-02],\n",
      "        [ 5.0256e-01, -2.2234e-01],\n",
      "        [ 6.7752e-01, -1.0823e-01],\n",
      "        [ 5.7703e-01, -7.1520e-02],\n",
      "        [ 3.8462e-01,  1.8280e-01],\n",
      "        [ 3.2951e-01,  2.2960e-01],\n",
      "        [ 5.5501e-01,  8.5682e-07],\n",
      "        [ 1.0532e+00,  3.0382e-03],\n",
      "        [ 5.8123e-01, -1.5008e-02],\n",
      "        [ 9.0413e-01, -8.9602e-02],\n",
      "        [ 9.2973e-01,  2.7373e-01],\n",
      "        [ 7.0666e-01, -3.4046e-02],\n",
      "        [ 4.7946e-01, -2.2430e-01],\n",
      "        [ 4.3022e-01,  2.6444e-01],\n",
      "        [ 7.0114e-01, -2.7425e-03],\n",
      "        [ 8.2147e-01,  3.6411e-01],\n",
      "        [ 7.6217e-01,  1.5399e-01],\n",
      "        [ 8.0327e-01,  1.3306e-01],\n",
      "        [ 4.9227e-01,  1.2052e-01],\n",
      "        [ 5.5891e-01,  2.8131e-02],\n",
      "        [ 1.0530e+00,  1.5947e-01]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8915, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6541,  0.1869],\n",
      "        [ 0.9245,  0.0933],\n",
      "        [ 0.6409, -0.1758],\n",
      "        [ 0.7727,  0.1293],\n",
      "        [ 0.7079, -0.0155],\n",
      "        [ 1.2250, -0.1743],\n",
      "        [ 0.5758,  0.0355],\n",
      "        [ 0.7263,  0.1897],\n",
      "        [ 0.7353,  0.1886],\n",
      "        [ 1.1937,  0.2861],\n",
      "        [ 1.0705,  0.1682],\n",
      "        [ 0.7530,  0.3653],\n",
      "        [ 0.6046,  0.1443],\n",
      "        [ 0.6733,  0.3527],\n",
      "        [ 0.4922, -0.1195],\n",
      "        [ 0.9202,  0.2037],\n",
      "        [ 0.7192,  0.0842],\n",
      "        [ 0.9351, -0.0832],\n",
      "        [ 0.5547, -0.1712],\n",
      "        [ 0.6842,  0.1043],\n",
      "        [ 0.6441, -0.0214],\n",
      "        [ 0.7709,  0.0019],\n",
      "        [ 0.6520,  0.2431],\n",
      "        [ 0.7271,  0.0680],\n",
      "        [ 0.6205,  0.1727],\n",
      "        [ 0.8331,  0.1927],\n",
      "        [ 0.9178,  0.0563],\n",
      "        [ 0.7827,  0.0899],\n",
      "        [ 0.7786,  0.2065],\n",
      "        [ 0.6788,  0.1109],\n",
      "        [ 0.5806,  0.2688],\n",
      "        [ 0.7537, -0.0124]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9391, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4141,  0.1473],\n",
      "        [ 0.5670,  0.1472],\n",
      "        [ 0.5328,  0.2770],\n",
      "        [ 0.8451,  0.0204],\n",
      "        [ 0.6595,  0.1427],\n",
      "        [ 0.5493,  0.0832],\n",
      "        [ 0.7887,  0.0912],\n",
      "        [ 1.1626,  0.0705],\n",
      "        [ 0.7020,  0.1844],\n",
      "        [ 0.5974,  0.1642],\n",
      "        [ 0.5154,  0.1393],\n",
      "        [ 0.9533, -0.0156],\n",
      "        [ 0.7996,  0.0260],\n",
      "        [ 0.5525, -0.0447],\n",
      "        [ 0.8682,  0.1172],\n",
      "        [ 0.8742,  0.1146],\n",
      "        [ 0.6980,  0.2738],\n",
      "        [ 0.4178,  0.0867],\n",
      "        [ 0.7315, -0.0915],\n",
      "        [ 0.9074, -0.1373],\n",
      "        [ 0.7491,  0.0348],\n",
      "        [ 0.7794,  0.2354],\n",
      "        [ 0.2686,  0.1007],\n",
      "        [ 0.7063,  0.1973],\n",
      "        [ 0.7623,  0.1262],\n",
      "        [ 0.8654,  0.0124],\n",
      "        [ 0.7305,  0.1130],\n",
      "        [ 0.6948, -0.0466],\n",
      "        [ 0.9632, -0.2168],\n",
      "        [ 0.7789,  0.2681],\n",
      "        [ 0.6361,  0.1619],\n",
      "        [ 0.8590,  0.2771]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.8644, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7321,  0.1110],\n",
      "        [ 0.3449, -0.2533],\n",
      "        [ 0.6733,  0.2346],\n",
      "        [ 0.8205,  0.1627],\n",
      "        [ 0.5382,  0.2527],\n",
      "        [ 0.7462, -0.2218],\n",
      "        [ 0.3237,  0.1147],\n",
      "        [ 0.8820,  0.1094],\n",
      "        [ 0.8743,  0.1434],\n",
      "        [ 0.1228,  0.2041],\n",
      "        [ 0.8742,  0.0013],\n",
      "        [ 0.5255,  0.0444],\n",
      "        [ 0.4294,  0.0520],\n",
      "        [ 0.6645, -0.0922],\n",
      "        [ 0.4689,  0.2126],\n",
      "        [ 1.0452,  0.2805],\n",
      "        [ 1.1184, -0.0411],\n",
      "        [ 0.5989, -0.1034],\n",
      "        [ 0.6647, -0.1488],\n",
      "        [ 0.8060,  0.2471],\n",
      "        [ 0.9626,  0.3399],\n",
      "        [ 0.5143,  0.0645],\n",
      "        [ 0.5213, -0.1276],\n",
      "        [ 0.6938,  0.1689],\n",
      "        [ 0.4961,  0.0583],\n",
      "        [ 0.7400,  0.0421],\n",
      "        [ 0.5498, -0.1300],\n",
      "        [ 0.6627,  0.1586],\n",
      "        [ 0.9279,  0.0371],\n",
      "        [ 0.6143,  0.0023],\n",
      "        [ 0.8853,  0.0061],\n",
      "        [ 0.6448,  0.0817]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9404, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6860,  0.1218],\n",
      "        [ 0.8878,  0.1538],\n",
      "        [ 0.2454, -0.0627],\n",
      "        [ 0.5842,  0.0690],\n",
      "        [ 0.8946,  0.1981],\n",
      "        [ 0.7645,  0.0702],\n",
      "        [ 0.6324,  0.0243],\n",
      "        [ 0.8768, -0.0668],\n",
      "        [ 0.8090,  0.1498],\n",
      "        [ 0.8260,  0.2642],\n",
      "        [ 0.5994, -0.0751],\n",
      "        [ 0.9506,  0.1394],\n",
      "        [ 0.5807,  0.0197],\n",
      "        [ 0.7246, -0.2677],\n",
      "        [ 0.7028, -0.0342],\n",
      "        [ 0.5884,  0.1600],\n",
      "        [ 0.7370,  0.0966],\n",
      "        [ 0.7594, -0.0270],\n",
      "        [ 0.4329, -0.1826],\n",
      "        [ 0.8369, -0.1204],\n",
      "        [ 0.9586,  0.0041],\n",
      "        [ 0.5926,  0.1613],\n",
      "        [ 0.5622,  0.1465],\n",
      "        [ 0.6285,  0.0602],\n",
      "        [ 0.6818, -0.0040],\n",
      "        [ 1.0993, -0.1000],\n",
      "        [ 1.0125,  0.1111],\n",
      "        [ 0.8344,  0.1901],\n",
      "        [ 0.6569, -0.0763],\n",
      "        [-0.0114,  0.1379],\n",
      "        [ 0.6792,  0.1695],\n",
      "        [ 0.3479,  0.2565]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8905, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6915,  0.1293],\n",
      "        [ 0.7234, -0.0820],\n",
      "        [ 0.8123,  0.0438],\n",
      "        [ 0.3038,  0.0148],\n",
      "        [ 0.8361, -0.0313],\n",
      "        [ 0.9232,  0.2734],\n",
      "        [ 0.6702, -0.1675],\n",
      "        [ 0.4712,  0.0784],\n",
      "        [ 0.4139,  0.0235],\n",
      "        [ 0.5740,  0.0986],\n",
      "        [ 0.3723,  0.1554],\n",
      "        [ 0.7227, -0.0268],\n",
      "        [ 1.0041,  0.1284],\n",
      "        [ 0.9241,  0.0554],\n",
      "        [ 0.7845,  0.0728],\n",
      "        [ 0.6159,  0.0971],\n",
      "        [ 1.0180, -0.0109],\n",
      "        [ 0.8965, -0.0038],\n",
      "        [ 0.8399,  0.1309],\n",
      "        [ 0.7904,  0.1444],\n",
      "        [ 0.7286,  0.3434],\n",
      "        [ 0.7091,  0.0162],\n",
      "        [ 0.6510, -0.0304],\n",
      "        [ 0.7544,  0.2403],\n",
      "        [ 0.7497,  0.0784],\n",
      "        [ 0.4290, -0.3881],\n",
      "        [ 0.7172,  0.1414],\n",
      "        [ 0.7325,  0.1050],\n",
      "        [ 0.7401, -0.0848],\n",
      "        [ 0.6732,  0.0985],\n",
      "        [ 1.0106,  0.0663],\n",
      "        [ 0.7117,  0.2120]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9750, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2806, -0.0053],\n",
      "        [ 0.3349, -0.2224],\n",
      "        [ 0.9145, -0.0642],\n",
      "        [ 0.6836,  0.0131],\n",
      "        [ 0.8048, -0.1441],\n",
      "        [ 0.8298, -0.0441],\n",
      "        [ 1.1488,  0.4020],\n",
      "        [ 0.4663, -0.0048],\n",
      "        [ 0.8265,  0.0380],\n",
      "        [ 0.6894, -0.0023],\n",
      "        [ 0.7762, -0.0511],\n",
      "        [ 0.8564,  0.2311],\n",
      "        [ 0.7928,  0.0451],\n",
      "        [ 1.0897,  0.2153],\n",
      "        [ 0.9844, -0.0164],\n",
      "        [ 0.7083,  0.2556],\n",
      "        [ 0.2527,  0.2118],\n",
      "        [ 0.9128,  0.0157],\n",
      "        [ 0.3031,  0.0243],\n",
      "        [ 0.8882,  0.2522],\n",
      "        [ 0.7082,  0.1405],\n",
      "        [ 0.9582,  0.0967],\n",
      "        [ 0.4240,  0.1019],\n",
      "        [ 0.7124, -0.0165],\n",
      "        [ 0.6673,  0.2826],\n",
      "        [ 0.7201, -0.0422],\n",
      "        [ 0.8164,  0.0705],\n",
      "        [-0.2731,  0.1604],\n",
      "        [ 0.7181,  0.1200],\n",
      "        [ 0.3015,  0.0999],\n",
      "        [ 0.6754, -0.0772],\n",
      "        [ 1.2230,  0.0940]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(1.0055, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6703, -0.1125],\n",
      "        [ 0.6031,  0.2513],\n",
      "        [ 0.6551, -0.0130],\n",
      "        [ 0.9297,  0.2770],\n",
      "        [ 0.9000,  0.1555],\n",
      "        [ 0.6589,  0.1622],\n",
      "        [ 0.6545, -0.1054],\n",
      "        [ 0.5069,  0.2491],\n",
      "        [ 1.1330, -0.0496],\n",
      "        [ 0.4114,  0.1564],\n",
      "        [ 0.6501, -0.0743],\n",
      "        [ 0.9854,  0.2197],\n",
      "        [ 1.0233, -0.0719],\n",
      "        [ 0.6115,  0.1869],\n",
      "        [ 0.5444, -0.0238],\n",
      "        [ 0.9812,  0.0471],\n",
      "        [ 0.7711,  0.0613],\n",
      "        [ 0.7830,  0.2704],\n",
      "        [ 0.8942,  0.1800],\n",
      "        [ 0.7889, -0.2090],\n",
      "        [ 0.7862,  0.3013],\n",
      "        [ 1.0688,  0.0319],\n",
      "        [ 0.6202,  0.1651],\n",
      "        [ 0.8969,  0.0722],\n",
      "        [ 0.9103,  0.0903],\n",
      "        [ 0.3374,  0.1550],\n",
      "        [ 0.5771,  0.0327],\n",
      "        [ 1.0407,  0.2399],\n",
      "        [ 0.6117,  0.0467],\n",
      "        [ 0.6275,  0.0802],\n",
      "        [ 0.5964,  0.0961],\n",
      "        [ 0.4625,  0.2812]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8289, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.5906, -0.1933],\n",
      "        [ 0.7215,  0.0685],\n",
      "        [ 0.4985,  0.1709],\n",
      "        [ 0.6088, -0.0022],\n",
      "        [ 0.5226,  0.0120],\n",
      "        [ 0.6467,  0.1532],\n",
      "        [ 0.5931,  0.0305],\n",
      "        [ 0.9550, -0.2312],\n",
      "        [ 0.6607, -0.0216],\n",
      "        [ 0.5085,  0.1589],\n",
      "        [ 0.5041,  0.0018],\n",
      "        [ 0.3808,  0.0904],\n",
      "        [ 0.4299, -0.2411],\n",
      "        [ 0.7443, -0.0662],\n",
      "        [ 1.0055, -0.0295],\n",
      "        [ 0.9039,  0.2553],\n",
      "        [ 0.8145,  0.1070],\n",
      "        [ 0.6482, -0.0132],\n",
      "        [ 0.9165,  0.0306],\n",
      "        [ 0.7182, -0.0030],\n",
      "        [ 0.7719,  0.1227],\n",
      "        [ 0.6911, -0.2087],\n",
      "        [ 0.5310,  0.1340],\n",
      "        [ 0.9079,  0.2246],\n",
      "        [ 0.8976,  0.1877],\n",
      "        [ 0.8328, -0.0888],\n",
      "        [ 1.2079,  0.0673],\n",
      "        [ 0.8299,  0.1850],\n",
      "        [ 0.4158, -0.1762],\n",
      "        [ 0.8113,  0.1068],\n",
      "        [ 0.7556, -0.0301],\n",
      "        [ 0.5894,  0.1208]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9769, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4170, -0.2189],\n",
      "        [ 0.9779, -0.0460],\n",
      "        [ 0.8300,  0.1348],\n",
      "        [ 0.4875,  0.0981],\n",
      "        [ 0.6688,  0.0405],\n",
      "        [ 0.6816,  0.3266],\n",
      "        [ 0.6816,  0.0849],\n",
      "        [ 0.3988,  0.0589],\n",
      "        [ 0.7039, -0.0992],\n",
      "        [ 0.7754, -0.0209],\n",
      "        [ 0.4923,  0.0419],\n",
      "        [ 0.8569,  0.1430],\n",
      "        [ 0.7521,  0.0644],\n",
      "        [ 0.9649, -0.0406],\n",
      "        [ 0.5479,  0.0282],\n",
      "        [ 0.9703,  0.1103],\n",
      "        [ 0.6998,  0.1510],\n",
      "        [ 0.7601, -0.0125],\n",
      "        [ 0.7105, -0.0261],\n",
      "        [ 0.8658,  0.1978],\n",
      "        [ 0.5810, -0.0822],\n",
      "        [ 0.5471,  0.1487],\n",
      "        [ 0.6525,  0.1254],\n",
      "        [ 0.8308,  0.1283],\n",
      "        [ 1.0794,  0.1946],\n",
      "        [ 0.8909,  0.0870],\n",
      "        [ 0.4609,  0.0977],\n",
      "        [ 0.6825,  0.2572],\n",
      "        [ 0.7290,  0.1773],\n",
      "        [ 0.8011,  0.1529],\n",
      "        [ 0.8188,  0.1507],\n",
      "        [ 0.5315,  0.0347]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9283, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.8580,  0.1637],\n",
      "        [ 0.5374,  0.0788],\n",
      "        [ 0.9942,  0.0831],\n",
      "        [ 0.4583,  0.0396],\n",
      "        [ 0.6043, -0.1303],\n",
      "        [ 0.6183,  0.0208],\n",
      "        [ 0.7046,  0.0945],\n",
      "        [ 0.9466,  0.0595],\n",
      "        [ 1.0546,  0.1758],\n",
      "        [ 0.7541,  0.0714],\n",
      "        [ 0.9648,  0.1507],\n",
      "        [ 0.8200,  0.0173],\n",
      "        [ 0.7213, -0.1474],\n",
      "        [ 0.7519,  0.0189],\n",
      "        [ 0.2334, -0.2257],\n",
      "        [ 0.8303,  0.2280],\n",
      "        [ 0.6222,  0.0820],\n",
      "        [ 0.9450,  0.0515],\n",
      "        [ 0.4187, -0.1373],\n",
      "        [ 0.4342,  0.1845],\n",
      "        [ 1.0213,  0.3186],\n",
      "        [ 0.8725,  0.1458],\n",
      "        [ 0.8633,  0.0937],\n",
      "        [ 0.7210,  0.5067],\n",
      "        [ 0.7071,  0.1818],\n",
      "        [ 0.9381, -0.1048],\n",
      "        [ 1.0820,  0.0991],\n",
      "        [ 0.6128,  0.1475],\n",
      "        [ 0.9431, -0.0825],\n",
      "        [ 0.7473,  0.2838],\n",
      "        [ 0.9152,  0.1165],\n",
      "        [ 0.7321,  0.2410]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.8113, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.2742, -0.0861],\n",
      "        [ 0.7952,  0.1754],\n",
      "        [ 0.5790,  0.0347],\n",
      "        [ 0.7233,  0.1138],\n",
      "        [ 0.5932, -0.0757],\n",
      "        [ 0.9297,  0.4483],\n",
      "        [ 0.6412, -0.0017],\n",
      "        [ 0.7459,  0.1648],\n",
      "        [ 1.0211,  0.1403],\n",
      "        [ 0.4018,  0.0673],\n",
      "        [ 0.8412,  0.3081],\n",
      "        [ 0.1640, -0.1971],\n",
      "        [ 0.7211,  0.1126],\n",
      "        [ 0.5360,  0.1680],\n",
      "        [ 0.6924,  0.1794],\n",
      "        [ 0.5352, -0.1557],\n",
      "        [ 0.7021,  0.1991],\n",
      "        [ 0.9737,  0.3409],\n",
      "        [ 0.6079,  0.0364],\n",
      "        [ 0.4994,  0.2987],\n",
      "        [ 0.4055, -0.0366],\n",
      "        [ 0.3359,  0.0957],\n",
      "        [ 0.4537, -0.0596],\n",
      "        [ 0.5312,  0.1395],\n",
      "        [ 0.9242,  0.1025],\n",
      "        [ 0.2828, -0.1101],\n",
      "        [ 0.8126,  0.1567],\n",
      "        [ 0.5846, -0.0074],\n",
      "        [ 0.7382,  0.4529],\n",
      "        [ 0.4919, -0.1134],\n",
      "        [ 0.5502, -0.0040],\n",
      "        [ 0.9292,  0.0609]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8822, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.7476,  0.2325],\n",
      "        [ 0.5274, -0.1632],\n",
      "        [ 0.4087,  0.0435],\n",
      "        [ 1.0056,  0.0332],\n",
      "        [-0.2295,  0.2307],\n",
      "        [ 0.7453,  0.3252],\n",
      "        [ 0.8194, -0.2542],\n",
      "        [ 0.6614,  0.0625],\n",
      "        [ 0.6519, -0.0539],\n",
      "        [ 0.5105,  0.1310],\n",
      "        [ 0.5586,  0.0218],\n",
      "        [ 0.7431,  0.1666],\n",
      "        [ 0.4551,  0.1098],\n",
      "        [ 0.8644, -0.0812],\n",
      "        [ 0.6808,  0.2091],\n",
      "        [ 0.6638,  0.0701],\n",
      "        [ 0.8104,  0.0669],\n",
      "        [ 0.6315, -0.0738],\n",
      "        [ 0.4470,  0.1066],\n",
      "        [ 0.6364,  0.1788],\n",
      "        [ 0.5186,  0.0038],\n",
      "        [ 0.6420,  0.0063],\n",
      "        [ 1.0587,  0.0721],\n",
      "        [ 0.7733, -0.0247],\n",
      "        [ 0.7923,  0.1333],\n",
      "        [ 0.7710,  0.0391],\n",
      "        [ 0.8667,  0.1370],\n",
      "        [ 0.6794,  0.1806],\n",
      "        [ 0.6653,  0.0238],\n",
      "        [ 0.8733,  0.2088],\n",
      "        [ 0.4282,  0.0464],\n",
      "        [ 0.9245,  0.1488]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9443, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6896,  0.2071],\n",
      "        [ 0.6180,  0.1096],\n",
      "        [ 0.7498,  0.1167],\n",
      "        [ 0.5458,  0.1068],\n",
      "        [ 0.4676,  0.1466],\n",
      "        [ 0.5702, -0.1814],\n",
      "        [ 0.4806,  0.0530],\n",
      "        [ 0.6182,  0.3045],\n",
      "        [ 0.7805, -0.0822],\n",
      "        [ 0.6584, -0.0281],\n",
      "        [ 0.7071,  0.0818],\n",
      "        [ 1.0260, -0.0233],\n",
      "        [ 0.5266,  0.0241],\n",
      "        [ 0.7873,  0.0405],\n",
      "        [ 0.5214, -0.0163],\n",
      "        [ 0.9049, -0.1523],\n",
      "        [ 0.6181, -0.1694],\n",
      "        [ 0.8414,  0.0801],\n",
      "        [ 0.4746, -0.0450],\n",
      "        [ 0.5513,  0.1749],\n",
      "        [ 0.8136, -0.1254],\n",
      "        [ 0.8317, -0.0580],\n",
      "        [ 0.7845,  0.2360],\n",
      "        [ 0.5551,  0.0672],\n",
      "        [ 0.6906, -0.1350],\n",
      "        [ 0.9730,  0.2218],\n",
      "        [ 0.5154,  0.0928],\n",
      "        [ 0.9119,  0.1379],\n",
      "        [ 0.7007, -0.0863],\n",
      "        [ 1.0628,  0.1877],\n",
      "        [ 0.8381,  0.0224],\n",
      "        [ 0.6855, -0.0081]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.8530, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.9246,  0.1241],\n",
      "        [ 1.0928,  0.1924],\n",
      "        [ 0.6778,  0.0926],\n",
      "        [ 1.0764,  0.0912],\n",
      "        [ 0.7560, -0.0959],\n",
      "        [ 0.5205, -0.2359],\n",
      "        [ 0.9072, -0.0980],\n",
      "        [ 0.5716, -0.0416],\n",
      "        [ 0.8748,  0.0314],\n",
      "        [ 0.6752,  0.1577],\n",
      "        [ 0.7860, -0.0185],\n",
      "        [ 0.5964, -0.1348],\n",
      "        [ 0.5329, -0.1142],\n",
      "        [ 0.7600,  0.1065],\n",
      "        [ 1.0717,  0.0581],\n",
      "        [ 0.6526,  0.1648],\n",
      "        [ 0.9363,  0.0351],\n",
      "        [ 0.8202,  0.1630],\n",
      "        [ 0.5470, -0.1253],\n",
      "        [ 0.6270, -0.1002],\n",
      "        [ 0.6843,  0.0596],\n",
      "        [ 0.8082,  0.0598],\n",
      "        [ 0.5911,  0.0409],\n",
      "        [ 0.5459,  0.1157],\n",
      "        [ 0.8786,  0.1826],\n",
      "        [ 0.8662, -0.1127],\n",
      "        [ 0.9722,  0.2725],\n",
      "        [ 0.5651,  0.0218],\n",
      "        [ 0.7150, -0.1497],\n",
      "        [ 0.7666, -0.2148],\n",
      "        [ 0.8942,  0.1382],\n",
      "        [ 0.8487, -0.0248]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step: 210it [00:12, 16.88it/s]\n",
      "epoch:   0%|                                                                                      | 0/4 [00:12<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.9246, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.4728,  0.1264],\n",
      "        [ 0.8969,  0.1241],\n",
      "        [ 0.7220,  0.2474],\n",
      "        [ 0.7090,  0.1989],\n",
      "        [ 0.4975, -0.1529],\n",
      "        [ 0.6892,  0.1214],\n",
      "        [ 0.5973, -0.1205],\n",
      "        [ 0.8064, -0.0767],\n",
      "        [ 0.9888, -0.1634],\n",
      "        [ 0.9957,  0.4381],\n",
      "        [ 0.6598, -0.0743],\n",
      "        [ 0.7961,  0.4322],\n",
      "        [ 0.6839,  0.0813],\n",
      "        [ 0.5933,  0.1730],\n",
      "        [ 0.5757, -0.2757],\n",
      "        [ 0.7534,  0.0568],\n",
      "        [ 0.5466, -0.0593],\n",
      "        [ 0.2677,  0.1505],\n",
      "        [ 0.8103, -0.0018],\n",
      "        [ 0.7269,  0.0943],\n",
      "        [ 0.8505, -0.1025],\n",
      "        [ 0.9394, -0.0565],\n",
      "        [ 1.1651,  0.2563],\n",
      "        [ 0.7906,  0.0263],\n",
      "        [ 0.6901, -0.1205],\n",
      "        [ 0.7536,  0.1464],\n",
      "        [ 0.6808,  0.0594],\n",
      "        [ 0.9388,  0.2185],\n",
      "        [ 0.2205,  0.2450],\n",
      "        [ 0.7671,  0.0276],\n",
      "        [ 0.6813, -0.0056],\n",
      "        [ 1.1233,  0.0213]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "SequenceClassifierOutput(loss=tensor(0.9480, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[ 0.6797,  0.2073],\n",
      "        [ 0.7343,  0.1358],\n",
      "        [ 0.3900,  0.0984],\n",
      "        [ 1.0849,  0.4197],\n",
      "        [ 1.0942,  0.3656],\n",
      "        [ 0.9433,  0.1378],\n",
      "        [ 0.6804,  0.1096],\n",
      "        [ 0.6821,  0.1444],\n",
      "        [ 0.4036,  0.3275],\n",
      "        [ 0.6665,  0.2248],\n",
      "        [ 0.9387,  0.0989],\n",
      "        [ 0.7081,  0.1381],\n",
      "        [ 0.5060,  0.0035],\n",
      "        [ 1.2009,  0.1473],\n",
      "        [ 0.5812, -0.0613],\n",
      "        [ 0.8394,  0.2373],\n",
      "        [ 0.7177,  0.0122],\n",
      "        [ 0.5973,  0.1080],\n",
      "        [ 0.6507,  0.1356],\n",
      "        [ 0.6760, -0.0876],\n",
      "        [ 0.5269,  0.1651],\n",
      "        [ 0.8354, -0.0280],\n",
      "        [ 0.8437,  0.3776],\n",
      "        [ 0.3864, -0.0725],\n",
      "        [ 0.7747,  0.1733],\n",
      "        [ 0.7983,  0.0459],\n",
      "        [ 0.8349,  0.3139],\n",
      "        [ 1.0563,  0.0836],\n",
      "        [ 0.9632,  0.1300],\n",
      "        [ 0.7388,  0.0201],\n",
      "        [ 0.7416,  0.0125],\n",
      "        [ 0.0534, -0.2349]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/paneah/Desktop/bert-multiprocessing/debug.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Byonseiserver/home/paneah/Desktop/bert-multiprocessing/debug.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m b_labels \u001b[39m=\u001b[39m batch[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Byonseiserver/home/paneah/Desktop/bert-multiprocessing/debug.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m model\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Byonseiserver/home/paneah/Desktop/bert-multiprocessing/debug.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m loss, logits \u001b[39m=\u001b[39m model(b_input_ids, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Byonseiserver/home/paneah/Desktop/bert-multiprocessing/debug.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m                      token_type_ids\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Byonseiserver/home/paneah/Desktop/bert-multiprocessing/debug.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m                      attention_mask\u001b[39m=\u001b[39;49mb_input_mask, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Byonseiserver/home/paneah/Desktop/bert-multiprocessing/debug.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m                      labels\u001b[39m=\u001b[39;49mb_labels)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Byonseiserver/home/paneah/Desktop/bert-multiprocessing/debug.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mprint\u001b[39m(model(b_input_ids, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Byonseiserver/home/paneah/Desktop/bert-multiprocessing/debug.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m                      token_type_ids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Byonseiserver/home/paneah/Desktop/bert-multiprocessing/debug.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m                      attention_mask\u001b[39m=\u001b[39mb_input_mask, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Byonseiserver/home/paneah/Desktop/bert-multiprocessing/debug.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m                      labels\u001b[39m=\u001b[39mb_labels))\n",
      "File \u001b[0;32m~/anaconda3/envs/LIS8040/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/LIS8040/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1564\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1557\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1560\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1561\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1564\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m   1565\u001b[0m     input_ids,\n\u001b[1;32m   1566\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1567\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1568\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1569\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1570\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1571\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1572\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1573\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1574\u001b[0m )\n\u001b[1;32m   1576\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m   1578\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m~/anaconda3/envs/LIS8040/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/LIS8040/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1013\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1015\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1016\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1017\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[0;32m-> 1022\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1023\u001b[0m     embedding_output,\n\u001b[1;32m   1024\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1025\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1026\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1027\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1028\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1029\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1030\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1031\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1032\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1033\u001b[0m )\n\u001b[1;32m   1034\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1035\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/LIS8040/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/LIS8040/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:612\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    603\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    604\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    605\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    610\u001b[0m     )\n\u001b[1;32m    611\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 612\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    613\u001b[0m         hidden_states,\n\u001b[1;32m    614\u001b[0m         attention_mask,\n\u001b[1;32m    615\u001b[0m         layer_head_mask,\n\u001b[1;32m    616\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    617\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    618\u001b[0m         past_key_value,\n\u001b[1;32m    619\u001b[0m         output_attentions,\n\u001b[1;32m    620\u001b[0m     )\n\u001b[1;32m    622\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    623\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/LIS8040/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/LIS8040/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    486\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    487\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    495\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    498\u001b[0m         hidden_states,\n\u001b[1;32m    499\u001b[0m         attention_mask,\n\u001b[1;32m    500\u001b[0m         head_mask,\n\u001b[1;32m    501\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    502\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    503\u001b[0m     )\n\u001b[1;32m    504\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/LIS8040/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/LIS8040/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:436\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    418\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    419\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    427\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself(\n\u001b[1;32m    428\u001b[0m         hidden_states,\n\u001b[1;32m    429\u001b[0m         attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    434\u001b[0m         output_attentions,\n\u001b[1;32m    435\u001b[0m     )\n\u001b[0;32m--> 436\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(self_outputs[\u001b[39m0\u001b[39;49m], hidden_states)\n\u001b[1;32m    437\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n\u001b[1;32m    438\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/LIS8040/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/LIS8040/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:388\u001b[0m, in \u001b[0;36mBertSelfOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    386\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense(hidden_states)\n\u001b[1;32m    387\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m--> 388\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mLayerNorm(hidden_states \u001b[39m+\u001b[39;49m input_tensor)\n\u001b[1;32m    389\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/anaconda3/envs/LIS8040/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/LIS8040/lib/python3.11/site-packages/torch/nn/modules/normalization.py:191\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    190\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlayer_norm(\n\u001b[0;32m--> 191\u001b[0m         \u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalized_shape, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meps)\n",
      "File \u001b[0;32m~/anaconda3/envs/LIS8040/lib/python3.11/site-packages/torch/nn/modules/module.py:1601\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1598\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_backward_pre_hooks\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   1599\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39m=\u001b[39m OrderedDict()\n\u001b[0;32m-> 1601\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tensor, \u001b[39m'\u001b[39m\u001b[39mModule\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m   1602\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   1603\u001b[0m         _parameters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "for epoch_i in tqdm(range(0, epochs), desc = \"epoch\", mininterval=0.01):\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in tqdm(enumerate(train_dataloader), desc = \"step\", mininterval=0.01):\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        loss, logits = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "    \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
