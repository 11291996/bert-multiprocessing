{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/paneah/anaconda3/envs/LIS8040/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from utils.train import trainer\n",
    "test_trainer = trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original bert cpu\n",
    "test_trainer.train(\"bert\", 1, 32, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#original bert with 1 gpu\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtest_trainer\u001b[49m\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_trainer' is not defined"
     ]
    }
   ],
   "source": [
    "#original bert with 1 gpu\n",
    "test_trainer.train(\"bert\", 1, 32, \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/paneah/anaconda3/envs/LIS8040/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/paneah/anaconda3/envs/LIS8040/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/paneah/anaconda3/envs/LIS8040/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/paneah/anaconda3/envs/LIS8040/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).\n",
      "[W socket.cpp:426] [c10d] The server socket cannot be initialized on [::]:29500 (errno: 97 - Address family not supported by protocol).\n",
      "[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).\n",
      "[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).\n",
      "[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [localhost]:29500 (errno: 97 - Address family not supported by protocol).\n",
      "train_step: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 121/121 [00:24<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 1.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_step: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14/14 [00:00<00:00, 35.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.79\n",
      "  Validation Loss: 0.97\n",
      "\n",
      "Training complete!\n",
      "Time taken: 24.84622597694397\n"
     ]
    }
   ],
   "source": [
    "#must be run after test_trainer\n",
    "from utils.base_train import multigpu\n",
    "from accelerate import notebook_launcher\n",
    "args = (1, 32)\n",
    "notebook_launcher(multigpu, args, num_processes = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.6.attention.self.value.left_weight', 'bert.encoder.layer.10.attention.self.value.right_weight', 'bert.encoder.layer.9.intermediate.dense.right_weight', 'bert.encoder.layer.2.attention.self.query.right_weight', 'bert.encoder.layer.1.output.dense.left_weight', 'bert.encoder.layer.0.attention.self.query.right_weight', 'bert.encoder.layer.8.attention.self.value.right_weight', 'bert.encoder.layer.10.intermediate.dense.right_weight', 'bert.encoder.layer.5.output.dense.left_weight', 'bert.encoder.layer.4.attention.self.value.right_weight', 'bert.encoder.layer.7.intermediate.dense.right_weight', 'bert.encoder.layer.8.output.dense.right_weight', 'bert.encoder.layer.8.attention.self.value.left_weight', 'bert.encoder.layer.8.attention.output.dense.left_weight', 'bert.encoder.layer.7.output.dense.left_weight', 'bert.encoder.layer.11.intermediate.dense.right_weight', 'bert.encoder.layer.3.intermediate.dense.left_weight', 'bert.encoder.layer.2.output.dense.left_weight', 'bert.encoder.layer.2.attention.self.query.left_weight', 'bert.encoder.layer.0.output.dense.left_weight', 'bert.encoder.layer.9.attention.self.value.left_weight', 'bert.encoder.layer.11.intermediate.dense.left_weight', 'bert.encoder.layer.7.attention.self.key.right_weight', 'bert.encoder.layer.9.output.dense.right_weight', 'bert.encoder.layer.6.attention.self.key.right_weight', 'bert.encoder.layer.4.intermediate.dense.right_weight', 'bert.encoder.layer.9.attention.output.dense.left_weight', 'bert.encoder.layer.0.attention.output.dense.left_weight', 'bert.encoder.layer.5.attention.self.query.right_weight', 'bert.encoder.layer.8.attention.self.key.left_weight', 'bert.encoder.layer.5.attention.output.dense.right_weight', 'bert.encoder.layer.3.attention.self.key.left_weight', 'bert.encoder.layer.6.attention.self.query.left_weight', 'bert.encoder.layer.2.attention.output.dense.right_weight', 'bert.encoder.layer.10.output.dense.right_weight', 'bert.encoder.layer.0.attention.self.key.left_weight', 'bert.encoder.layer.9.output.dense.left_weight', 'bert.encoder.layer.4.attention.self.query.right_weight', 'bert.encoder.layer.4.attention.self.value.left_weight', 'bert.encoder.layer.3.output.dense.left_weight', 'bert.encoder.layer.2.attention.self.key.right_weight', 'bert.encoder.layer.2.attention.self.value.right_weight', 'bert.encoder.layer.2.attention.self.value.left_weight', 'bert.encoder.layer.7.attention.self.query.left_weight', 'bert.encoder.layer.6.attention.self.value.right_weight', 'bert.encoder.layer.7.intermediate.dense.left_weight', 'bert.encoder.layer.8.attention.output.dense.right_weight', 'bert.encoder.layer.2.attention.output.dense.left_weight', 'bert.encoder.layer.11.output.dense.left_weight', 'bert.encoder.layer.4.output.dense.left_weight', 'classifier.left_weight', 'bert.encoder.layer.11.attention.output.dense.right_weight', 'bert.encoder.layer.6.attention.self.key.left_weight', 'bert.encoder.layer.9.attention.self.value.right_weight', 'bert.encoder.layer.1.output.dense.right_weight', 'bert.encoder.layer.1.intermediate.dense.left_weight', 'bert.encoder.layer.5.intermediate.dense.right_weight', 'bert.pooler.dense.right_weight', 'bert.encoder.layer.11.attention.output.dense.left_weight', 'bert.encoder.layer.10.attention.output.dense.right_weight', 'bert.encoder.layer.8.intermediate.dense.right_weight', 'bert.encoder.layer.9.attention.self.key.left_weight', 'bert.encoder.layer.3.attention.self.value.left_weight', 'bert.encoder.layer.5.attention.self.value.right_weight', 'bert.encoder.layer.7.output.dense.right_weight', 'bert.encoder.layer.4.intermediate.dense.left_weight', 'bert.encoder.layer.5.attention.output.dense.left_weight', 'bert.encoder.layer.5.intermediate.dense.left_weight', 'bert.encoder.layer.1.attention.self.query.left_weight', 'bert.encoder.layer.1.attention.output.dense.right_weight', 'bert.encoder.layer.3.attention.self.query.right_weight', 'bert.encoder.layer.6.attention.self.query.right_weight', 'bert.encoder.layer.6.intermediate.dense.right_weight', 'bert.encoder.layer.3.attention.output.dense.left_weight', 'bert.encoder.layer.6.output.dense.right_weight', 'bert.encoder.layer.11.output.dense.right_weight', 'bert.encoder.layer.4.attention.output.dense.left_weight', 'bert.encoder.layer.11.attention.self.value.right_weight', 'bert.encoder.layer.0.attention.self.query.left_weight', 'bert.encoder.layer.7.attention.self.query.right_weight', 'bert.encoder.layer.7.attention.self.value.left_weight', 'bert.encoder.layer.1.intermediate.dense.right_weight', 'bert.encoder.layer.11.attention.self.key.left_weight', 'bert.encoder.layer.10.attention.self.query.left_weight', 'bert.encoder.layer.2.attention.self.key.left_weight', 'bert.encoder.layer.8.attention.self.query.right_weight', 'bert.encoder.layer.9.attention.output.dense.right_weight', 'bert.encoder.layer.4.attention.self.key.right_weight', 'bert.encoder.layer.11.attention.self.value.left_weight', 'bert.encoder.layer.8.output.dense.left_weight', 'bert.encoder.layer.0.intermediate.dense.right_weight', 'bert.encoder.layer.6.attention.output.dense.right_weight', 'bert.encoder.layer.0.output.dense.right_weight', 'bert.encoder.layer.4.attention.output.dense.right_weight', 'bert.encoder.layer.0.attention.output.dense.right_weight', 'bert.encoder.layer.1.attention.self.value.left_weight', 'bert.encoder.layer.10.attention.self.query.right_weight', 'bert.encoder.layer.5.attention.self.key.left_weight', 'bert.pooler.dense.left_weight', 'bert.encoder.layer.6.attention.output.dense.left_weight', 'classifier.bias', 'bert.encoder.layer.5.attention.self.value.left_weight', 'bert.encoder.layer.7.attention.output.dense.right_weight', 'bert.encoder.layer.0.attention.self.value.left_weight', 'bert.encoder.layer.4.attention.self.query.left_weight', 'bert.encoder.layer.1.attention.self.key.right_weight', 'bert.encoder.layer.10.attention.output.dense.left_weight', 'bert.encoder.layer.5.output.dense.right_weight', 'bert.encoder.layer.6.output.dense.left_weight', 'bert.encoder.layer.2.output.dense.right_weight', 'bert.encoder.layer.10.intermediate.dense.left_weight', 'bert.encoder.layer.4.output.dense.right_weight', 'bert.encoder.layer.1.attention.output.dense.left_weight', 'bert.encoder.layer.4.attention.self.key.left_weight', 'bert.encoder.layer.11.attention.self.key.right_weight', 'bert.encoder.layer.10.attention.self.key.left_weight', 'bert.encoder.layer.7.attention.self.value.right_weight', 'bert.encoder.layer.9.attention.self.key.right_weight', 'bert.encoder.layer.9.intermediate.dense.left_weight', 'bert.encoder.layer.10.output.dense.left_weight', 'bert.encoder.layer.3.output.dense.right_weight', 'bert.encoder.layer.6.intermediate.dense.left_weight', 'bert.encoder.layer.0.attention.self.key.right_weight', 'bert.encoder.layer.3.attention.self.key.right_weight', 'bert.encoder.layer.1.attention.self.key.left_weight', 'bert.encoder.layer.9.attention.self.query.right_weight', 'bert.encoder.layer.3.attention.self.value.right_weight', 'bert.encoder.layer.5.attention.self.key.right_weight', 'bert.encoder.layer.1.attention.self.value.right_weight', 'bert.encoder.layer.11.attention.self.query.right_weight', 'classifier.weight', 'bert.encoder.layer.3.intermediate.dense.right_weight', 'bert.encoder.layer.5.attention.self.query.left_weight', 'bert.encoder.layer.8.attention.self.query.left_weight', 'classifier.right_weight', 'bert.encoder.layer.3.attention.output.dense.right_weight', 'bert.encoder.layer.7.attention.self.key.left_weight', 'bert.encoder.layer.3.attention.self.query.left_weight', 'bert.encoder.layer.2.intermediate.dense.left_weight', 'bert.encoder.layer.8.attention.self.key.right_weight', 'bert.encoder.layer.0.intermediate.dense.left_weight', 'bert.encoder.layer.8.intermediate.dense.left_weight', 'bert.encoder.layer.9.attention.self.query.left_weight', 'bert.encoder.layer.10.attention.self.value.left_weight', 'bert.encoder.layer.0.attention.self.value.right_weight', 'bert.encoder.layer.1.attention.self.query.right_weight', 'bert.encoder.layer.11.attention.self.query.left_weight', 'bert.encoder.layer.2.intermediate.dense.right_weight', 'bert.encoder.layer.10.attention.self.key.right_weight', 'bert.encoder.layer.7.attention.output.dense.left_weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/paneah/Desktop/bert-multiprocessing/modules/parallel_linear.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.left_weight = Parameter(torch.tensor(self.weight[...,:self.in_features // 2], device=self.left_device))\n",
      "/home/paneah/Desktop/bert-multiprocessing/modules/parallel_linear.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.right_weight = Parameter(torch.tensor(self.weight[...,self.in_features // 2:], device=self.right_device))\n",
      "/home/paneah/anaconda3/envs/LIS8040/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "train_step:   8%|█████████████▌                                                                                                                                                      | 20/241 [00:33<06:10,  1.68s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#ours\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtest_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpbert\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/bert-multiprocessing/utils/train.py:264\u001b[0m, in \u001b[0;36mtrainer.train\u001b[0;34m(self, model_name, epoch, batch_size, device)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m PBertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# Use the 12-layer BERT model, with an uncased vocab.\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     num_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;66;03m# The number of output labels--2 for binary classification.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    259\u001b[0m     output_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;66;03m# Whether the model returns all hidden-states.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m )\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mupdate_weight()\n\u001b[0;32m--> 264\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/bert-multiprocessing/utils/train.py:143\u001b[0m, in \u001b[0;36mtrainer.training\u001b[0;34m(self, model, epoch, batch_size, device_name)\u001b[0m\n\u001b[1;32m    141\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather(loss)\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[0;32m--> 143\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    147\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/LIS8040/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/LIS8040/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#ours\n",
    "test_trainer.train(\"pbert\", 1, 32, None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
