{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = nn.Linear(10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import math\n",
    "from torch.nn import init, functional as F\n",
    "\n",
    "class ParallelLinear(Module):\n",
    "    __constants__ = ['in_features', 'out_features']\n",
    "    in_features: int\n",
    "    out_features: int\n",
    "    weight: Tensor\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True,\n",
    "                 dtype=None) -> None:\n",
    "        \n",
    "        device = [i for i in range(torch.cuda.device_count())]\n",
    "\n",
    "        factory_kwargs = {'device': torch.device(\"cpu\"), 'dtype': dtype}\n",
    "        factory_kwargs_right = {'device': torch.device(\"cuda:0\"), 'dtype': dtype}\n",
    "        factory_kwargs_left = {'device': torch.device(\"cuda:1\"), 'dtype': dtype}\n",
    "\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.left_weight = Parameter(torch.empty((out_features, in_features // 2), **factory_kwargs_left))\n",
    "        self.right_weight = Parameter(torch.empty((out_features, in_features // 2 + in_features % 2), **factory_kwargs_right))\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n",
    "        # uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\n",
    "        # https://github.com/pytorch/pytorch/issues/57109\n",
    "        init.kaiming_uniform_(self.left_weight, a=math.sqrt(5))\n",
    "        init.kaiming_uniform_(self.right_weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.left_weight)\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        left_input = input[:,:(self.in_features // 2)].to(torch.device(\"cuda:1\"))\n",
    "        right_input = input[:,(self.in_features // 2):].to(torch.device(\"cuda:0\"))\n",
    "\n",
    "        left_product = F.linear(left_input, self.left_weight)\n",
    "        right_product = F.linear(right_input, self.right_weight)\n",
    "        \n",
    "        left_product = left_product.to(torch.device(\"cpu\"))\n",
    "        right_product = right_product.to(torch.device(\"cpu\"))\n",
    "        return left_product + right_product + self.bias\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plinear = ParallelLinear(10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plinear.left_weight.shape, plinear.right_weight.shape, data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plinear.forward(data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Step : 1\tLoss : 1.006647\n",
      "Train Step : 2\tLoss : 1.013926\n",
      "Train Step : 3\tLoss : 1.009988\n",
      "Train Step : 4\tLoss : 1.013408\n",
      "Train Step : 5\tLoss : 1.012748\n",
      "Train Step : 6\tLoss : 1.009166\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/paneah/Desktop/bert-multiprocessing/exp.ipynb Cell 8\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Byonseiserver/home/paneah/Desktop/bert-multiprocessing/exp.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m target \u001b[39m=\u001b[39m target\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Byonseiserver/home/paneah/Desktop/bert-multiprocessing/exp.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Byonseiserver/home/paneah/Desktop/bert-multiprocessing/exp.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m output \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Byonseiserver/home/paneah/Desktop/bert-multiprocessing/exp.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, target)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Byonseiserver/home/paneah/Desktop/bert-multiprocessing/exp.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/LIS8040/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/paneah/Desktop/bert-multiprocessing/exp.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Byonseiserver/home/paneah/Desktop/bert-multiprocessing/exp.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mtanh(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(x))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Byonseiserver/home/paneah/Desktop/bert-multiprocessing/exp.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mtanh(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x))\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Byonseiserver/home/paneah/Desktop/bert-multiprocessing/exp.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mtanh(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc3(x))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Byonseiserver/home/paneah/Desktop/bert-multiprocessing/exp.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mtanh(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc4(x))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Byonseiserver/home/paneah/Desktop/bert-multiprocessing/exp.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mtanh(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc5(x))\n",
      "File \u001b[0;32m~/anaconda3/envs/LIS8040/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time \n",
    "\n",
    "input_size = 768\n",
    "output_size = 10\n",
    "\n",
    "batch_size = 10000\n",
    "data_size = 100000\n",
    "learning_rate = 0.001\n",
    "epoch = 10\n",
    "\n",
    "class RandomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, size, length):\n",
    "        self.len = length\n",
    "        self.data = torch.randn(length, size)\n",
    "        self.output = torch.randn(length, output_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.output[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "rand_loader = DataLoader(dataset=RandomDataset(input_size, data_size),\n",
    "                         batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class ParallelNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(ParallelNet, self).__init__()\n",
    "    self.fc1 = ParallelLinear(768, 100)\n",
    "    self.fc2 = ParallelLinear(100, 1000)\n",
    "    self.fc3 = ParallelLinear(1000, 100000)\n",
    "    self.fc4 = ParallelLinear(100000, 10000)\n",
    "    self.fc5 = ParallelLinear(10000, 1000)\n",
    "    self.fc6 = ParallelLinear(1000, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.tanh(self.fc1(x))\n",
    "    x = F.tanh(self.fc2(x))\n",
    "    x = F.tanh(self.fc3(x))\n",
    "    x = F.tanh(self.fc4(x))\n",
    "    x = F.tanh(self.fc5(x))\n",
    "    x = F.tanh(self.fc6(x))\n",
    "    x = x.view(-1, 10)\n",
    "    x = F.softmax(x, dim = 1)\n",
    "    return x\n",
    "  \n",
    "model = ParallelNet()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) \n",
    "\n",
    "i = 1\n",
    "start = time.time()\n",
    "for epoch in range(epoch):\n",
    "    for data, target in rand_loader:\n",
    "        data = data\n",
    "        target = target\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 1 == 0:\n",
    "            print(\"Train Step : {}\\tLoss : {:3f}\".format(i, loss.item()))\n",
    "        i += 1\n",
    "end = time.time()\n",
    "print(\"Time taken : {}\".format(end - start))\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.73 GiB (GPU 0; 23.69 GiB total capacity; 19.54 GiB already allocated; 138.06 MiB free; 23.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/paneah/Desktop/bert-multiprocessing/exp.ipynb Cell 9\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Byonseiserver/home/paneah/Desktop/bert-multiprocessing/exp.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m output \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Byonseiserver/home/paneah/Desktop/bert-multiprocessing/exp.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, target)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Byonseiserver/home/paneah/Desktop/bert-multiprocessing/exp.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Byonseiserver/home/paneah/Desktop/bert-multiprocessing/exp.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Byonseiserver/home/paneah/Desktop/bert-multiprocessing/exp.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m1\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/LIS8040/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/LIS8040/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.73 GiB (GPU 0; 23.69 GiB total capacity; 19.54 GiB already allocated; 138.06 MiB free; 23.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time \n",
    "\n",
    "input_size = 1\n",
    "output_size = 10\n",
    "\n",
    "batch_size = 10000\n",
    "data_size = 100000\n",
    "learning_rate = 0.001\n",
    "epoch = 10\n",
    "\n",
    "class RandomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, size, length):\n",
    "        self.len = length\n",
    "        self.data = torch.randn(length, size)\n",
    "        self.output = torch.randn(length, output_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.output[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "rand_loader = DataLoader(dataset=RandomDataset(input_size, data_size),\n",
    "                         batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.fc1 = torch.nn.Linear(1, 100)\n",
    "    self.fc2 = torch.nn.Linear(100, 1000)\n",
    "    self.fc3 = torch.nn.Linear(1000, 100000)\n",
    "    self.fc4 = torch.nn.Linear(100000, 10000)\n",
    "    self.fc5 = torch.nn.Linear(10000, 1000)\n",
    "    self.fc6 = torch.nn.Linear(1000, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.tanh(self.fc1(x))\n",
    "    x = F.tanh(self.fc2(x))\n",
    "    x = F.tanh(self.fc3(x))\n",
    "    x = F.tanh(self.fc4(x))\n",
    "    x = F.tanh(self.fc5(x))\n",
    "    x = F.tanh(self.fc6(x))\n",
    "    x = x.view(-1, 10)\n",
    "    x = F.softmax(x, dim = 1)\n",
    "    return x\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "model = MyNet()\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) \n",
    "\n",
    "i = 1\n",
    "start = time.time()\n",
    "for epoch in range(epoch):\n",
    "    for data, target in rand_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 1 == 0:\n",
    "            print(\"Train Step : {}\\tLoss : {:3f}\".format(i, loss.item()))\n",
    "        i += 1\n",
    "end = time.time()\n",
    "print(\"Time taken : {}\".format(end - start))\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallelLinear = ParallelLinear(10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tensor = torch.Tensor([1.0, 1.0, 1.2, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 2.1])\n",
    "\n",
    "parallelLinear.forward(my_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.forward(my_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor의 bias를 언제 더해줘야 하는지?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tensor = torch.Tensor([0.0, 0.1, 0.2]).unsqueeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
